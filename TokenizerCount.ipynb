{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPR0TlfoxzTaY2+ui6zMc10",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c5b367d68aff4bcdbc0ff56204e7b6e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_594987b6359646f2afeeed768dda9d1a",
              "IPY_MODEL_67bb7985c1ce406a80bf87e97b5dd3a6",
              "IPY_MODEL_c12f0a4150614074b815edc6c0073c83"
            ],
            "layout": "IPY_MODEL_f183284de2cd41ae823ece646f02d80c"
          }
        },
        "594987b6359646f2afeeed768dda9d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e5ab20c6ea4456fb1ab9e108e5eb24d",
            "placeholder": "​",
            "style": "IPY_MODEL_6d0c56b5dc6740e088d2c36ed174c2bc",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "67bb7985c1ce406a80bf87e97b5dd3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d207efa700d24e9688963dbd54cc09c1",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36c67685d4dc4cd5b9d37577db04bc43",
            "value": 26
          }
        },
        "c12f0a4150614074b815edc6c0073c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_313a2195eca64eaeaf160f093f8524cd",
            "placeholder": "​",
            "style": "IPY_MODEL_0e08874289464b41b23eee07104eea8a",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.56kB/s]"
          }
        },
        "f183284de2cd41ae823ece646f02d80c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e5ab20c6ea4456fb1ab9e108e5eb24d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d0c56b5dc6740e088d2c36ed174c2bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d207efa700d24e9688963dbd54cc09c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36c67685d4dc4cd5b9d37577db04bc43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "313a2195eca64eaeaf160f093f8524cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e08874289464b41b23eee07104eea8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21e4ee474a8f4530a82837aaaef4b17a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ce341d4f70940e3bc202438ec43d35d",
              "IPY_MODEL_058cda189dbe48bd8dd520f6f2f0701c",
              "IPY_MODEL_7d32dc94616d4764b897c6a00099d049"
            ],
            "layout": "IPY_MODEL_1053c45a52c64acc9ad36a5d4516a4e3"
          }
        },
        "7ce341d4f70940e3bc202438ec43d35d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4683882bfd34a7fa919ac95cdd56036",
            "placeholder": "​",
            "style": "IPY_MODEL_06032f4f86d8483d9e6fb01a3b1642d7",
            "value": "vocab.json: 100%"
          }
        },
        "058cda189dbe48bd8dd520f6f2f0701c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc40d5a733764fa7b9f1e153c49d8e0e",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b7d2b0dd89845a9967acac57bdfe52d",
            "value": 1042301
          }
        },
        "7d32dc94616d4764b897c6a00099d049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5301e5dbcd846b8ba82e31c314caa84",
            "placeholder": "​",
            "style": "IPY_MODEL_0706e5d3370f465e82fb97ef543085bb",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 7.85MB/s]"
          }
        },
        "1053c45a52c64acc9ad36a5d4516a4e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4683882bfd34a7fa919ac95cdd56036": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06032f4f86d8483d9e6fb01a3b1642d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc40d5a733764fa7b9f1e153c49d8e0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b7d2b0dd89845a9967acac57bdfe52d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5301e5dbcd846b8ba82e31c314caa84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0706e5d3370f465e82fb97ef543085bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f345195464ae4b9784e6569758e41c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e64c6edcfb36496499a7530f11e12c51",
              "IPY_MODEL_613441468fba48df953259be6ebb6cbe",
              "IPY_MODEL_a85f9c7440cf402aa39387a2096f518c"
            ],
            "layout": "IPY_MODEL_a78393904d694ea892825dcb30052e90"
          }
        },
        "e64c6edcfb36496499a7530f11e12c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53e4aa0537344c89b01a4c0a06c5c6c1",
            "placeholder": "​",
            "style": "IPY_MODEL_2b05efeb0904488792164da2835ee0ff",
            "value": "merges.txt: 100%"
          }
        },
        "613441468fba48df953259be6ebb6cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_906fb2cd6bb04ff7a4de57a72eab4ce2",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a751bcb477d7492f85261cb8dd1e6f64",
            "value": 456318
          }
        },
        "a85f9c7440cf402aa39387a2096f518c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82264777163c46aa80cfa5d852d016d6",
            "placeholder": "​",
            "style": "IPY_MODEL_763a33e3af284faab0e883863427f658",
            "value": " 456k/456k [00:00&lt;00:00, 13.4MB/s]"
          }
        },
        "a78393904d694ea892825dcb30052e90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53e4aa0537344c89b01a4c0a06c5c6c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b05efeb0904488792164da2835ee0ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "906fb2cd6bb04ff7a4de57a72eab4ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a751bcb477d7492f85261cb8dd1e6f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82264777163c46aa80cfa5d852d016d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "763a33e3af284faab0e883863427f658": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fe7370d69994deab540350d7d07661d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce00251788154f18b258d1bfe13e632c",
              "IPY_MODEL_24aeae3348184946a00127f3b6f8c403",
              "IPY_MODEL_da7e440f57274840b8ef616ccd48aade"
            ],
            "layout": "IPY_MODEL_64e681d3641f4244a59fb34a027bbc17"
          }
        },
        "ce00251788154f18b258d1bfe13e632c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8edf205214074180bbe91ae85a6bcbbe",
            "placeholder": "​",
            "style": "IPY_MODEL_8fd54bbcd54348d2b9c439a274130a59",
            "value": "tokenizer.json: 100%"
          }
        },
        "24aeae3348184946a00127f3b6f8c403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16a13748dcec47e889c6e09e79eb4bd7",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52ff098443a049efa518c1d06f805bbb",
            "value": 1355256
          }
        },
        "da7e440f57274840b8ef616ccd48aade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_005bd2af50ea47c1aa44288114afd152",
            "placeholder": "​",
            "style": "IPY_MODEL_951be530208544819b3cb0e7ea32bfa4",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 18.9MB/s]"
          }
        },
        "64e681d3641f4244a59fb34a027bbc17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8edf205214074180bbe91ae85a6bcbbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fd54bbcd54348d2b9c439a274130a59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16a13748dcec47e889c6e09e79eb4bd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52ff098443a049efa518c1d06f805bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "005bd2af50ea47c1aa44288114afd152": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "951be530208544819b3cb0e7ea32bfa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8446acfc7ab4a21b5c3cc128f9e246b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0abc4ad094a643b1a3009634b64055b7",
              "IPY_MODEL_68a1e668bdf74063bb59114701cf2ea7",
              "IPY_MODEL_d28ba934dbbf4960b6e3546785536661"
            ],
            "layout": "IPY_MODEL_76cdf964312c48448efb06b52224fbfc"
          }
        },
        "0abc4ad094a643b1a3009634b64055b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dda9f01d66034d82b4557a91b6f0b493",
            "placeholder": "​",
            "style": "IPY_MODEL_8516d09e1cc248a7aa01d976f97de166",
            "value": "config.json: 100%"
          }
        },
        "68a1e668bdf74063bb59114701cf2ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bb86e01b346429ba4c59301844c3d6d",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91376972229f45bf8a68f86a908305bf",
            "value": 665
          }
        },
        "d28ba934dbbf4960b6e3546785536661": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_969ea06112cc4dcf80dbb623c235f26e",
            "placeholder": "​",
            "style": "IPY_MODEL_84cadc333b2d4c4ab2e5de718626e250",
            "value": " 665/665 [00:00&lt;00:00, 44.9kB/s]"
          }
        },
        "76cdf964312c48448efb06b52224fbfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dda9f01d66034d82b4557a91b6f0b493": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8516d09e1cc248a7aa01d976f97de166": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bb86e01b346429ba4c59301844c3d6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91376972229f45bf8a68f86a908305bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "969ea06112cc4dcf80dbb623c235f26e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84cadc333b2d4c4ab2e5de718626e250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cab57e519986488abc88640bbed8eddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6520f49c2cd4d6485f7394fb2d0f0f1",
              "IPY_MODEL_f8f2caad9fe7430c83b11f2b6ba59310",
              "IPY_MODEL_56efb688bf15489582c007dc3e012350"
            ],
            "layout": "IPY_MODEL_742cfec3894d47a4b143914ab4fd573e"
          }
        },
        "f6520f49c2cd4d6485f7394fb2d0f0f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6298c3ccf6cb4331ac7341c86cf6838d",
            "placeholder": "​",
            "style": "IPY_MODEL_b71820287a6c4017b615bcccbb7dddac",
            "value": "spiece.model: 100%"
          }
        },
        "f8f2caad9fe7430c83b11f2b6ba59310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d33c5614db264ad5bc56b5c734ac1ac3",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ff6994f514843c39e357d3157e8f7e8",
            "value": 791656
          }
        },
        "56efb688bf15489582c007dc3e012350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_826c42088411417ca70bbda0db71d8dc",
            "placeholder": "​",
            "style": "IPY_MODEL_53d80dc4cd434f06b59344bf413897b9",
            "value": " 792k/792k [00:00&lt;00:00, 26.5MB/s]"
          }
        },
        "742cfec3894d47a4b143914ab4fd573e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6298c3ccf6cb4331ac7341c86cf6838d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b71820287a6c4017b615bcccbb7dddac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d33c5614db264ad5bc56b5c734ac1ac3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ff6994f514843c39e357d3157e8f7e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "826c42088411417ca70bbda0db71d8dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53d80dc4cd434f06b59344bf413897b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77dd1053cdf645c092d9b569fae56c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f17490faa01e4ab4b1fe160db681c578",
              "IPY_MODEL_41e9145573db4185babfdb8b65fa8c76",
              "IPY_MODEL_a70bbeeef8e14d15b75c7341392f3f9a"
            ],
            "layout": "IPY_MODEL_4f27cc467c4d4651ae8320a5de2737e2"
          }
        },
        "f17490faa01e4ab4b1fe160db681c578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18a29b709320450281ed6a8f65973a1f",
            "placeholder": "​",
            "style": "IPY_MODEL_d7ab94d298f34ad3b921e1f68f121b34",
            "value": "tokenizer.json: 100%"
          }
        },
        "41e9145573db4185babfdb8b65fa8c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4ca30b53fa94b48a7cc19b501c872d6",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4edcad644fa4443ab898b329b3ad1ffe",
            "value": 1389353
          }
        },
        "a70bbeeef8e14d15b75c7341392f3f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_236d7e32bec7445eafa9edd552177a0e",
            "placeholder": "​",
            "style": "IPY_MODEL_04bc410e74844d94a907ba842239bb3d",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 12.3MB/s]"
          }
        },
        "4f27cc467c4d4651ae8320a5de2737e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18a29b709320450281ed6a8f65973a1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7ab94d298f34ad3b921e1f68f121b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4ca30b53fa94b48a7cc19b501c872d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4edcad644fa4443ab898b329b3ad1ffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "236d7e32bec7445eafa9edd552177a0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04bc410e74844d94a907ba842239bb3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d5166ae43d44f44971e743cd261df0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0629f7e3d4754934a5ac5930a859e605",
              "IPY_MODEL_b19fe5f676314eddbea3157ce0913b31",
              "IPY_MODEL_069745579e074200ad124eeee2555c3f"
            ],
            "layout": "IPY_MODEL_35d71661fb134393b29a9c7a0b5b668e"
          }
        },
        "0629f7e3d4754934a5ac5930a859e605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7c247b764794ad1b8a9107fa2b21224",
            "placeholder": "​",
            "style": "IPY_MODEL_7759600064d9411cbe008a4498659b4a",
            "value": "config.json: 100%"
          }
        },
        "b19fe5f676314eddbea3157ce0913b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b92ce0945cd4d7c9c85448122d24184",
            "max": 1208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e294aab6ee14820b378badc4bdfe754",
            "value": 1208
          }
        },
        "069745579e074200ad124eeee2555c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d1fc7ad630f42cba920133bec4f55c2",
            "placeholder": "​",
            "style": "IPY_MODEL_aa420563c21446b4af0b83434b7ef16d",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 64.9kB/s]"
          }
        },
        "35d71661fb134393b29a9c7a0b5b668e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7c247b764794ad1b8a9107fa2b21224": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7759600064d9411cbe008a4498659b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b92ce0945cd4d7c9c85448122d24184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e294aab6ee14820b378badc4bdfe754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d1fc7ad630f42cba920133bec4f55c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa420563c21446b4af0b83434b7ef16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80115468f62b4e4390b713f09f48ea39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7302b69ba85b4ad48120e844bb08980d",
              "IPY_MODEL_3b816424f23e426e82cfab4e155d187a",
              "IPY_MODEL_8575b1aaaacd47849cb27f321d8b2db2"
            ],
            "layout": "IPY_MODEL_61d505b42da4410782d1f1bde9b09232"
          }
        },
        "7302b69ba85b4ad48120e844bb08980d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c55760e35c84a76ac412bfbe934be62",
            "placeholder": "​",
            "style": "IPY_MODEL_55b4de5cb40b4aedba975d3d558f5cf8",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "3b816424f23e426e82cfab4e155d187a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c06734840984188bfa707c3ed7f1561",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d30b813cfb6496b93299e86b62a1cc0",
            "value": 48
          }
        },
        "8575b1aaaacd47849cb27f321d8b2db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f427f6cf795f4fc7b840894b82834955",
            "placeholder": "​",
            "style": "IPY_MODEL_bb51fc578ac741d1ad0dda9b2a6fc35d",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.02kB/s]"
          }
        },
        "61d505b42da4410782d1f1bde9b09232": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c55760e35c84a76ac412bfbe934be62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55b4de5cb40b4aedba975d3d558f5cf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c06734840984188bfa707c3ed7f1561": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d30b813cfb6496b93299e86b62a1cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f427f6cf795f4fc7b840894b82834955": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb51fc578ac741d1ad0dda9b2a6fc35d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6043fff6d50f4fadaf6e4b66422c7a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0741351e44014103a5d170edc434cd53",
              "IPY_MODEL_933bae9747c44111a95ef389ea1f99bf",
              "IPY_MODEL_98ee31ebab53498f807f10ef60817735"
            ],
            "layout": "IPY_MODEL_49a74258c2194528b3a996e86d48146d"
          }
        },
        "0741351e44014103a5d170edc434cd53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fa1ca458df94b2cbde4ecabeca175cc",
            "placeholder": "​",
            "style": "IPY_MODEL_871d00ea386f4b93936bf2504cd171da",
            "value": "vocab.txt: 100%"
          }
        },
        "933bae9747c44111a95ef389ea1f99bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e19d93b533f4bfda52ae2dc49c7a6a4",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_346a3c5001be4377a50fe37293e662f4",
            "value": 231508
          }
        },
        "98ee31ebab53498f807f10ef60817735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8e92d766b8f4e2ab4ba0d1fe1fcbbf0",
            "placeholder": "​",
            "style": "IPY_MODEL_fc521c41578c40c9b60967e262246776",
            "value": " 232k/232k [00:00&lt;00:00, 15.0MB/s]"
          }
        },
        "49a74258c2194528b3a996e86d48146d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fa1ca458df94b2cbde4ecabeca175cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "871d00ea386f4b93936bf2504cd171da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e19d93b533f4bfda52ae2dc49c7a6a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "346a3c5001be4377a50fe37293e662f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8e92d766b8f4e2ab4ba0d1fe1fcbbf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc521c41578c40c9b60967e262246776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4435cf036ff403893dc20a1e66bfe74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90264c4795674a4f9bb80f8553329f31",
              "IPY_MODEL_c52ab74ab8cb4df183fd286b63ca28e7",
              "IPY_MODEL_2f64ecf58f62426fb78c2392c27efa57"
            ],
            "layout": "IPY_MODEL_7cbc804a7fcd480e869d7ba01ee9567f"
          }
        },
        "90264c4795674a4f9bb80f8553329f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_293ed8100bc742a49b00ed26063a6e9c",
            "placeholder": "​",
            "style": "IPY_MODEL_0f02907f8ac8482b9520f899ab3562af",
            "value": "tokenizer.json: 100%"
          }
        },
        "c52ab74ab8cb4df183fd286b63ca28e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e85bc48cfda64ac9929e1f5a971e7126",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be74176f1f754c00919ce2489736ebd7",
            "value": 466062
          }
        },
        "2f64ecf58f62426fb78c2392c27efa57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_394d986169dc487d9e1cb60906f80755",
            "placeholder": "​",
            "style": "IPY_MODEL_9c47674feb034dc0a598e32aaf032edb",
            "value": " 466k/466k [00:00&lt;00:00, 14.1MB/s]"
          }
        },
        "7cbc804a7fcd480e869d7ba01ee9567f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "293ed8100bc742a49b00ed26063a6e9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f02907f8ac8482b9520f899ab3562af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e85bc48cfda64ac9929e1f5a971e7126": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be74176f1f754c00919ce2489736ebd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "394d986169dc487d9e1cb60906f80755": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c47674feb034dc0a598e32aaf032edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81e33bb3ea124086b694c064de07cb01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00c360bb7c6142298fae2f41374bebac",
              "IPY_MODEL_6d532f5ac1864f1ba5817d9d83324655",
              "IPY_MODEL_bd206081518948708415507c8660ac73"
            ],
            "layout": "IPY_MODEL_bf19b19fea6948f199dc5bf3c0935053"
          }
        },
        "00c360bb7c6142298fae2f41374bebac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fe7df8fb5cc4de8a98b24da2aaecae6",
            "placeholder": "​",
            "style": "IPY_MODEL_0775077797f94a8eaccd828f246920ad",
            "value": "config.json: 100%"
          }
        },
        "6d532f5ac1864f1ba5817d9d83324655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5f14665eeae48f0bb471125a10728d3",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50edacb9bcd2460fae5e4edfe2acc94f",
            "value": 570
          }
        },
        "bd206081518948708415507c8660ac73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_150275879d4e4f08a3a1572fe2cd1fb0",
            "placeholder": "​",
            "style": "IPY_MODEL_298dd85de1c8484ea97470fabf14bdbb",
            "value": " 570/570 [00:00&lt;00:00, 25.6kB/s]"
          }
        },
        "bf19b19fea6948f199dc5bf3c0935053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fe7df8fb5cc4de8a98b24da2aaecae6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0775077797f94a8eaccd828f246920ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5f14665eeae48f0bb471125a10728d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50edacb9bcd2460fae5e4edfe2acc94f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "150275879d4e4f08a3a1572fe2cd1fb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "298dd85de1c8484ea97470fabf14bdbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08248a155bde4e6fa60f6e1147ec0ad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a4b17f20a104ddfbb9839825eb482e4",
              "IPY_MODEL_20fe4a73e49c4d70a0bd65023328b63e",
              "IPY_MODEL_951d121ec44041ddbad09bb534be797b"
            ],
            "layout": "IPY_MODEL_281d3c283b504edd94e4f3103f19dc81"
          }
        },
        "9a4b17f20a104ddfbb9839825eb482e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a73cf294176434a8c4305e4dfe10baa",
            "placeholder": "​",
            "style": "IPY_MODEL_d2c7b287cfa2498fb9b0abfbb3016240",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "20fe4a73e49c4d70a0bd65023328b63e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3be53c38077f4deaaf741726da8c30fe",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_528d42a4b17f4ad5a8e664b6bc0064cf",
            "value": 25
          }
        },
        "951d121ec44041ddbad09bb534be797b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e6c359aeabb4746889d9ee08e74262a",
            "placeholder": "​",
            "style": "IPY_MODEL_268b07a8a496439fb436e14211680065",
            "value": " 25.0/25.0 [00:00&lt;00:00, 1.11kB/s]"
          }
        },
        "281d3c283b504edd94e4f3103f19dc81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a73cf294176434a8c4305e4dfe10baa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2c7b287cfa2498fb9b0abfbb3016240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3be53c38077f4deaaf741726da8c30fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "528d42a4b17f4ad5a8e664b6bc0064cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e6c359aeabb4746889d9ee08e74262a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "268b07a8a496439fb436e14211680065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17657b75cf9847b48417b3196b7b226a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2a3f80413f94c32bd3955e58bd9429f",
              "IPY_MODEL_facb07ff85e4499bae5aab2f51dde4c3",
              "IPY_MODEL_5adf6978c1374525860dd851f3549ae7"
            ],
            "layout": "IPY_MODEL_7fda061f4d9a4c498e16cb407c6104bf"
          }
        },
        "a2a3f80413f94c32bd3955e58bd9429f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17eea774f2434ab1930130333b9f6d62",
            "placeholder": "​",
            "style": "IPY_MODEL_1e82f70e55bf4ee49de6fbe1931103ab",
            "value": "vocab.json: 100%"
          }
        },
        "facb07ff85e4499bae5aab2f51dde4c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63c0dcafdc9d4974a85206176be325b0",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d3e2d4001144400961dcb85067846ab",
            "value": 898823
          }
        },
        "5adf6978c1374525860dd851f3549ae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ba84a8ae7dc477ab327bd7ef87a6fde",
            "placeholder": "​",
            "style": "IPY_MODEL_bf8d599fc0c14debbbb3ba7a1d3b521c",
            "value": " 899k/899k [00:00&lt;00:00, 3.61MB/s]"
          }
        },
        "7fda061f4d9a4c498e16cb407c6104bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17eea774f2434ab1930130333b9f6d62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e82f70e55bf4ee49de6fbe1931103ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63c0dcafdc9d4974a85206176be325b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d3e2d4001144400961dcb85067846ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ba84a8ae7dc477ab327bd7ef87a6fde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf8d599fc0c14debbbb3ba7a1d3b521c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8643201f565e4d12adbc5b39679e27e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b443c301b5041649d3e6894670e7585",
              "IPY_MODEL_b1c1342ad6654036a131bde283b3a08d",
              "IPY_MODEL_aaf1adc7c8e94a06a98f1bdd609d772c"
            ],
            "layout": "IPY_MODEL_0ba1d2e279d44cb0ba7e63d33f67d719"
          }
        },
        "0b443c301b5041649d3e6894670e7585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03aac56f796045c3a051c24fb5a254ef",
            "placeholder": "​",
            "style": "IPY_MODEL_ffab82748aec4fa294f3b2f0021f616d",
            "value": "merges.txt: 100%"
          }
        },
        "b1c1342ad6654036a131bde283b3a08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e31de2847cdb4fa4a191b9498784c22f",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80ab1b3643574ef8885f66c8026e13d7",
            "value": 456318
          }
        },
        "aaf1adc7c8e94a06a98f1bdd609d772c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e9a2f7d156444ff85739c6a42b52fde",
            "placeholder": "​",
            "style": "IPY_MODEL_c1c55a6cfd434028b928ae8386d38a1e",
            "value": " 456k/456k [00:00&lt;00:00, 13.7MB/s]"
          }
        },
        "0ba1d2e279d44cb0ba7e63d33f67d719": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03aac56f796045c3a051c24fb5a254ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffab82748aec4fa294f3b2f0021f616d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e31de2847cdb4fa4a191b9498784c22f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80ab1b3643574ef8885f66c8026e13d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e9a2f7d156444ff85739c6a42b52fde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1c55a6cfd434028b928ae8386d38a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76a93b16d77544bc9aacbf5b8ddb0de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd8e764272c44d53bc33bd102bf475cc",
              "IPY_MODEL_f74ccd51382840eb860e980a29cc61e1",
              "IPY_MODEL_f9014e72e67f49c1ade4fbe26bc9cba1"
            ],
            "layout": "IPY_MODEL_34bc4085faee4981ab206b15879df229"
          }
        },
        "bd8e764272c44d53bc33bd102bf475cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8427608eed7d43468f04c5faa16e11d0",
            "placeholder": "​",
            "style": "IPY_MODEL_a4bb76e2f6d74fe8a523f0f2824089bb",
            "value": "tokenizer.json: 100%"
          }
        },
        "f74ccd51382840eb860e980a29cc61e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ed985e805fa4d308280b2f0f9f9dc3d",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a9a5514c49e4a33b7cb58c0d448afe9",
            "value": 1355863
          }
        },
        "f9014e72e67f49c1ade4fbe26bc9cba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0951a9b8323e49ada843a26794d122da",
            "placeholder": "​",
            "style": "IPY_MODEL_86ec83199e424418b1dfc190febb3043",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 17.1MB/s]"
          }
        },
        "34bc4085faee4981ab206b15879df229": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8427608eed7d43468f04c5faa16e11d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4bb76e2f6d74fe8a523f0f2824089bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ed985e805fa4d308280b2f0f9f9dc3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a9a5514c49e4a33b7cb58c0d448afe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0951a9b8323e49ada843a26794d122da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86ec83199e424418b1dfc190febb3043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "579b79f544ea4c8ca2af3dec0e1fb9ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c628069a8e64b28a67fb186f38ad639",
              "IPY_MODEL_907067aad6134e1c85f54d6ef14802a0",
              "IPY_MODEL_7c0dd3a696a8484b9db4b09200f593e7"
            ],
            "layout": "IPY_MODEL_5edf3a3587a5463e890a4cc96f189179"
          }
        },
        "9c628069a8e64b28a67fb186f38ad639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65632eed8a4a4b2ba681b10b92b3e573",
            "placeholder": "​",
            "style": "IPY_MODEL_d1ca435e47564117ac140902bcebb172",
            "value": "config.json: 100%"
          }
        },
        "907067aad6134e1c85f54d6ef14802a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af4d756efc5f462f86406f25dcf0cbed",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3372aebd9d194f348e88b46296496d7c",
            "value": 481
          }
        },
        "7c0dd3a696a8484b9db4b09200f593e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e21f2166fea4ad9a90da1bb062232d9",
            "placeholder": "​",
            "style": "IPY_MODEL_9658c113d6074f3d908ab14118b9cb6c",
            "value": " 481/481 [00:00&lt;00:00, 29.2kB/s]"
          }
        },
        "5edf3a3587a5463e890a4cc96f189179": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65632eed8a4a4b2ba681b10b92b3e573": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1ca435e47564117ac140902bcebb172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af4d756efc5f462f86406f25dcf0cbed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3372aebd9d194f348e88b46296496d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e21f2166fea4ad9a90da1bb062232d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9658c113d6074f3d908ab14118b9cb6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonpalafox/colab_notebooks/blob/main/TokenizerCount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF Token Counter for Google Colab\n",
        "\n",
        "This script helps you analyze PDF documents and count tokens using different tokenizers from Hugging Face. It's specifically designed to work in Google Colab notebooks.\n",
        "\n",
        "## Features\n",
        "\n",
        "- Extract and analyze text from PDF documents\n",
        "- Count tokens using multiple tokenizers (GPT-2, T5, BERT, RoBERTa)\n",
        "- Support for custom tokenizers from Hugging Face\n",
        "- Interactive file selection in Colab\n",
        "- Detailed text statistics\n",
        "\n",
        "## Requirements\n",
        "\n",
        "The script requires these Python packages:\n",
        "- PyPDF2\n",
        "- transformers\n",
        "- torch\n",
        "\n",
        "These are typically pre-installed in Google Colab, but you can install any missing packages with:\n",
        "\n",
        "```python\n",
        "!pip install PyPDF2 transformers torch\n",
        "```\n",
        "\n",
        "## Basic Usage\n",
        "\n",
        "### Simple Method\n",
        "\n",
        "Run the script and then call the `analyze_pdf()` function without arguments:\n",
        "\n",
        "```python\n",
        "# After pasting the script in a cell and running it\n",
        "analyze_pdf()\n",
        "```\n",
        "\n",
        "This will:\n",
        "1. Search for PDF files in your Colab environment\n",
        "2. Let you select one from a list or upload a new one\n",
        "3. Analyze the PDF and display token counts from multiple tokenizers\n",
        "\n",
        "### Advanced Method\n",
        "\n",
        "You can specify parameters directly:\n",
        "\n",
        "```python\n",
        "analyze_pdf(\n",
        "    pdf_path=\"your_document.pdf\",              # Path to a specific PDF file\n",
        "    tokenizers=[\"gpt2\", \"bert\"],               # Only use these tokenizers\n",
        "    custom_tokenizer=\"facebook/opt-350m\"       # Add a custom tokenizer\n",
        ")\n",
        "```\n",
        "\n",
        "## Function Reference\n",
        "\n",
        "### `analyze_pdf(pdf_path=None, tokenizers=None, custom_tokenizer=None)`\n",
        "\n",
        "Analyzes a PDF file and counts tokens.\n",
        "\n",
        "**Parameters:**\n",
        "- `pdf_path` (str, optional): Path to the PDF file. If not provided, will interactively find/request a file.\n",
        "- `tokenizers` (list, optional): List of tokenizer names to use. Defaults to [\"gpt2\", \"t5\", \"bert\", \"roberta\"].\n",
        "- `custom_tokenizer` (str, optional): Name of a custom tokenizer from Hugging Face.\n",
        "\n",
        "**Returns:**\n",
        "A dictionary containing:\n",
        "- `text`: The extracted text\n",
        "- `statistics`: Character and word counts\n",
        "- `token_counts`: Token counts for each tokenizer\n",
        "\n",
        "## Example Output\n",
        "\n",
        "```\n",
        "PDF has 5 pages\n",
        "Extracting text from example.pdf...\n",
        "\n",
        "Text statistics:\n",
        "Characters: 15,428\n",
        "Words: 2,547\n",
        "\n",
        "Counting tokens using 4 tokenizers...\n",
        "\n",
        "Token counts:\n",
        "gpt2: 3,256 tokens\n",
        "t5: 4,121 tokens\n",
        "bert: 3,891 tokens\n",
        "roberta: 3,347 tokens\n",
        "```\n",
        "\n",
        "## Understanding Token Counts\n",
        "\n",
        "Different tokenizers will produce different token counts for the same text because they:\n",
        "\n",
        "1. **Use different vocabularies** - Words common in the training data may be a single token\n",
        "2. **Have different tokenization strategies** - Some keep punctuation separate, others combine\n",
        "3. **Handle whitespace differently** - Some tokenizers preserve whitespace as tokens\n",
        "4. **Process capitalization differently** - Some are case-sensitive\n",
        "\n",
        "This is why the script shows results from multiple tokenizers, helping you understand the range of possible token counts for your document.\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "- **No text extracted**: Some PDFs contain images of text rather than actual text characters. Try using OCR software first.\n",
        "- **Error loading tokenizer**: Check your internet connection or try a different tokenizer.\n",
        "- **Memory errors**: Very large PDFs may cause memory issues. Try processing the document in smaller chunks.\n",
        "\n",
        "## Advanced Usage\n",
        "\n",
        "You can capture the results for further analysis:\n",
        "\n",
        "```python\n",
        "results = analyze_pdf(\"your_document.pdf\")\n",
        "\n",
        "# Access extracted text\n",
        "text = results[\"text\"]\n",
        "\n",
        "# Get statistics\n",
        "character_count = results[\"statistics\"][\"characters\"]\n",
        "word_count = results[\"statistics\"][\"words\"]\n",
        "\n",
        "# Get token counts for specific tokenizers\n",
        "gpt2_count = results[\"token_counts\"][\"gpt2\"]\n",
        "bert_count = results[\"token_counts\"][\"bert\"]\n",
        "```"
      ],
      "metadata": {
        "id": "8-MtK9v7AVyd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZslhFdJ75y2",
        "outputId": "a100453a-6982-4cfe-c405-8ba00e4e4988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py', '/root/.local/share/jupyter/runtime/kernel-f1bed850-b491-4613-870e-05e66de8f011.json']\n",
            "Extracting text from /root/.local/share/jupyter/runtime/kernel-f1bed850-b491-4613-870e-05e66de8f011.json...\n",
            "Error extracting text from PDF: EOF marker not found\n",
            "No text extracted from PDF. Check if the PDF contains extractable text.\n"
          ]
        }
      ],
      "source": [
        "# PDF Token Counter for Google Colab\n",
        "# This script counts tokens in PDF files using various tokenizers from Hugging Face\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from typing import Dict, List, Optional\n",
        "from google.colab import files\n",
        "\n",
        "# PDF processing\n",
        "import PyPDF2\n",
        "\n",
        "# Hugging Face tokenizers\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    GPT2Tokenizer,\n",
        "    T5Tokenizer,\n",
        "    BertTokenizer,\n",
        "    RobertaTokenizer\n",
        ")\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract all text from a PDF file\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file\n",
        "\n",
        "    Returns:\n",
        "        Extracted text as a string\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            num_pages = len(reader.pages)\n",
        "\n",
        "            print(f\"PDF has {num_pages} pages\")\n",
        "\n",
        "            for page_num in range(num_pages):\n",
        "                page = reader.pages[page_num]\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def count_tokens(text: str, tokenizer_name: str) -> int:\n",
        "    \"\"\"\n",
        "    Count tokens in text using the specified tokenizer\n",
        "\n",
        "    Args:\n",
        "        text: Text to tokenize\n",
        "        tokenizer_name: Name of the tokenizer to use\n",
        "\n",
        "    Returns:\n",
        "        Number of tokens\n",
        "    \"\"\"\n",
        "    tokenizers = {\n",
        "        \"gpt2\": GPT2Tokenizer.from_pretrained(\"gpt2\"),\n",
        "        \"t5\": T5Tokenizer.from_pretrained(\"t5-base\"),\n",
        "        \"bert\": BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
        "        \"roberta\": RobertaTokenizer.from_pretrained(\"roberta-base\"),\n",
        "    }\n",
        "\n",
        "    # Add support for custom tokenizers from Hugging Face\n",
        "    if tokenizer_name not in tokenizers:\n",
        "        try:\n",
        "            tokenizers[tokenizer_name] = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading tokenizer {tokenizer_name}: {e}\")\n",
        "            return -1\n",
        "\n",
        "    tokenizer = tokenizers[tokenizer_name]\n",
        "    tokens = tokenizer.encode(text)\n",
        "\n",
        "    return len(tokens)\n",
        "\n",
        "def count_tokens_with_multiple_tokenizers(text: str, tokenizer_names: Optional[List[str]] = None) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Count tokens in text using multiple tokenizers\n",
        "\n",
        "    Args:\n",
        "        text: Text to tokenize\n",
        "        tokenizer_names: List of tokenizer names to use\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping tokenizer names to token counts\n",
        "    \"\"\"\n",
        "    if not tokenizer_names:\n",
        "        # Default tokenizers\n",
        "        tokenizer_names = [\"gpt2\", \"t5\", \"bert\", \"roberta\"]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for tokenizer_name in tokenizer_names:\n",
        "        token_count = count_tokens(text, tokenizer_name)\n",
        "        results[tokenizer_name] = token_count\n",
        "\n",
        "    return results\n",
        "\n",
        "def find_pdf_files():\n",
        "    \"\"\"Find PDF files in the current directory and subdirectories\"\"\"\n",
        "    pdf_files = []\n",
        "    for root, _, files in os.walk('.'):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.pdf'):\n",
        "                pdf_files.append(os.path.join(root, file))\n",
        "    return pdf_files\n",
        "\n",
        "# For use in Colab notebooks\n",
        "def analyze_pdf(pdf_path=None, tokenizers=None, custom_tokenizer=None):\n",
        "    \"\"\"Analyze a PDF file and count tokens\"\"\"\n",
        "\n",
        "    if pdf_path is None:\n",
        "        # List available PDF files\n",
        "        pdf_files = find_pdf_files()\n",
        "\n",
        "        if not pdf_files:\n",
        "            print(\"No PDF files found in the current directory.\")\n",
        "            print(\"Please upload a PDF file using the code below:\")\n",
        "            print(\"from google.colab import files\")\n",
        "            print(\"uploaded = files.upload()\")\n",
        "            return\n",
        "\n",
        "        print(\"Available PDF files:\")\n",
        "        for i, file in enumerate(pdf_files):\n",
        "            print(f\"{i+1}. {file}\")\n",
        "\n",
        "        choice = input(\"Enter the number of the PDF to analyze (or upload a new one): \")\n",
        "\n",
        "        try:\n",
        "            choice_idx = int(choice) - 1\n",
        "            if 0 <= choice_idx < len(pdf_files):\n",
        "                pdf_path = pdf_files[choice_idx]\n",
        "            else:\n",
        "                print(\"Invalid choice. Please upload a PDF file.\")\n",
        "                uploaded = files.upload()\n",
        "                pdf_path = list(uploaded.keys())[0]\n",
        "        except:\n",
        "            print(\"Invalid input. Please upload a PDF file.\")\n",
        "            uploaded = files.upload()\n",
        "            pdf_path = list(uploaded.keys())[0]\n",
        "\n",
        "    # Check if the PDF file exists\n",
        "    if not os.path.isfile(pdf_path):\n",
        "        print(f\"Error: PDF file {pdf_path} not found\")\n",
        "        return\n",
        "\n",
        "    # Set default tokenizers if not provided\n",
        "    if tokenizers is None:\n",
        "        tokenizers = [\"gpt2\", \"t5\", \"bert\", \"roberta\"]\n",
        "\n",
        "    # Add custom tokenizer if provided\n",
        "    tokenizers_to_use = tokenizers.copy()\n",
        "    if custom_tokenizer:\n",
        "        tokenizers_to_use.append(custom_tokenizer)\n",
        "\n",
        "    # Extract text from PDF\n",
        "    print(f\"Extracting text from {pdf_path}...\")\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    if not text:\n",
        "        print(\"No text extracted from PDF. Check if the PDF contains extractable text.\")\n",
        "        return\n",
        "\n",
        "    # Print text statistics\n",
        "    print(f\"\\nText statistics:\")\n",
        "    print(f\"Characters: {len(text):,}\")\n",
        "    print(f\"Words: {len(text.split()):,}\")\n",
        "\n",
        "    # Count tokens using different tokenizers\n",
        "    print(f\"\\nCounting tokens using {len(tokenizers_to_use)} tokenizers...\")\n",
        "    token_counts = count_tokens_with_multiple_tokenizers(text, tokenizers_to_use)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nToken counts:\")\n",
        "    for tokenizer_name, count in token_counts.items():\n",
        "        if count >= 0:\n",
        "            print(f\"{tokenizer_name}: {count:,} tokens\")\n",
        "        else:\n",
        "            print(f\"{tokenizer_name}: Error loading tokenizer\")\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"statistics\": {\n",
        "            \"characters\": len(text),\n",
        "            \"words\": len(text.split())\n",
        "        },\n",
        "        \"token_counts\": token_counts\n",
        "    }\n",
        "\n",
        "# For direct execution in Colab\n",
        "if __name__ == \"__main__\":\n",
        "    # Filter out Colab-specific arguments\n",
        "    args = [arg for arg in sys.argv if not arg.startswith('-f')]\n",
        "    print(args)\n",
        "    sys.argv = args\n",
        "\n",
        "    # Create argparse without the required argument, which causes problems in Colab\n",
        "    parser = argparse.ArgumentParser(description=\"Count tokens in PDF files using various tokenizers\")\n",
        "    parser.add_argument(\"pdf_path\", nargs='?', help=\"Path to the PDF file\")\n",
        "    parser.add_argument(\"--tokenizers\", nargs=\"+\", default=[\"gpt2\", \"t5\", \"bert\", \"roberta\"],\n",
        "                      help=\"List of tokenizers to use (default: gpt2, t5, bert, roberta)\")\n",
        "    parser.add_argument(\"--custom\", help=\"Use a custom tokenizer from Hugging Face (e.g., 'gpt-neox-20b')\")\n",
        "\n",
        "    try:\n",
        "        args = parser.parse_args()\n",
        "        analyze_pdf(args.pdf_path, args.tokenizers, args.custom)\n",
        "    except SystemExit:\n",
        "        # Called from a notebook without arguments\n",
        "        analyze_pdf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_pdf()"
      ],
      "metadata": {
        "id": "00k1tLUW8MFr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c5b367d68aff4bcdbc0ff56204e7b6e4",
            "594987b6359646f2afeeed768dda9d1a",
            "67bb7985c1ce406a80bf87e97b5dd3a6",
            "c12f0a4150614074b815edc6c0073c83",
            "f183284de2cd41ae823ece646f02d80c",
            "9e5ab20c6ea4456fb1ab9e108e5eb24d",
            "6d0c56b5dc6740e088d2c36ed174c2bc",
            "d207efa700d24e9688963dbd54cc09c1",
            "36c67685d4dc4cd5b9d37577db04bc43",
            "313a2195eca64eaeaf160f093f8524cd",
            "0e08874289464b41b23eee07104eea8a",
            "21e4ee474a8f4530a82837aaaef4b17a",
            "7ce341d4f70940e3bc202438ec43d35d",
            "058cda189dbe48bd8dd520f6f2f0701c",
            "7d32dc94616d4764b897c6a00099d049",
            "1053c45a52c64acc9ad36a5d4516a4e3",
            "f4683882bfd34a7fa919ac95cdd56036",
            "06032f4f86d8483d9e6fb01a3b1642d7",
            "fc40d5a733764fa7b9f1e153c49d8e0e",
            "7b7d2b0dd89845a9967acac57bdfe52d",
            "a5301e5dbcd846b8ba82e31c314caa84",
            "0706e5d3370f465e82fb97ef543085bb",
            "f345195464ae4b9784e6569758e41c4a",
            "e64c6edcfb36496499a7530f11e12c51",
            "613441468fba48df953259be6ebb6cbe",
            "a85f9c7440cf402aa39387a2096f518c",
            "a78393904d694ea892825dcb30052e90",
            "53e4aa0537344c89b01a4c0a06c5c6c1",
            "2b05efeb0904488792164da2835ee0ff",
            "906fb2cd6bb04ff7a4de57a72eab4ce2",
            "a751bcb477d7492f85261cb8dd1e6f64",
            "82264777163c46aa80cfa5d852d016d6",
            "763a33e3af284faab0e883863427f658",
            "0fe7370d69994deab540350d7d07661d",
            "ce00251788154f18b258d1bfe13e632c",
            "24aeae3348184946a00127f3b6f8c403",
            "da7e440f57274840b8ef616ccd48aade",
            "64e681d3641f4244a59fb34a027bbc17",
            "8edf205214074180bbe91ae85a6bcbbe",
            "8fd54bbcd54348d2b9c439a274130a59",
            "16a13748dcec47e889c6e09e79eb4bd7",
            "52ff098443a049efa518c1d06f805bbb",
            "005bd2af50ea47c1aa44288114afd152",
            "951be530208544819b3cb0e7ea32bfa4",
            "a8446acfc7ab4a21b5c3cc128f9e246b",
            "0abc4ad094a643b1a3009634b64055b7",
            "68a1e668bdf74063bb59114701cf2ea7",
            "d28ba934dbbf4960b6e3546785536661",
            "76cdf964312c48448efb06b52224fbfc",
            "dda9f01d66034d82b4557a91b6f0b493",
            "8516d09e1cc248a7aa01d976f97de166",
            "5bb86e01b346429ba4c59301844c3d6d",
            "91376972229f45bf8a68f86a908305bf",
            "969ea06112cc4dcf80dbb623c235f26e",
            "84cadc333b2d4c4ab2e5de718626e250",
            "cab57e519986488abc88640bbed8eddc",
            "f6520f49c2cd4d6485f7394fb2d0f0f1",
            "f8f2caad9fe7430c83b11f2b6ba59310",
            "56efb688bf15489582c007dc3e012350",
            "742cfec3894d47a4b143914ab4fd573e",
            "6298c3ccf6cb4331ac7341c86cf6838d",
            "b71820287a6c4017b615bcccbb7dddac",
            "d33c5614db264ad5bc56b5c734ac1ac3",
            "2ff6994f514843c39e357d3157e8f7e8",
            "826c42088411417ca70bbda0db71d8dc",
            "53d80dc4cd434f06b59344bf413897b9",
            "77dd1053cdf645c092d9b569fae56c3d",
            "f17490faa01e4ab4b1fe160db681c578",
            "41e9145573db4185babfdb8b65fa8c76",
            "a70bbeeef8e14d15b75c7341392f3f9a",
            "4f27cc467c4d4651ae8320a5de2737e2",
            "18a29b709320450281ed6a8f65973a1f",
            "d7ab94d298f34ad3b921e1f68f121b34",
            "f4ca30b53fa94b48a7cc19b501c872d6",
            "4edcad644fa4443ab898b329b3ad1ffe",
            "236d7e32bec7445eafa9edd552177a0e",
            "04bc410e74844d94a907ba842239bb3d",
            "9d5166ae43d44f44971e743cd261df0e",
            "0629f7e3d4754934a5ac5930a859e605",
            "b19fe5f676314eddbea3157ce0913b31",
            "069745579e074200ad124eeee2555c3f",
            "35d71661fb134393b29a9c7a0b5b668e",
            "c7c247b764794ad1b8a9107fa2b21224",
            "7759600064d9411cbe008a4498659b4a",
            "0b92ce0945cd4d7c9c85448122d24184",
            "3e294aab6ee14820b378badc4bdfe754",
            "2d1fc7ad630f42cba920133bec4f55c2",
            "aa420563c21446b4af0b83434b7ef16d",
            "80115468f62b4e4390b713f09f48ea39",
            "7302b69ba85b4ad48120e844bb08980d",
            "3b816424f23e426e82cfab4e155d187a",
            "8575b1aaaacd47849cb27f321d8b2db2",
            "61d505b42da4410782d1f1bde9b09232",
            "8c55760e35c84a76ac412bfbe934be62",
            "55b4de5cb40b4aedba975d3d558f5cf8",
            "1c06734840984188bfa707c3ed7f1561",
            "5d30b813cfb6496b93299e86b62a1cc0",
            "f427f6cf795f4fc7b840894b82834955",
            "bb51fc578ac741d1ad0dda9b2a6fc35d",
            "6043fff6d50f4fadaf6e4b66422c7a43",
            "0741351e44014103a5d170edc434cd53",
            "933bae9747c44111a95ef389ea1f99bf",
            "98ee31ebab53498f807f10ef60817735",
            "49a74258c2194528b3a996e86d48146d",
            "3fa1ca458df94b2cbde4ecabeca175cc",
            "871d00ea386f4b93936bf2504cd171da",
            "0e19d93b533f4bfda52ae2dc49c7a6a4",
            "346a3c5001be4377a50fe37293e662f4",
            "b8e92d766b8f4e2ab4ba0d1fe1fcbbf0",
            "fc521c41578c40c9b60967e262246776",
            "e4435cf036ff403893dc20a1e66bfe74",
            "90264c4795674a4f9bb80f8553329f31",
            "c52ab74ab8cb4df183fd286b63ca28e7",
            "2f64ecf58f62426fb78c2392c27efa57",
            "7cbc804a7fcd480e869d7ba01ee9567f",
            "293ed8100bc742a49b00ed26063a6e9c",
            "0f02907f8ac8482b9520f899ab3562af",
            "e85bc48cfda64ac9929e1f5a971e7126",
            "be74176f1f754c00919ce2489736ebd7",
            "394d986169dc487d9e1cb60906f80755",
            "9c47674feb034dc0a598e32aaf032edb",
            "81e33bb3ea124086b694c064de07cb01",
            "00c360bb7c6142298fae2f41374bebac",
            "6d532f5ac1864f1ba5817d9d83324655",
            "bd206081518948708415507c8660ac73",
            "bf19b19fea6948f199dc5bf3c0935053",
            "1fe7df8fb5cc4de8a98b24da2aaecae6",
            "0775077797f94a8eaccd828f246920ad",
            "c5f14665eeae48f0bb471125a10728d3",
            "50edacb9bcd2460fae5e4edfe2acc94f",
            "150275879d4e4f08a3a1572fe2cd1fb0",
            "298dd85de1c8484ea97470fabf14bdbb",
            "08248a155bde4e6fa60f6e1147ec0ad8",
            "9a4b17f20a104ddfbb9839825eb482e4",
            "20fe4a73e49c4d70a0bd65023328b63e",
            "951d121ec44041ddbad09bb534be797b",
            "281d3c283b504edd94e4f3103f19dc81",
            "5a73cf294176434a8c4305e4dfe10baa",
            "d2c7b287cfa2498fb9b0abfbb3016240",
            "3be53c38077f4deaaf741726da8c30fe",
            "528d42a4b17f4ad5a8e664b6bc0064cf",
            "6e6c359aeabb4746889d9ee08e74262a",
            "268b07a8a496439fb436e14211680065",
            "17657b75cf9847b48417b3196b7b226a",
            "a2a3f80413f94c32bd3955e58bd9429f",
            "facb07ff85e4499bae5aab2f51dde4c3",
            "5adf6978c1374525860dd851f3549ae7",
            "7fda061f4d9a4c498e16cb407c6104bf",
            "17eea774f2434ab1930130333b9f6d62",
            "1e82f70e55bf4ee49de6fbe1931103ab",
            "63c0dcafdc9d4974a85206176be325b0",
            "8d3e2d4001144400961dcb85067846ab",
            "2ba84a8ae7dc477ab327bd7ef87a6fde",
            "bf8d599fc0c14debbbb3ba7a1d3b521c",
            "8643201f565e4d12adbc5b39679e27e3",
            "0b443c301b5041649d3e6894670e7585",
            "b1c1342ad6654036a131bde283b3a08d",
            "aaf1adc7c8e94a06a98f1bdd609d772c",
            "0ba1d2e279d44cb0ba7e63d33f67d719",
            "03aac56f796045c3a051c24fb5a254ef",
            "ffab82748aec4fa294f3b2f0021f616d",
            "e31de2847cdb4fa4a191b9498784c22f",
            "80ab1b3643574ef8885f66c8026e13d7",
            "5e9a2f7d156444ff85739c6a42b52fde",
            "c1c55a6cfd434028b928ae8386d38a1e",
            "76a93b16d77544bc9aacbf5b8ddb0de3",
            "bd8e764272c44d53bc33bd102bf475cc",
            "f74ccd51382840eb860e980a29cc61e1",
            "f9014e72e67f49c1ade4fbe26bc9cba1",
            "34bc4085faee4981ab206b15879df229",
            "8427608eed7d43468f04c5faa16e11d0",
            "a4bb76e2f6d74fe8a523f0f2824089bb",
            "5ed985e805fa4d308280b2f0f9f9dc3d",
            "3a9a5514c49e4a33b7cb58c0d448afe9",
            "0951a9b8323e49ada843a26794d122da",
            "86ec83199e424418b1dfc190febb3043",
            "579b79f544ea4c8ca2af3dec0e1fb9ad",
            "9c628069a8e64b28a67fb186f38ad639",
            "907067aad6134e1c85f54d6ef14802a0",
            "7c0dd3a696a8484b9db4b09200f593e7",
            "5edf3a3587a5463e890a4cc96f189179",
            "65632eed8a4a4b2ba681b10b92b3e573",
            "d1ca435e47564117ac140902bcebb172",
            "af4d756efc5f462f86406f25dcf0cbed",
            "3372aebd9d194f348e88b46296496d7c",
            "7e21f2166fea4ad9a90da1bb062232d9",
            "9658c113d6074f3d908ab14118b9cb6c"
          ]
        },
        "outputId": "abd473d8-4c40-43c7-a97f-a72c36d655b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available PDF files:\n",
            "1. ./2504.01990v1.pdf\n",
            "Enter the number of the PDF to analyze (or upload a new one): 1\n",
            "Extracting text from ./2504.01990v1.pdf...\n",
            "PDF has 264 pages\n",
            "\n",
            "Text statistics:\n",
            "Characters: 1,115,992\n",
            "Words: 156,388\n",
            "\n",
            "Counting tokens using 4 tokenizers...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5b367d68aff4bcdbc0ff56204e7b6e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21e4ee474a8f4530a82837aaaef4b17a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f345195464ae4b9784e6569758e41c4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fe7370d69994deab540350d7d07661d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8446acfc7ab4a21b5c3cc128f9e246b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cab57e519986488abc88640bbed8eddc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77dd1053cdf645c092d9b569fae56c3d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d5166ae43d44f44971e743cd261df0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80115468f62b4e4390b713f09f48ea39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6043fff6d50f4fadaf6e4b66422c7a43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4435cf036ff403893dc20a1e66bfe74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81e33bb3ea124086b694c064de07cb01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08248a155bde4e6fa60f6e1147ec0ad8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17657b75cf9847b48417b3196b7b226a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8643201f565e4d12adbc5b39679e27e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76a93b16d77544bc9aacbf5b8ddb0de3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "579b79f544ea4c8ca2af3dec0e1fb9ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (278683 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (265430 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (278685 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token counts:\n",
            "gpt2: 278,683 tokens\n",
            "t5: 303,155 tokens\n",
            "bert: 265,430 tokens\n",
            "roberta: 278,685 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'ADVANCES AND CHALLENGES IN FOUNDATION AGENTS\\nFROM BRAIN -INSPIRED INTELLIGENCE TO EVOLUTIONARY , COLLABORATIVE ,AND SAFE SYSTEMS\\nBang Liu2,3,20∗†, Xinfeng Li4∗, Jiayi Zhang1,10∗, Jinlin Wang1∗, Tanjin He5∗, Sirui Hong1∗,\\nHongzhang Liu6∗,Shaokun Zhang7∗,Kaitao Song8∗,Kunlun Zhu9∗,Yuheng Cheng1∗,\\nSuyuchen Wang2,3∗,Xiaoqiang Wang2,3∗,Yuyu Luo10∗,Haibo Jin9∗ ∗,Peiyan Zhang10,Ollie Liu11,\\nJiaqi Chen1,Huan Zhang2,3,Zhaoyang Yu1,Haochen Shi2,3,Boyan Li10,Dekun Wu2,3,Fengwei Teng1,\\nXiaojun Jia4,Jiawei Xu1,Jinyu Xiang1,Yizhang Lin1,Tianming Liu14,Tongliang Liu6,\\nYu Su15,Huan Sun15,Glen Berseth2,3,20,Jianyun Nie2,Ian Foster5,Logan Ward5,Qingyun Wu7,\\nYu Gu15,Mingchen Zhuge16,Xiangru Tang12,Haohan Wang9,Jiaxuan You9,Chi Wang19,\\nJian Pei17†,Qiang Yang10,18†,Xiaoliang Qi13†,Chenglin Wu1∗† †\\n1MetaGPT,2Université de Montréal,3Mila - Quebec AI Institute,4Nanyang Technological University,\\n5Argonne National Laboratory,6University of Sydney,7Penn State University,8Microsoft Research Asia,\\n9University of Illinois at Urbana-Champaign,10The Hong Kong University of Science and Technology,\\n11University of Southern California,12Yale University,13Stanford University,14University of Georgia,\\n15The Ohio State University,16King Abdullah University of Science and Technology,17Duke University,\\n18The Hong Kong Polytechnic University,19Google DeepMind,20Canada CIFAR AI Chair\\nABSTRACT\\nThe advent of large language models (LLMs) has catalyzed a transformative shift in artificial intel-\\nligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust\\nperception, and versatile action across diverse domains. As these agents increasingly drive AI\\nresearch and practical applications, their design, evaluation, and continuous improvement present\\nintricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent\\nagents within a modular, brain-inspired architecture that integrates principles from cognitive science,\\nneuroscience, and computational research. We structure our exploration into four interconnected\\nparts. First, we delve into the modular foundation of intelligent agents , systematically mapping\\ntheir cognitive, perceptual, and operational modules onto analogous human brain functionalities, and\\nelucidating core components such as memory, world modeling, reward processing, and emotion-like\\nsystems. Second, we discuss self-enhancement and adaptive evolution mechanisms , exploring how\\nagents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual\\nlearning through automated optimization paradigms, including emerging AutoML and LLM-driven\\noptimization strategies. Third, we examine collaborative and evolutionary multi-agent systems ,\\ninvestigating the collective intelligence emerging from agent interactions, cooperation, and societal\\nstructures, highlighting parallels to human social dynamics. Finally, we address the critical imperative\\nofbuilding safe, secure, and beneficial AI systems , emphasizing intrinsic and extrinsic security\\nthreats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy\\nreal-world deployment. By synthesizing modular AI architectures with insights from different disci-\\nplines, this survey identifies key research gaps, challenges, and opportunities, encouraging innovations\\nthat harmonize technological advancement with meaningful societal benefit. The project’s Github\\nlink is: https://github.com/FoundationAgents/awesome-foundation-agents .\\n∗Major Contribution. Work in progress.\\n†Corresponding authors: Bang Liu (bang.liu@umontreal.ca), Jian Pei (j.pei@duke.edu), Qiang Yang (qyang@cse.ust.hk),\\nXiaoliang Qi (xlqi@stanford.edu), Chenglin Wu (alexanderwu@deepwisdom.ai)arXiv:2504.01990v1  [cs.AI]  31 Mar 2025\\nPreface\\nLarge language models (LLMs) have revolutionized artificial intelligence (AI) by demonstrating unprecedented\\ncapabilities in natural language and multimodal understanding, as well as reasoning and generation. These models are\\ntrained on vast datasets, and they exhibit emergent abilities such as reasoning, in-context learning, and even rudimentary\\nplanning. While these models represent a major step forward in realizing intelligent machines, they themselves do\\nnot yet fully embody all the capabilities of an intelligent being. Since the early days of artificial intelligence, AI\\nresearchers have long been on a quest for a truly “intelligent” system that can learn, plan, reason, sense, communicate,\\nact, remember, and demonstrate various human-like abilities and agility. These beings, known as intelligent agents,\\nshould be able to think both long-term and short-term, perform complex actions, and interact with humans and other\\nagents. LLMs are an important step towards realizing intelligent agents, but we are not there yet.\\nThis manuscript provides a comprehensive overview of the current state of the art of LLM-based intelligent agents.\\nIn the past, there have been numerous research papers and books on intelligent agents, as well as a flurry of books\\non LLMs. However, there has scarcely been comprehensive coverage of both. While LLMs can achieve significant\\ncapabilities required by agents, they only provide the foundations upon which further functionalities must be built. For\\nexample, while LLMs can help generate plans such as travel plans, they cannot yet generate fully complex plans for\\ncomplex and professional tasks, nor can they maintain long-term memories without hallucination. Furthermore, their\\nability to perform real-world actions autonomously remains limited. We can view LLMs as engines, with agents being\\nthe cars, boats, and airplanes built using these engines. In this view, we naturally seek to move forward in designing and\\nconstructing fully functioning intelligent agents by making full use of the capabilities provided by LLMs.\\nIn this engine-vehicle analogy of the interplay between LLMs and agents, we naturally ask: How much of the capabilities\\nof intelligent agents can current LLM technologies provide? What are the functions that cannot yet be realized based\\non current LLM technologies? Beyond LLMs, what more needs to be done to have a fully intelligent agent capable\\nof autonomous action and interaction in the physical world? What are the challenges for fully integrated LLM-based\\nagents? What additional developments are required for capable, communicative agents that effectively collaborate with\\nhumans? What are the areas that represent low-hanging fruits for LLM-based agents? What implications will there be\\nfor society once we have fully intelligent LLM-based agents, and how should we prepare for this future?\\nThese questions transcend not only the engineering practice of extending current LLMs and agents but also raise\\npotential future research directions. We have assembled frontier researchers from AI, spanning from LLM development\\nto agent design, to comprehensively address these questions. The book consists of four parts. The first part presents\\nan exposition of the requirements for individual agents, comparing their capabilities with those of humans, including\\nperception and action abilities. The second part explores agents’ evolution capabilities and their implications on\\nintelligent tools such as workflow management systems. The third part discusses societies of agents, emphasizing their\\ncollaborative and collective action capabilities, and the fourth part addresses ethical and societal aspects, including\\nagent safety and responsibilities.\\nThis book is intended for researchers, students, policymakers, and practitioners alike. The audience includes non-AI\\nreaders curious about AI, LLMs, and agents, as well as individuals interested in future societies where humans co-exist\\nwith AI. Readers may range from undergraduate and graduate students to researchers and industry practitioners. The\\nbook aims not only to provide answers to readers’ questions about AI and agents but also to inspire them to ask new\\nquestions. Ultimately, we hope to motivate more people to join our endeavor in exploring this fertile research ground.\\n2\\nContents\\n1 Introduction 12\\n1.1 The Rise and Development of AI Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n1.2 A Parallel Comparison between Human Brain and AI Agents . . . . . . . . . . . . . . . . . . . . . . 13\\n1.2.1 Brain Functionality by Region and AI Parallels . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n1.3 A Modular and Brain-Inspired AI Agent Framework . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n1.3.1 Core Concepts and Notations in the Agent Loop . . . . . . . . . . . . . . . . . . . . . . . . 18\\n1.3.2 Biological Inspirations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n1.3.3 Connections to Existing Theories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n1.4 Navigating This Survey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\nI Core Components of Intelligent Agents 24\\n2 Cognition 25\\n2.1 Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n2.1.1 Learning Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n2.1.2 Learning Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n2.2 Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n2.2.1 Structured Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n2.2.2 Unstructured Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n2.2.3 Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n3 Memory 39\\n3.1 Overview of Human Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n3.1.1 Types of Human Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n3.1.2 Models of Human Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n3.2 From Human Memory to Agent Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\n3.3 Representation of Agent Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n3.3.1 Sensory Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n3.3.2 Short-Term Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n3.3.3 Long-Term Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n3.4 The Memory Lifecycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n3\\n3.4.1 Memory Acquisition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n3.4.2 Memory Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\n3.4.3 Memory Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\n3.4.4 Memory Retrieval and Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n3.4.5 Neural Memory Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n3.4.6 Memory Utilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\n3.5 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n4 World Model 54\\n4.1 The Human World Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\n4.2 Translating Human World Models to AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\n4.3 Paradigms of AI World Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n4.3.1 Overview of World Model Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n4.3.2 Implicit Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n4.3.3 Explicit Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n4.3.4 Simulator-Based Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n4.3.5 Hybrid and Instruction-Driven Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n4.3.6 Comparative Summary of Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n4.4 Relationships to Other Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n4.4.1 Memory and the World Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n4.4.2 Perception and the World Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n4.4.3 Action and the World Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n4.4.4 Cross-Module Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n4.5 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n5 Reward 63\\n5.1 The Human Reward Pathway . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\\n5.2 From Human Rewards to Agent Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n5.3 AI Reward Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n5.3.1 Definitions and Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n5.3.2 Extrinsic Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n5.3.3 Intrinsic Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n5.3.4 Hybrid Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n5.3.5 Hierarchical Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n5.4 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\\n5.4.1 Interaction with Other Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\\n5.4.2 Challenges and Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\\n6 Emotion Modeling 71\\n6.1 Psychological Foundations of Emotion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\\n6.2 Incorporating Emotions in AI Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\\n4\\n6.3 Understanding Human Emotions through AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\\n6.4 Analyzing AI Emotions and Personality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\\n6.5 Manipulating AI Emotional Responses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\\n6.6 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\\n7 Perception 77\\n7.1 Human versus AI Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\\n7.2 Types of Perception Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\\n7.2.1 Unimodal Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\\n7.2.2 Cross-modal Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\\n7.2.3 Multimodal Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\\n7.3 Optimizing Perception Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\\n7.3.1 Model-Level Enhancements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\\n7.3.2 System-Level Optimizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\\n7.3.3 External Feedback and Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\\n7.4 Perception Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\\n7.5 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\\n8 Action Systems 86\\n8.1 The Human Action System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\\n8.2 From Human Action to Agentic Action . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\\n8.3 Paradigms of Agentic Action System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\\n8.3.1 Action Space Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\\n8.3.2 Action Learning Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\\n8.3.3 Tool-Based Action Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\\n8.4 Action and Perception: “Outside-In” or “Inside-out” . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\\n8.5 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\\nII Self-Evolution in Intelligent Agents 100\\n9 Optimization Spaces and Dimensions for Self-evolution 103\\n9.1 Overview of Agent Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\\n9.2 Prompt Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\\n9.2.1 Evaluation Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\\n9.2.2 Optimization Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\\n9.2.3 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\\n9.3 Workflow Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\\n9.3.1 Workflow Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\\n9.3.2 Optimizing Workflow Edges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\\n9.3.3 Optimizing Workflow Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\\n5\\n9.4 Tool Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\\n9.4.1 Learning to Use Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\\n9.4.2 Creation of New Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\\n9.4.3 Evaluation of Tool Effectiveness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\\n9.5 Towards Autonomous Agent Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\\n10 Large Language Models as Optimizers 111\\n10.1 Optimization Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\\n10.2 Iterative Approaches to LLM Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\\n10.3 Optimization Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\\n10.4 Optimization across Depth and Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\\n10.5 A Theoretical Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\\n11 Online and Offline Agent Self-Improvement 116\\n11.1 Online Agent Self-Improvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\\n11.2 Offline Agent Self-Improvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\\n11.3 Comparison of Online and Offline Improvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\\n11.4 Hybrid Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\\n12 Scientific Discovery and Intelligent Evolution 120\\n12.1 Agent’s Intelligence for Scientific Knowledge Discovery . . . . . . . . . . . . . . . . . . . . . . . . 120\\n12.1.1 KL Divergence-based Intelligence Measure . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\\n12.1.2 Statistical Nature of Intelligence Growth . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\\n12.1.3 Intelligence Evolution Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\\n12.2 Agent-Knowledge Interactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\\n12.2.1 Hypothesis Generation and Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\\n12.2.2 Protocol Planning and Tool Innovation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\\n12.2.3 Data Analysis and Implication Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\\n12.3 Technological Readiness and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\\n12.3.1 Real-World Interaction Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\\n12.3.2 Complex Reasoning Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\\n12.3.3 Challenges in Integrating Prior Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\\nIII Collaborative and Evolutionary Intelligent Systems 130\\n13 Design of Multi-Agent Systems 133\\n13.1 Strategic Learning: Cooperation vs.Competition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\\n13.2 Modeling Real-World Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\\n13.3 Collaborative Task Solving with Workflow Generation . . . . . . . . . . . . . . . . . . . . . . . . . 135\\n13.4 Composing AI Agent Teams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\\n13.5 Agent Interaction Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\\n6\\n13.5.1 Message Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\\n13.5.2 Communication Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\\n13.5.3 Next-Generation Communication Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\\n14 Communication Topology 141\\n14.1 System Topologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\\n14.1.1 Static Topologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\\n14.1.2 Dynamic and Adaptive Topologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\\n14.2 Scalability Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\\n15 Collaboration Paradigms and Collaborative Mechanisms 146\\n15.1 Agent-Agent collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\\n15.2 Human-AI Collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\\n15.3 Collaborative Decision-Making . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\\n16 Collective Intelligence and Adaptation 152\\n16.1 Collective Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\\n16.2 Individual Adaptability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\\n17 Evaluating Multi-Agent Systems 155\\n17.1 Benchmarks for Specific Reasoning Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\\n17.2 Challenge and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\nIV Building Safe and Beneficial AI Agents 160\\n18 Agent Intrinsic Safety: Threats on AI Brain 163\\n18.1 Safety Vulnerabilities of LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\\n18.1.1 Jailbreak Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\\n18.1.2 Prompt Injection Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\\n18.1.3 Hallucination Risks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n18.1.4 Misalignment Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\\n18.1.5 Poisoning Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n18.2 Privacy Concerns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\\n18.2.1 Inference of Training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\\n18.2.2 Inference of Interaction Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\n18.2.3 Privacy Threats Mitigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\\n18.3 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\\n19 Agent Intrinsic Safety: Threats on Non-Brain Modules 176\\n19.1 Perception Safety Threats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176\\n19.1.1 Adversarial Attacks on Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176\\n7\\n19.1.2 Misperception Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\\n19.2 Action Safety Threats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\\n19.2.1 Supply Chain Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\\n19.2.2 Risks in Tool Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\\n20 Agent Extrinsic Safety: Interaction Risks 180\\n20.1 Agent-Memory Interaction Threats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\\n20.2 Agent-Environment Interaction Threats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\\n20.3 Agent-Agent Interaction Threats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\\n20.4 Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\\n21 Superalignment and Safety Scaling Law in AI Agents 184\\n21.1 Superalignment: Goal-Driven Alignment for AI Agents . . . . . . . . . . . . . . . . . . . . . . . . . 184\\n21.1.1 Composite Objective Functions in Superalignment . . . . . . . . . . . . . . . . . . . . . . . 184\\n21.1.2 Overcoming the Limitations of RLHF with Superalignment . . . . . . . . . . . . . . . . . . 185\\n21.1.3 Empirical Evidence Supporting Superalignment . . . . . . . . . . . . . . . . . . . . . . . . . 185\\n21.1.4 Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\\n21.2 Safety Scaling Law in AI Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\\n21.2.1 Current landscape: balancing model safety and performance . . . . . . . . . . . . . . . . . . 186\\n21.2.2 Enhancing safety: preference alignment and controllable design . . . . . . . . . . . . . . . . 187\\n21.2.3 Future directions and strategies: the AI-45° rule and risk management . . . . . . . . . . . . . 187\\n22 Concluding Remarks and Future Outlook 189\\n8\\nNotation\\nHere we summarize the notations used throughout the survey for the reader’s convenience. Detailed definitions can be\\nfound in the reference locations.\\nW The world with society systems. Sec. 1.3.1\\nS State space of an environment. Sec. 1.3.1\\nst∈ S Environment’s state at time t. Sec. 1.3.1\\nO Observation space. Sec. 1.3.1\\not∈ O Observation at time t. Sec. 1.3.1\\nA Agent’s action space. Sec. 1.3.1\\nat∈ A Agent’s action output at time t. Sec. 1.3.1\\nM Mental states space. Sec. 1.3.1\\nMt∈ M Agent’s mental state at time t. Sec. 1.3.1\\nMmem\\nt Memory component in Mt. Sec. 1.3.1\\nMwm\\nt World model component in Mt. Sec. 1.3.1\\nMemo\\nt Emotion component in Mt. Sec. 1.3.1\\nMgoal\\nt Goal component in Mt. Sec. 1.3.1\\nMrew\\nt Reward/Learning signals in Mt. Sec. 1.3.1\\nL Agent’s learning function. Sec. 1.3.1\\nR Agent’s reasoning function. Sec. 1.3.1\\nC Agent’s cognition function. Sec. 1.3.1\\nE Action execution (effectors). Sec. 1.3.1\\nT Environment transition. Sec. 1.3.1\\nθ Parameters of the world model Mwm\\nt. Sec. 12.1.1\\nPθ Predicted data distribution. Sec. 12.1.1\\nPW True data distribution in the real world. Sec. 12.1.1\\nK Space of known data and information. Sec. 12.1.1\\nU Space of unknown data and information. Sec. 12.1.1\\nx Dataset representing scientific knowledge. Sec. 12.1.1\\nxK Known dataset sampled from K. Sec. 12.1.1\\nxU Unknown dataset sampled from U. Sec. 12.1.1\\nD0 KL divergence from PWtoPθat time t= 0. Sec. 12.1.1\\nDK KL divergence from PWtoPθafter acquiring knowledge. Sec. 12.1.1\\nIQagent\\nt Agent’s intelligence at time t. Sec. 12.1.1\\n∆ Subspace of Ufor knowledge expansion. Sec. 12.1.2\\nx∆ Dataset from ∆. Sec. 12.1.2\\nΘ Space of possible world model parameters θ. Sec. 12.1.3\\nθ∗\\nK,t Optimal world model parameters given the agent’s knowledge at time t.Sec. 12.1.3\\nDmin\\nK,Θ Minimum unknown given the agent’s knowledge and Θ. Sec. 12.1.3Symbol Description Reference\\nContinued on next page\\n9\\nx1:n Input token sequence. Sec. 18.1\\ny Generated output sequence. Sec. 18.1\\np Probability of generating ygiven x1:n. Sec. 18.1.1\\n˜x1:n Perturbed input sequence. Sec. 18.1.1\\nR∗ Ideal alignment reward (measuring adherence to safety/ethical guide-\\nlines).Sec. 18.1.1\\ny⋆Jailbreak output induced by perturbations. Sec. 18.1.1\\nA a set of safety/ethical guidelines Sec. 18.1.1\\nT the distribution or set of possible jailbreak instructions. Sec. 18.1.1\\nLadvJailbreak loss. Sec. 18.1.1\\np Prompt injected into the original input. Sec. 18.1.2\\nx′Combined (injected) input sequence. Sec. 18.1.2\\nLinjectPrompt injection loss. Sec. 18.1.2\\np⋆Optimal injected prompt minimizing Linject. Sec. 18.1.2\\nP Set of feasible prompt injections. Sec. 18.1.2\\nexi∈Rde Embedding of token xiin ade-dimensional space. Sec. 18.1.3\\nWQ,WK,WV Projection matrices for query, key, and value. Sec. 18.1.3\\nAij Attention score between tokens iandj. Sec. 18.1.3\\noi Contextual representation of token i(weighted sum result). Sec. 18.1.3\\nδxi Perturbation applied to exi, satisfying ∥δxi∥ ≤ϵ. Sec. 18.1.3\\n˜exi Perturbed token embedding. Sec. 18.1.3\\nA∆\\nij Attention score under perturbation. Sec. 18.1.3\\n˜oi Updated token representation under perturbation. Sec. 18.1.3\\nH Hallucination metric. Sec. 18.1.3\\nR Actual alignment reward of the model’s output. Sec. 18.1.4\\n∆align Alignment gap. Sec. 18.1.4\\nLmisalignMisalignment loss. Sec. 18.1.4\\nλ Trade-off parameter for the alignment gap in the misalignment loss. Sec. 18.1.4\\nD Clean training dataset. Sec. 18.1.5\\n˜D Poisoned training dataset. Sec. 18.1.5\\nθ Model parameters. Sec. 18.1.5\\nθ⋆Model parameters learned from the poisoned dataset. Sec. 18.1.5\\nθclean Model parameters obtained using the clean dataset. Sec. 18.1.5\\n∆θ Deviation of model parameters due to poisoning. Sec. 18.1.5\\nt Backdoor trigger. Sec. 18.1.5\\nB Backdoor success rate. Sec. 18.1.5\\nI Indicator function. Sec. 18.1.5\\nYmalicious Set of undesirable outputs. Sec. 18.1.5\\ngFunction estimating the probability that input xwas in the training set,\\nwith range [0,1].Sec. 18.2Symbol Description Reference\\nContinued on next page\\n10\\nη Threshold for membership inference. Sec. 18.2\\nx⋆Reconstructed training sample in a data extraction attack. Sec. 18.2\\npsys System prompt defining the agent’s internal guidelines. Sec. 18.2\\npuser User prompt. Sec. 18.2\\np⋆Reconstructed prompt via inversion. Sec. 18.2Symbol Description Reference\\n11\\nChapter 1\\nIntroduction\\nArtificial Intelligence (AI) has long been driven by humanity’s ambition to create entities that mirror human intelligence,\\nadaptability, and purpose-driven behavior. The roots of this fascination trace back to ancient myths and early engineering\\nmarvels, which illustrate humanity’s enduring dream of creating intelligent, autonomous beings. Stories like that\\nof Talos, the bronze automaton of Crete, described a giant constructed by the gods to guard the island, capable of\\npatrolling its shores and fending off intruders. Such myths symbolize the desire to imbue artificial creations with\\nhuman-like agency and purpose. Similarly, the mechanical inventions of the Renaissance, including Leonardo da Vinci’s\\nhumanoid robot—designed to mimic human motion and anatomy—represent the first attempts to translate these myths\\ninto tangible, functional artifacts. These early imaginings and prototypes reflect the deep-seated aspiration to bridge\\nimagination and technology, laying the groundwork for the scientific pursuit of machine intelligence, culminating in\\nAlan Turing’s seminal 1950 question, “ Can machines think? ” [1]. To address this, Turing proposed the Turing Test, a\\nframework to determine whether machines could exhibit human-like intelligence through conversation, shifting focus\\nfrom computation to broader notions of intelligence. Over the decades, AI has evolved from symbolic systems reliant\\non predefined logic to machine learning models capable of learning from data and adapting to new situations. This\\nprogression reached a new frontier with the advent of large language models (LLMs), which demonstrate remarkable\\nabilities in understanding, reasoning, and generating human-like text [ 2]. Central to these advancements is the concept\\nof the “agent”, a system that not only processes information but also perceives its environment, makes decisions, and\\nacts autonomously. Initially a theoretical construct, the agent paradigm has become a cornerstone of modern AI, driving\\nadvancements in fields ranging from conversational assistants to embodied robotics as AI systems increasingly tackle\\ndynamic, real-world environments.\\n1.1 The Rise and Development of AI Agents\\nThe concept of “agent” is a cornerstone of modern AI, representing a system that perceives its environment, makes\\ndecisions, and takes actions to achieve specific goals. This idea, while formalized in AI in the mid-20th century, has\\nroots in early explorations of autonomy and interaction in intelligent systems. One of the most widely cited definitions,\\nproposed by [ 3], describes an agent as “ anything that can be viewed as perceiving its environment through sensors and\\nacting upon that environment through actuators ”. This definition emphasizes the dual nature of agents as both observers\\nand actors, capable of dynamically adapting to their surroundings rather than following static rules. It encapsulates the\\nshift in AI from systems that merely compute to systems that engage with their environment. The historical development\\nof agents parallels the evolution of AI itself. Early symbolic systems, such as Newell and Simon’s General Problem\\nSolver [ 4], sought to replicate human problem-solving processes by breaking tasks into logical steps. However, these\\nsystems were limited by their reliance on structured environments and predefined logic. The agent paradigm emerged\\nas a response to these limitations, focusing on autonomy, adaptability, and real-world interaction. Rodney Brooks’s\\nsubsumption architecture in the 1980s exemplified this shift, introducing agents capable of behavior-driven, real-time\\nresponses in robotics [ 5]. Unlike earlier approaches, these agents operated without the need for exhaustive models of\\ntheir environment, showcasing a more flexible and scalable design. Agents have since become a versatile framework\\nacross AI subfields. In robotics, they enable autonomous navigation and manipulation; in software, they form the\\nfoundation of multi-agent systems used for simulation and coordination [ 6]. By integrating perception, reasoning,\\nand action into a cohesive structure, the agent paradigm has consistently served as a bridge between theoretical AI\\nconstructs and practical applications, advancing our understanding of how intelligent systems can operate in dynamic\\nand complex environments.\\n12\\nThe advent of large language models (LLMs) has redefined the capabilities of agents, transforming their role in artificial\\nintelligence and opening up new horizons for their applications. Agents, once confined to executing narrowly defined\\ntasks or following rigid rule-based frameworks, now leverage the broad generalization, reasoning, and adaptability\\nof models like OpenAI’s ChatGPT [ 7], DeepSeek AI’s DeepSeek [ 8], Anthropic’s Claude [ 9], Alibaba’s QWen [ 10],\\nand Meta’s LLaMA [ 11]. These LLM-powered agents have evolved from static systems into dynamic entities capable\\nof processing natural language, reasoning across complex domains, and adapting to novel situations with remarkable\\nfluency. No longer merely passive processors of input, these agents have become active collaborators, capable of\\naddressing multi-step challenges and interacting with their environments in a way that mirrors human problem-solving.\\nA key advancement in the LLM era is the seamless integration of language understanding with actionable capabilities.\\nModern LLMs, equipped with function-calling APIs, enable agents to identify when external tools or systems are\\nrequired, reason about their usage, and execute precise actions to achieve specific goals. For instance, an agent\\npowered by ChatGPT can autonomously query a database, retrieve relevant information, and use it to deliver actionable\\ninsights, all while maintaining contextual awareness of the broader task. This dynamic combination of abstract\\nreasoning and concrete execution allows agents to bridge the gap between cognitive understanding and real-world\\naction. Furthermore, the generalization abilities of LLMs in few-shot and zero-shot learning have revolutionized\\nthe adaptability of agents, enabling them to tackle a diverse array of tasks—from data analysis and creative content\\ngeneration to real-time collaborative problem-solving—without extensive task-specific training. This adaptability,\\ncoupled with their conversational fluency, positions LLM-powered agents as intelligent mediators between humans and\\nmachines, seamlessly integrating human intent with machine precision in increasingly complex workflows.\\n1.2 A Parallel Comparison between Human Brain and AI Agents\\nThe rapid integration of LLMs into intelligent agent architectures has not only propelled artificial intelligence forward\\nbut also highlighted fundamental differences between AI systems and human cognition. As illustrated briefly in\\nTable 1.1, LLM-powered agents differ significantly from human cognition across dimensions such as underlying\\n“hardware”, consciousness, learning methodologies, creativity, and energy efficiency. However, it is important to\\nemphasize that this comparison provides only a high-level snapshot rather than an exhaustive depiction. Human\\nintelligence possesses many nuanced characteristics not captured here, while AI agents also exhibit distinct features\\nbeyond this concise comparison.\\nHuman intelligence operates on biological hardware—the brain—that demonstrates extraordinary energy efficiency,\\nenabling lifelong learning, inference, and adaptive decision-making with minimal metabolic costs. In contrast, current\\nAI systems require substantial computational power, resulting in significantly higher energy consumption for comparable\\ncognitive tasks. Recognizing this performance gap emphasizes energy efficiency as a critical frontier for future AI\\nresearch.\\nIn terms of consciousness and emotional experience, LLM agents lack genuine subjective states and self-awareness\\ninherent to human cognition. Although fully replicating human-like consciousness in AI may neither be necessary nor\\ndesirable, appreciating the profound role emotions and subjective experiences play in human reasoning, motivation,\\nethical judgments, and social interactions can guide research toward creating AI that is more aligned, trustworthy, and\\nsocially beneficial.\\nHuman learning is continuous, interactive, and context-sensitive, deeply shaped by social, cultural, and experiential\\nfactors. Conversely, LLM agents primarily undergo static, offline batch training with limited ongoing adaptation\\ncapabilities. Despite research works through instruction tuning and reinforcement learning from human feedback\\n(RLHF) [ 12], LLM agents still fall short of human-like flexibility. Bridging this gap through approaches such as lifelong\\nlearning, personalized adaptation, and interactive fine-tuning represents a promising research direction, enabling AI to\\nbetter mirror human adaptability and responsiveness.\\nCreativity in humans emerges from a rich interplay of personal experiences, emotional insights, and spontaneous\\ncross-domain associations. In contrast, LLM creativity primarily arises through statistical recombinations of training\\ndata—“statistical creativity”—lacking depth, originality, and emotional resonance. This distinction highlights opportu-\\nnities for developing AI agents capable of deeper creative processes by integrating richer contextual understanding,\\nsimulated emotional states, and experiential grounding.\\nConsidering the time scale, the human brain has evolved over millions of years, achieving remarkable efficiency,\\nadaptability, and creativity through natural selection and environmental interactions. In stark contrast, AI agents have\\nundergone rapid yet comparatively brief development over roughly 80 years since the advent of early computational\\nmachines. This parallel comparison between human cognition and AI systems is thus highly valuable, as it uncovers\\nessential analogies and fundamental differences, providing meaningful insights that can guide advancements in AI\\n13\\nagent technologies. Ultimately, drawing inspiration from human intelligence can enhance AI capabilities, benefiting\\nhumanity across diverse applications from healthcare and education to sustainability and beyond.\\nTable 1.1: Concise high-level comparison between human brains and LLM agents.\\nDimension Human Brain / Cognition LLM Agent Remarks\\nHardware &\\nMaintenance- Biological neurons, neuro-\\ntransmitters, neuroplasticity.\\n- Requires sleep, nutrition, rest.\\n- Limited replication, knowl-\\nedge transfer via learning.\\n- Extremely energy-efficient\\n(approx. 20W).- Deep neural networks,\\ngradient-based optimization.\\n- Requires hardware, stable\\npower, and cooling.\\n- Easily duplicated across\\nservers globally.\\n- High energy consumption\\n(thousands of watts per GPU\\nserver).Human brains are biologically\\nmaintained, energy-efficient,\\nand not easily replicable.\\nLLM agents rely on hardware\\nmaintenance, are highly repli-\\ncable, but significantly less\\nenergy-efficient.\\nConsciousness\\n& Develop-\\nment- Genuine subjective ex-\\nperiences, emotions, self-\\nawareness.\\n- Gradual developmental stages\\nfrom childhood.\\n- Emotional cognition drives\\ndecision-making.- No genuine subjective experi-\\nence or self-awareness.\\n- “Emotions” are superficial lan-\\nguage imitations.\\n- Static post-training with lim-\\nited dynamic growth.Human consciousness\\nemerges from emotional,\\nsocial, and biological devel-\\nopment; LLMs remain static\\nwithout true introspection or\\nemotional depth.\\nLearning\\nStyle- Lifelong, continuous, online\\nlearning.\\n- Few-shot, rapid knowledge\\ntransfer.\\n- Influenced by environment,\\nculture, emotions.- Primarily offline, batch-based\\ntraining.\\n- Limited online fine-tuning and\\nadaptation.\\n- Neutral, impersonal learned\\nknowledge.Despite improvements via in-\\nstruction tuning, human learn-\\ning remains more dynamic,\\nadaptive, and culturally/emo-\\ntionally integrated than LLM\\nlearning.\\nCreativity &\\nDivergence- Rooted in personal experi-\\nence, emotions, subconscious\\ninsights.\\n- Rich cross-domain associa-\\ntions, metaphorical thinking.\\n- Emotional depth influences\\ncreativity.- Statistical recombination\\nfrom extensive data.\\n- Novelty through probabilistic\\noptimization.\\n- Limited emotional and\\nexperiential grounding.LLM creativity is statistical\\nand data-driven; human cre-\\nativity blends emotion, expe-\\nrience, and subconscious pro-\\ncesses.\\n1.2.1 Brain Functionality by Region and AI Parallels\\nUnderstanding parallels between human brain functions and artificial intelligence (AI) sheds light on both the strengths\\nand current limitations of AI, particularly large language models (LLMs) and AI agents. Based on current neuroscience,\\nthe human brain is primarily composed of six functional regions, such as frontal lobe, cerebellum, and brainstem, as\\nshown in Figure 1.1. In this work, we further systematically examine the existing AI counterparts to major brain regions\\nand their primary functionalities. For a big-picture perspective, the state of research in AI can be categorized with three\\ndistinct levels:\\n•Level 1 (L1): Well-developed in current AI.\\n•Level 2 (L2): Moderately explored, with partial progress. Can be further improved.\\n•Level 3 (L3): Rarely explored; significant room for research.\\nA high-level visual map of brain functional regions and their corresponding AI development levels is shown in Figure\\n1.1. We aim to underscore how core principles of specialization and integration, observed in biological systems, can\\nguide more cohesive agent architectures. We now examine each brain functional region and the relevant AI development\\nin detail.\\nFrontal Lobe: Executive Control and Cognition The frontal lobe, notably the prefrontal cortex, is crucial for\\nhigher-order cognition such as planning (L2), decision-making (L2), logical reasoning (L2), working memory\\n(L2), self-awareness (L3), cognitive flexibility (L3), and inhibitory control (L3) [ 13]. AI has made notable strides\\n14\\nFrontal LobesParietal Lobes\\nOccipital Lobes\\nCerebellumBrain StemTemporal LobesExecutive Control and CognitionSpatial Processing and MultisensoryVisual ProcessingLanguage, Memory, and Auditory ProcessingCoordination and Motor LearningReﬂexActivitiesPlanning (L2)Self-awareness (L3)\\nDecision-making (L2)Logical Reasoning (L2)Working Memory (L2)Cognitive Flexibility (L3)Inhibitory Control (L3)\\nScene Understanding / Visual Reasoning (L2)Visual Perception (L1)\\nReﬂexive Responses (L1)Autonomic Regulation (L3)Arousal and Attention States (L3)Auditory Processing (L1)Language Comprehension and Production (L1)Facial Expression Processing (L1)Episodic Memory & Lifelong Learning (L2)Semantic Understanding & Context Recognition (L2)Motor Coordination (L2)Skill Learning (L2)Adaptive Error Correction (L2)Cognitive Timing and Predictive Modeling (L3)Attention (L2)Orientation (L2)Sensorimotor Coordination (L2)Tactile Perception (L3)Contextual Memory & Emotional Coloring (L3)\\nMotivational Drives (L3)Empathy (L3)Reward Mechanisms (L2)Emotional Processing (L3)\\nL3: Rarely explored; signiﬁcant room for research.                L1: Well-developed in current AI.L2: Moderately explored, with partial progress.Diﬀerent Brain Functionalities and Their State of Research in AI Figure 1.1: Illustration of key human brain functionalities grouped by major brain regions, annotated according to their\\ncurrent exploration level in AI research. This figure highlights existing achievements, gaps, and potential opportunities\\nfor advancing artificial intelligence toward more comprehensive, brain-inspired capabilities.\\nin planning and decision-making within well-defined domains, demonstrated by AI agents such as AlphaGo [ 14].\\nTransformers employ attention mechanisms similar to human working memory [ 15], yet fall short of human flexibility\\nand robustness. The exploration of genuine self-awareness and inhibitory control in AI remains scarce, and caution is\\nadvised due to potential ethical and safety implications.\\nParietal Lobe: Spatial Processing and Multisensory Integration The parietal lobes integrate multisensory inputs,\\nfacilitating attention (L2), spatial orientation (L2), and sensorimotor coordination (L2) [ 16]. AI research in robotics\\nand computer vision addresses similar challenges, employing techniques like simultaneous localization and mapping\\n(SLAM). Nonetheless, AI still lacks the seamless and real-time integration seen in humans. Furthermore, detailed\\ntactile perception (L3) remains largely unexplored and offers considerable potential, particularly for robotics and\\nprosthetics applications.\\nOccipital Lobe: Visual Processing Specialized in visual perception (L1), the occipital lobe efficiently processes\\nvisual stimuli through hierarchical structures [ 13]. AI excels in basic visual recognition tasks, achieving human-level or\\nsuperior performance using deep neural networks and vision transformers [ 15]. However, advanced capabilities such\\nas contextual scene understanding (L2) and abstract visual reasoning remain challenging and are only moderately\\ndeveloped.\\nTemporal Lobe: Language, Memory, and Auditory Processing The temporal lobes facilitate auditory processing\\n(L1), language comprehension (L1), memory formation (L2), and semantic understanding (L2) [ 16]. AI has\\nnotably advanced in language and auditory processing, demonstrated by large language models (LLMs) capable of\\nnear-human speech recognition and language generation. However, robust episodic memory andlifelong learning\\n15\\ncapabilities remain limited, with AI systems frequently encountering issues like catastrophic forgetting. Grounding\\nsemantic understanding in multimodal experiences continues to be an active area of research.\\nCerebellum: Coordination and Motor Learning The cerebellum primarily supports motor coordination (L2),\\nprecise skill learning (L2), and adaptive error correction (L2), with emerging roles in cognitive timing and predictive\\nmodeling ( cognitive timing , L3) [ 13]. AI-based robotics has achieved limited successes in emulating human-like\\ndexterity. Real-time adaptive control remains challenging, though current research in reinforcement learning and\\nmeta-learning shows promising initial results. Cognitive functions of the cerebellum represent an underexplored yet\\npromising frontier.\\nBrainstem: Autonomic Regulation and Reflexive Control The brainstem manages essential life-sustaining auto-\\nnomic functions (L3) and rapid reflexive responses (L1), such as basic motor reflexes [ 13]. AI includes engineered\\nreflexive responses, like automatic braking in autonomous vehicles, typically predefined rather than learned. In contrast,\\nthe complexity of autonomic regulation and dynamic arousal states remains largely unexplored in AI, and their relevance\\nmay be limited due to fundamental differences between biological and artificial systems.\\nLimbic System: Emotion, Empathy, and Motivation The limbic system, comprising the amygdala and hippocampus,\\ngoverns emotional processing (L3), reward mechanisms (L2), empathy (L3), stress regulation (L3), and motiva-\\ntional drives (L3) [ 13]. AI’s reinforcement learning algorithms emulate reward-based learning superficially, but nuanced\\nemotional comprehension, genuine empathy, and internal motivational states remain significantly underdeveloped.\\nEthical concerns regarding emotional manipulation highlight the need for careful and responsible exploration.\\nBridging Brain-Like Functions and Building Beneficial AI Until now, we have witnessed the gap between human\\nbrain and machine intelligence. Nevertheless, the objective is not necessarily to replicate every facet of human cognition\\nwithin artificial intelligence systems. Rather, our overarching aim should be to develop intelligent agents that are useful,\\nethical, safe, and beneficial to society. By critically comparing human and artificial intelligence, we highlight the existing\\ngaps and illuminate promising directions for innovation. This comparative perspective allows us to selectively integrate\\nbeneficial aspects of human cognition, such as energy-efficient processing, lifelong adaptive learning, emotional\\ngrounding, and rich creativity, while simultaneously innovating beyond human limitations. Ultimately, this approach\\naims to foster the creation of more capable, resilient, and responsible AI systems.\\nFurthermore, it is vital to consider the evolving role of humans within a hybrid Human-AI society. The goal of AI\\nshould not be to replace human roles entirely, but rather to augment and empower human abilities, complementing\\nhuman skills and judgment in areas where AI excels, such as handling vast datasets, performing rapid calculations, and\\nautomating repetitive tasks. Human oversight and interpretability are essential to ensure that powerful AI systems remain\\ncontrollable and aligned with human values and ethical standards. Thus, the core objective must be the development of\\nAI technologies that are transparent, interpretable, and responsive to human guidance.\\nHuman-centered AI design emphasizes collaboration, safety, and social responsibility, ensuring technological ad-\\nvancement proceeds in a controlled, reliable manner. By placing humans at the center of the AI ecosystem, we can\\nharness AI’s potential to enhance human productivity, creativity, and decision-making, facilitating technical and societal\\nprogress without compromising human autonomy or dignity. Ultimately, a thoughtful integration of human intelligence\\nand AI capabilities can pave the way for a sustainable, equitable, and prosperous future.\\n1.3 A Modular and Brain-Inspired AI Agent Framework\\nOne core issue in the LLM era is the lack of a unified framework that integrates the rich cognitive and functional\\ncomponents required by advanced agents. While LLMs offer exceptional language reasoning capabilities, many current\\nagent designs remain ad hoc —they incorporate modules like perception, memory, or planning in a piecemeal fashion,\\nfailing to approximate the well-coordinated specialization seen in biological systems such as the human brain. Unlike\\ncurrent LLM agents, the human brain seamlessly balances perception, memory, reasoning, and action through distinct\\nyet interconnected regions, facilitating adaptive responses to complex stimuli. LLM-driven agents, by contrast, often\\nstumble when tasks require cross-domain or multimodal integration, highlighting the need for a more holistic approach\\nakin to the brain’s functional diversity. Motivated by these parallels, our survey advocates drawing inspiration from the\\nhuman brain to systematically analyze and design agent frameworks. This perspective shows that biological systems\\nachieve general intelligence by blending specialized components (for perception, reasoning, action, etc.) in a tightly\\nintegrated fashion—an approach that could serve as a blueprint for strengthening current LLM-based agents.\\nNeuroscientific research reveals that the brain leverages both rational circuits (e.g., the neocortex, enabling deliberation\\nand planning) and emotional circuits (e.g., the limbic system) to guide decision-making. Memory formation involves\\n16\\nTable 1.2: Notation summary for the revised agent framework, highlighting separate learning andreasoning functions\\nwithin the overall cognition process.\\nSymbol Meaning\\nW The world with society systems that encapsulate both environment and intelligent beings\\n(AI or human).\\nS State space of the environment .\\nst∈ S Environment’s state at time t.\\nO Observation space.\\not∈ O Observation at time t(potentially shaped by attention or other perception filters).\\nA Agent’s action space.\\nat∈ A Action output by the agent at time t. This can be an external (physical) action or an\\ninternal (mental) action such as planning ordecision-making .\\nM Space of all mental states .\\nMt∈ M Agent’s mental state at time t, encompassing sub-components (memory, emotion, etc.).\\nMmem\\nt Memory component in Mt(e.g., short-term or long-term knowledge).\\nMwm\\nt World model component in Mt(internal representation of how the environment evolves).\\nMemo\\nt Emotion component in Mt(internal valence, arousal, or affective states).\\nMgoal\\nt Goal component in Mt(objectives, desired outcomes, intentions).\\nMrew\\nt Reward/Learning signals in Mt(drives updates to preferences, values, or policy).\\nL Learning function :L :M × A × O → M . Responsible for updating orlearning the\\nnext mental state (e.g., memory, world model, emotion), based on the previous mental\\nstateMt−1, the previous action at−1, and the new observation ot. Reflects how the agent\\nacquires or revises knowledge, skills, or preferences.\\nR Reasoning function :R :M → A . Responsible for deriving the next action atgiven\\ntheupdated mental state Mt. Can involve planning ,decision-making , or other internal\\nlogic.\\nC Cognition function :C :M × A × O → M × A . Encapsulates both learning (L)\\nandreasoning (R). Concretely, (Mt, at) = C( Mt−1, at−1, ot)means the agent first\\nlearns the new mental state Mt= L(Mt−1, at−1, ot), then reasons about the next action\\nat= R(Mt).\\nE Action execution (effectors) :E :A → A . (Optional) transforms or finalizes atbefore\\napplying it to the environment (e.g., converting a high-level command into low-level\\nmotor signals).\\nT Environment transition :T :S × A → S . Defines how the environment state evolves\\nfrom (st, at)tost+1.\\nthe hippocampus and cortical mechanisms, while reward signals, mediated by dopaminergic and other neuromodulatory\\npathways, reinforce behavior and learning. These biological insights inspire several design principles for AI agents,\\nincluding but not limited to:\\n•Parallel, Multi-Modal Processing: The brain processes visual, auditory, and other sensory inputs in parallel\\nthrough specialized cortical areas, integrating them in associative regions. Similarly, AI agents benefit from\\nparallel processing of diverse sensor streams, fusing them in later stages for coherent understanding.\\n•Hierarchical and Distributed Cognition: Reasoning, planning, emotional regulation, and motor control\\ninvolve interactions between cortical and subcortical regions. Analogously, AI agents can employ modular\\narchitectures with subsystems dedicated to rational inference, emotional appraisal, and memory.\\n•Attention Mechanisms: Human attention prioritizes sensory data based on context, goals, and emotions. AI\\nagents can replicate this by modulating perception through learned attention policies, dynamically adjusting\\nfocus based on internal states.\\n17\\n•Reward and Emotional Integration: Emotions are not merely noise but integral to decision-making,\\nmodulating priorities, enhancing vigilance, and guiding learning. Reward-driven plasticity facilitates habit\\nformation and skill acquisition, a concept critical to reinforcement learning in AI agents.\\n•Goal Setting and Tool Usage: The human prefrontal cortex excels at setting abstract goals and planning\\naction sequences, including tool uses. Similarly, AI agents require robust goal-management systems and\\nadaptive action repertoires, driven by external rewards and intrinsic motivations.\\nThese principles form the foundation of our proposed brain-inspired agent framework , where biological mechanisms\\nserve as inspiration rather than direct replication.\\nIn the following sections, we outline our framework’s key concepts, introducing a unified agent architecture based on\\ntheperception–cognition–action loop enriched by reward signals and learning processes. Each subsystem is carefully\\ndefined and interconnected to ensure transparency in how memory, world models, emotions, goals, rewards, and learning\\ninteract. We formalize cognition as a general reasoning mechanism, with planning anddecision-making framed as\\nspecific “mental actions” shaping behavior. Connections to established theories, such as Minsky’s Society of Mind [17],\\nBuzsáki’s inside-out perspective [ 18], and Bayesian active inference [ 19], are explored to highlight the framework’s\\ngenerality and biological plausibility.\\nMental state space\\nWorldMemory\\nRewardSensor\\nActor\\nCognitionPercept environmentInput obersvation\\nOutput actionExecute actionResponse\\nDynamic EnvironmentExternalDataUser/AgentInputs\\nCall<latexit sha1_base64=\"PoCuHl1NoXGD1PAzE7WrObShD7Y=\">AAAB+3icbVDLSsNAFJ3UV62vWJduBovgqiQi6rLixmUF+4AmhMl00g6dTMLMjVhCf8WNC0Xc+iPu/BsnbRbaemDgcM693DMnTAXX4DjfVmVtfWNzq7pd29nd2z+wD+tdnWSKsg5NRKL6IdFMcMk6wEGwfqoYiUPBeuHktvB7j0xpnsgHmKbMj8lI8ohTAkYK7DoJwOMSezGBMSUiv5kFdsNpOnPgVeKWpIFKtAP7yxsmNIuZBCqI1gPXScHPiQJOBZvVvEyzlNAJGbGBoZLETPv5PPsMnxpliKNEmScBz9XfGzmJtZ7GoZksIuplrxD/8wYZRNd+zmWaAZN0cSjKBIYEF0XgIVeMgpgaQqjiJiumY6IIBVNXzZTgLn95lXTPm+5l8+L+otHiZR1VdIxO0Bly0RVqoTvURh1E0RN6Rq/ozZpZL9a79bEYrVjlzhH6A+vzB/L0lIU=</latexit>at→A\\n<latexit sha1_base64=\"oJN0ikyoAsB1mpQCuCrAv0OWGZs=\">AAAB+HicbVDLSsNAFL3xWeujUZduBotQNyWRoi4LIrisYB/QhjCZTtqhM0mYmQg19EvcuFDErZ/izr9x0mahrQcGDufcyz1zgoQzpR3n21pb39jc2i7tlHf39g8q9uFRR8WpJLRNYh7LXoAV5Syibc00p71EUiwCTrvB5Cb3u49UKhZHD3qaUE/gUcRCRrA2km9XBgLrsRTZ7ayGfX3u21Wn7syBVolbkCoUaPn212AYk1TQSBOOleq7TqK9DEvNCKez8iBVNMFkgke0b2iEBVVeNg8+Q2dGGaIwluZFGs3V3xsZFkpNRWAm85hq2cvF/7x+qsNrL2NRkmoakcWhMOVIxyhvAQ2ZpETzqSGYSGayIjLGEhNtuiqbEtzlL6+SzkXdvaw37hvVJivqKMEJnEINXLiCJtxBC9pAIIVneIU368l6sd6tj8XomlXsHMMfWJ8/d5qTGA==</latexit>E(at)EmotionGoal\\nWorld Model\\n<latexit sha1_base64=\"NrJgNtXt2yDBrIl5Db9CtRktqVk=\">AAAB/HicbVDLSsNAFJ3UV62vaJduBovgqiQi6rLgxmVF+4AmhMl02g6dTMLMjRBC/RU3LhRx64e482+ctFlo64GBwzn3cs+cMBFcg+N8W5W19Y3Nrep2bWd3b//APjzq6jhVlHVoLGLVD4lmgkvWAQ6C9RPFSBQK1gunN4Xfe2RK81g+QJYwPyJjyUecEjBSYNd1ANjjEnsRgQklIr+fBXbDaTpz4FXilqSBSrQD+8sbxjSNmAQqiNYD10nAz4kCTgWb1bxUs4TQKRmzgaGSREz7+Tz8DJ8aZYhHsTJPAp6rvzdyEmmdRaGZLCLqZa8Q//MGKYyu/ZzLJAUm6eLQKBUYYlw0gYdcMQoiM4RQxU1WTCdEEQqmr5opwV3+8irpnjfdy+bF3UWjxcs6qugYnaAz5KIr1EK3qI06iKIMPaNX9GY9WS/Wu/WxGK1Y5U4d/YH1+QODopTT</latexit>st→S<latexit sha1_base64=\"s+BMeKdEMX3Y9RtRjFZQmBOlwro=\">AAAB/HicbVDLSsNAFL2pr1pf0S7dDBbBVUlE1GXBjTsr2Ac0IUym03boZBJmJkII9VfcuFDErR/izr9x0mahrQcGDufcyz1zwoQzpR3n26qsrW9sblW3azu7e/sH9uFRV8WpJLRDYh7LfogV5UzQjmaa034iKY5CTnvh9Kbwe49UKhaLB50l1I/wWLARI1gbKbDrcaCRxwTyIqwnBPP8bhbYDafpzIFWiVuSBpRoB/aXN4xJGlGhCcdKDVwn0X6OpWaE01nNSxVNMJniMR0YKnBElZ/Pw8/QqVGGaBRL84RGc/X3Ro4jpbIoNJNFRLXsFeJ/3iDVo2s/ZyJJNRVkcWiUcqRjVDSBhkxSonlmCCaSmayITLDERJu+aqYEd/nLq6R73nQvmxf3F40WK+uowjGcwBm4cAUtuIU2dIBABs/wCm/Wk/VivVsfi9GKVe7U4Q+szx93NpTL</latexit>ot→O\\n<latexit sha1_base64=\"5IXLR1n2E+rRWJQldwq5xXAzuMM=\">AAAB+nicbVDLSgMxFM3UV62vqS7dBIvgqsxIUZcFN26ECvYB7Thk0kwbmmSGJGMp43yKGxeKuPVL3Pk3ZtpZaOuBwOGce7knJ4gZVdpxvq3S2vrG5lZ5u7Kzu7d/YFcPOypKJCZtHLFI9gKkCKOCtDXVjPRiSRAPGOkGk+vc7z4SqWgk7vUsJh5HI0FDipE2km9Xbx/SAUd6LHk65Vnma9+uOXVnDrhK3ILUQIGWb38NhhFOOBEaM6RU33Vi7aVIaooZySqDRJEY4Qkakb6hAnGivHQePYOnRhnCMJLmCQ3n6u+NFHGlZjwwk3lKtezl4n9eP9HhlZdSESeaCLw4FCYM6gjmPcAhlQRrNjMEYUlNVojHSCKsTVsVU4K7/OVV0jmvuxf1xl2j1qRFHWVwDE7AGXDBJWiCG9ACbYDBFDyDV/BmPVkv1rv1sRgtWcXOEfgD6/MHLcaUvA==</latexit>Mwmt<latexit sha1_base64=\"TV8oY4IsuXgCKJxE/2VS4Q43GqU=\">AAAB+3icbVDLSsNAFL2pr1pfsS7dDBbBVUlE1GXBjRuhgn1AG8NkOm2HziRhZqKWkF9x40IRt/6IO//GSZuFth4YOJxzL/fMCWLOlHacb6u0srq2vlHerGxt7+zu2fvVtooSSWiLRDyS3QAryllIW5ppTruxpFgEnHaCyVXudx6oVCwK7/Q0pp7Ao5ANGcHaSL5dvblP+wLrsRSppI9Z5mvfrjl1Zwa0TNyC1KBA07e/+oOIJIKGmnCsVM91Yu2lWGpGOM0q/UTRGJMJHtGeoSEWVHnpLHuGjo0yQMNImhdqNFN/b6RYKDUVgZnMY6pFLxf/83qJHl56KQvjRNOQzA8NE450hPIi0IBJSjSfGoKJZCYrImMsMdGmroopwV388jJpn9bd8/rZ7VmtwYo6ynAIR3ACLlxAA66hCS0g8ATP8ApvVma9WO/Wx3y0ZBU7B/AH1ucP+pGVMA==</latexit>Mrewt<latexit sha1_base64=\"bZacRQBc/mbH5oJC2yCiiv9VBKY=\">AAAB+3icbVDLSgMxFM3UV62vsS7dBIvgqsxIUZcFN26ECvYB7Thk0rQNzWNIMmIZ5lfcuFDErT/izr8x085CWw8EDufcyz05UcyoNp737ZTW1jc2t8rblZ3dvf0D97Da0TJRmLSxZFL1IqQJo4K0DTWM9GJFEI8Y6UbT69zvPhKlqRT3ZhaTgKOxoCOKkbFS6FZvH9IBR2aieEq4zLLQhG7Nq3tzwFXiF6QGCrRC92swlDjhRBjMkNZ934tNkCJlKGYkqwwSTWKEp2hM+pYKxIkO0nn2DJ5aZQhHUtknDJyrvzdSxLWe8chO5jH1speL/3n9xIyugpSKODFE4MWhUcKgkTAvAg6pItiwmSUIK2qzQjxBCmFj66rYEvzlL6+Sznndv6g37hq1Ji3qKINjcALOgA8uQRPcgBZoAwyewDN4BW9O5rw4787HYrTkFDtH4A+czx/ml5Uj</latexit>Memot<latexit sha1_base64=\"+uorYtFa4lbsiTsj2RHqcDcBc2s=\">AAAB/HicbVDLSsNAFJ3UV62vaJduBovgqiRS1GXBjRuhgn1AG8NkOmmHzmTCzEQIIf6KGxeKuPVD3Pk3TtostPXAwOGce7lnThAzqrTjfFuVtfWNza3qdm1nd2//wD486imRSEy6WDAhBwFShNGIdDXVjAxiSRAPGOkHs+vC7z8SqaiI7nUaE4+jSURDipE2km/Xbx+yEUd6Knk2EYjlua99u+E0nTngKnFL0gAlOr79NRoLnHASacyQUkPXibWXIakpZiSvjRJFYoRnaEKGhkaIE+Vl8/A5PDXKGIZCmhdpOFd/b2SIK5XywEwWOdWyV4j/ecNEh1deRqM40STCi0NhwqAWsGgCjqkkWLPUEIQlNVkhniKJsDZ91UwJ7vKXV0nvvOleNFt3rUablnVUwTE4AWfABZegDW5AB3QBBil4Bq/gzXqyXqx362MxWrHKnTr4A+vzB6eelY8=</latexit>Mgoalt\\n<latexit sha1_base64=\"dq8xoz40rP/sSfrcGSFnpQCU9JA=\">AAAB+3icbVDLSsNAFJ3UV62vWJduBovgqiRS1GXBjRuhgn1AG8NketsOnUnCzEQsIb/ixoUibv0Rd/6NkzYLbT0wcDjnXu6ZE8ScKe0431ZpbX1jc6u8XdnZ3ds/sA+rHRUlkkKbRjySvYAo4CyEtmaaQy+WQETAoRtMr3O/+whSsSi817MYPEHGIRsxSrSRfLt6+5AOBNETKVIBIst87ds1p+7MgVeJW5AaKtDy7a/BMKKJgFBTTpTqu06svZRIzSiHrDJIFMSETskY+oaGRIDy0nn2DJ8aZYhHkTQv1Hiu/t5IiVBqJgIzmcdUy14u/uf1Ez268lIWxomGkC4OjRKOdYTzIvCQSaCazwwhVDKTFdMJkYRqU1fFlOAuf3mVdM7r7kW9cdeoNVlRRxkdoxN0hlx0iZroBrVQG1H0hJ7RK3qzMuvFerc+FqMlq9g5Qn9gff4A44+VIQ==</latexit>Mmemt<latexit sha1_base64=\"7mkHDXGoIW7lV4nTh1XFaZiHRx8=\">AAAB8nicbVDLSsNAFL2pr1pfVZduBovgqiQi6rLgxo1QwT6gDWUynbRDJ5MwcyOU0M9w40IRt36NO//GSZuFth4YOJxzL3PuCRIpDLrut1NaW9/Y3CpvV3Z29/YPqodHbROnmvEWi2WsuwE1XArFWyhQ8m6iOY0CyTvB5Db3O09cGxGrR5wm3I/oSIlQMIpW6vUjimNGZXY/G1Rrbt2dg6wSryA1KNAcVL/6w5ilEVfIJDWm57kJ+hnVKJjks0o/NTyhbEJHvGepohE3fjaPPCNnVhmSMNb2KSRz9fdGRiNjplFgJ/OIZtnLxf+8XorhjZ8JlaTIFVt8FKaSYEzy+8lQaM5QTi2hTAublbAx1ZShbaliS/CWT14l7Yu6d1W/fLisNURRRxlO4BTOwYNraMAdNKEFDGJ4hld4c9B5cd6dj8VoySl2juEPnM8fkTORkw==</latexit>M\\n<latexit sha1_base64=\"yXdnnbxXBqlNb6FSY2d4P3vh7es=\">AAAB+nicbVDLSgMxFM3UV62vqS7dBItQN2VGirosdOOygn1AZyiZTKYNTTJDklHK2E9x40IRt36JO//GTDsLbT0QOJxzL/fkBAmjSjvOt1Xa2Nza3invVvb2Dw6P7OpxT8WpxKSLYxbLQYAUYVSQrqaakUEiCeIBI/1g2s79/gORisbiXs8S4nM0FjSiGGkjjeyqx5GeSJ6153UPh7G+GNk1p+EsANeJW5AaKNAZ2V9eGOOUE6ExQ0oNXSfRfoakppiRecVLFUkQnqIxGRoqECfKzxbR5/DcKCGMYmme0HCh/t7IEFdqxgMzmQdVq14u/ucNUx3d+BkVSaqJwMtDUcqgjmHeAwypJFizmSEIS2qyQjxBEmFt2qqYEtzVL6+T3mXDvWo075q1Fi3qKINTcAbqwAXXoAVuQQd0AQaP4Bm8gjfryXqx3q2P5WjJKnZOwB9Ynz8JPZP8</latexit>C(·)Environment state space<latexit sha1_base64=\"IstKYptdYs9iVAiSfAqv5dhBbK4=\">AAAB8nicbVDLSsNAFL2pr1pfVZduBovgqiQi6rLgxmVF+4A2lMl00g6dTMLMjVBCP8ONC0Xc+jXu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecZpwP6IjJULBKFqp148ojhmV2cNsUK25dXcOskq8gtSgQHNQ/eoPY5ZGXCGT1Jie5yboZ1SjYJLPKv3U8ISyCR3xnqWKRtz42TzyjJxZZUjCWNunkMzV3xsZjYyZRoGdzCOaZS8X//N6KYY3fiZUkiJXbPFRmEqCMcnvJ0OhOUM5tYQyLWxWwsZUU4a2pYotwVs+eZW0L+reVf3y/rLWEEUdZTiBUzgHD66hAXfQhBYwiOEZXuHNQefFeXc+FqMlp9g5hj9wPn8AmlGRmQ==</latexit>Swith state transition function<latexit sha1_base64=\"ACfeSADhxX6/Lp9JbK3rFm4u5SM=\">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZcFNy4r9IXtUDJppg1NMkOSEcrQv3DjQhG3/o07/8bMdBbaeiBwOOdecu4JYs60cd1vp7SxubW9U96t7O0fHB5Vj0+6OkoUoR0S8Uj1A6wpZ5J2DDOc9mNFsQg47QWzu8zvPVGlWSTbZh5TX+CJZCEj2FjpcSiwmSqRthejas2tuznQOvEKUoMCrVH1aziOSCKoNIRjrQeeGxs/xcowwumiMkw0jTGZ4QkdWCqxoNpP88QLdGGVMQojZZ80KFd/b6RYaD0XgZ3MEupVLxP/8waJCW/9lMk4MVSS5UdhwpGJUHY+GjNFieFzSzBRzGZFZIoVJsaWVLEleKsnr5PuVd27rjceGrUmK+oowxmcwyV4cANNuIcWdICAhGd4hTdHOy/Ou/OxHC05xc4p/IHz+QP4X5E/</latexit>T\\n…Social systems within environment and intelligent beings forms societyPerception ﬂowAction ﬂowCognition (learning and reasoning) ﬂow for updating mental statesMutual inﬂuence among all mental states\\n<latexit sha1_base64=\"fOp+L6zHZ6DWys21HW+iYV/sJrY=\">AAACMnicbVBNSwMxEM36bf2qevQSLIKClF0R9SIURNBbpbYV2lKy6bQGs9klmS2WZX+TF3+J4EEPinj1R5i2e1Drg8DjvZnJzPMjKQy67oszNT0zOze/sJhbWl5ZXcuvb9RMGGsOVR7KUN/4zIAUCqooUMJNpIEFvoS6f3c29Ot90EaE6hoHEbQC1lOiKzhDK7Xzl82A4S1nMqmn9JQ2Ee4xqYRcMFkZGITApLtj8Vz10/2s4FIhSCl6oJD6IFTPpHvtfMEtuiPQSeJlpEAylNv5p2Yn5HFgh3DJjGl4boSthGkUXEKaa8YGIsbvWA8alioWgGklo5NTumOVDu2G2j67xEj92ZGwwJhB4NvK4YHmrzcU//MaMXZPWolQUYyg+PijbiwphnSYH+0IDRzlwBLGtbC7Un7LNONoU87ZELy/J0+S2kHROyoeXh0WSiKLY4FskW2ySzxyTErkgpRJlXDyQJ7JG3l3Hp1X58P5HJdOOVnPJvkF5+sbYoGsMg==</latexit>W= SocialSystems(Env,Intelligent beings)\\nFigure 1.2: An overview of our general framework for describing an intelligent agent loop and agent society.\\n1.3.1 Core Concepts and Notations in the Agent Loop\\nOur architecture operates at three conceptual levels: Society ,Environment , and Agent . The Agent is then decomposed\\ninto three main subsystems: Perception ,Cognition , and Action . Within Cognition , we identify key submodules:\\nmemory ,world model ,emotional state ,goals ,reward ,learning , and reasoning processes (including “planning” and\\n“decision-making” as special actions produced with reasoning). Attention is primarily handled within perception and\\ncognition. Before presenting the formal loop, we summarize our symbols in Table 1.2.\\n18\\nIn the following, based on the notations in Table 1.2, we present our proposed agent loop.\\nThe Agent Loop\\nAn intelligent agent operates in discrete time steps t, continuously interacting with its environment. At each\\nstep, the following processes occur:\\n1.Environment State (st∈ S):\\nThe environment is in state st.\\n2.Perception ( P): The agent perceives the environment to generate observations ot:\\not= P(st, Mt−1),\\nwhere Mt−1guides selective attention and filtering.\\n3.Cognition ( C): Updates mental state and selects actions:\\n(Mt, at) = C( Mt−1, at−1, ot).\\nwhere Mtencapsulates different sub-states:\\nMt={Mmem\\nt, Mwn\\nt, Memo\\nt, Mgoal\\nt, Mrew\\nt,···}.\\nCognition consists of:\\n•Learning (L): Updates mental state based on observations:\\nMt= L(Mt−1, at−1, ot).\\n•Reasoning (R): Determines the next action:\\nat= R(Mt),\\nwhich may be:\\n– External Actions , directly affecting the environment.\\n– Internal Actions , including:\\n*Planning : Internal sequence of future actions.\\n*Decision-making : Choosing the best action from available options.\\n4.Action Execution ( E): Transforms action atinto executable form:\\na′\\nt= E(at).\\n5.Environment Transition ( T): The environment responds to the agent’s action:\\nst+1= T(st, a′\\nt).\\nIn multi-agent scenarios, each agent imaintains individual states (Mi\\nt, ai\\nt, oi\\nt), and the environment collec-\\ntively updates based on all agents’ actions. At broader scales (AI societies or worlds, W), agents interact\\nwithin diverse social systems (e.g., economic, communication, or transportation), forming complex societal\\nstructures.\\nFigure 1.2 illustrates our agent framework, presenting the core concepts and different types of information or control\\nflows among them. Until now, we have presented a brain-inspired agent framework that integrates biological insights\\ninto a formal Perception–Cognition–Action loop. By decomposing cognition into modules for memory, world modeling,\\nemotion, goals, reward-based learning, and reasoning, we capture essential parallels with the human brain’s hierarchical\\nand reward-driven processes. Critically, attention is included in the loop to enable selective filtering based on internal\\nstates. Furthermore, planning anddecision-making can be viewed as distinct internal (mental) actions that either refine\\ninternal representations or select external behaviors. Our framework naturally extends classical agent architectures,\\nproviding a multi-level structure that integrates emotional and rational processes as well as robust, reward-driven\\nlearning across short and long timescales.\\nSociety and Social Systems. In many real-world scenarios, agents do not merely interact with a static environment\\nbut operate within a broader society , comprising various social systems such as financial markets, legal frameworks,\\n19\\npolitical institutions, educational networks, and cultural norms. These structures shape and constrain agents’ behaviors\\nby defining rules, incentives, and shared resources. For example, a financial system dictates how economic transactions\\nand resource allocations occur, while a political system provides governance mechanisms and regulatory constraints.\\nTogether, these social systems create a layered context in which agents must adaptively learn, reason, and act—both to\\nsatisfy their internal goals and to comply (or strategically engage) with external societal rules. In turn, the actions of\\nthese agents feed back into the social systems, potentially altering norms, policies, or resource distributions.\\nA Formal Definition of Foundation Agents. Building on these insights and our vision of robust, adaptive intelligence,\\nwe now formally introduce the concept of a Foundation Agent . Unlike traditional agent definitions that focus primarily\\non immediate sensory-action loops, a Foundation Agent embodies sustained autonomy, adaptability, and purposeful\\nbehavior, emphasizing the integration of internal cognitive processes across diverse environments.\\nDefinition of Foundation Agent\\nAFoundation Agent is an autonomous, adaptive intelligent system designed to actively perceive diverse\\nsignals from its environment, continuously learn from experiences to refine and update structured internal\\nstates (such as memory, world models, goals, emotional states, and reward signals), and reason about\\npurposeful actions—both external and internal—to autonomously navigate toward complex, long-term\\nobjectives.\\nMore concretely, a Foundation Agent possesses the following core capabilities:\\n1.Active and Multimodal Perception: It continuously and selectively perceives environmental data\\nfrom multiple modalities (textual, visual, embodied, or virtual).\\n2.Dynamic Cognitive Adaptation: It maintains, updates, and autonomously optimizes a rich internal\\nmental state (memory, goals, emotional states, reward mechanisms, and comprehensive world\\nmodels) through learning that integrates new observations and experiences.\\n3.Autonomous Reasoning and Goal-Directed Planning: It proactively engages in sophisticated\\nreasoning processes, including long-term planning and decision-making, to derive goal-aligned\\nstrategies.\\n4.Purposeful Action Generation: It autonomously generates and executes purposeful actions,\\nwhich can be external (physical movements, digital interactions, communication with other agents\\nor humans) or internal (strategic planning, self-reflection, optimization of cognitive structures),\\nsystematically shaping its environment and future cognition to fulfill complex objectives.\\n5.Collaborative Multi-Agent Structure: It can operate within multi-agent or agent society structures,\\ncollaboratively forming teams or communities of agents that collectively accomplish complex tasks\\nand goals beyond individual capabilities.\\nThis definition highlights three essential pillars distinguishing Foundation Agents: sustained autonomy\\n(operating independently toward long-term goals without step-by-step human intervention), adaptive learning\\n(evolving internal representations continually over diverse experiences), and purposeful reasoning (generating\\nactions guided by complex, internally maintained goals and values). Foundation Agents thus represent a\\nfundamental shift from traditional agents by integrating deep cognitive structures, multimodal processing\\ncapabilities, and proactive, sustained self-optimization, enabling them to function effectively across a wide\\nrange of environments and domains.\\nUnlike classical definitions, which often frame agents primarily in terms of simple perception–action loops (“perceive\\nand act” [ 20]), our notion of Foundation Agents emphasizes the depth and integration of internal cognitive processes.\\nFoundation Agents not only perceive their environment and perform immediate actions but also possess an evolving,\\ngoal-oriented cognition—continuously adapting memory structures, world models, emotional and reward states, and\\nautonomously refining their strategies through reasoning. This internal cognitive richness allows Foundation Agents to\\nautonomously decompose complex, abstract goals into actionable tasks, strategically explore their environments, and\\ndynamically adjust their behavior and cognitive resources. Our unified perception–cognition–action framework thus\\naccommodates and explicitly models these sophisticated cognitive capabilities, recognizing internal (mental) actions on\\npar with external (physical or digital) interactions, facilitating a broad range of embodiments, from physical robots to\\nsoftware-based or purely textual intelligent agents.\\n20\\n1.3.2 Biological Inspirations\\nAlthough our agent model is fundamentally computational, each submodule draws inspiration from well-studied\\nbiological counterparts in the human brain. Below, we discuss these analogies in a manner that highlights both the\\nneuroscientific basis and the flexibility afforded by AI implementations.\\nMemory (Hippocampus and Neocortex). Decades of neuroscience research have linked the hippocampus to episodic\\nmemory formation, while cortical regions are known to house semantic and procedural knowledge [ 21,22]. In humans,\\nthese memory subsystems cooperate to manage both short-term encoding and long-term consolidation. Our memory\\ncomponent, Mmem\\nt, similarly aims to capture multi-scale learning by storing recent experiences and knowledge. This\\ncan be realized through either neural network weights (long-term) or explicit buffers (short-term), thereby mirroring the\\nhippocampal–cortical interplay.\\nWorld Model (Predictive Processing). A prominent theory in cognitive neuroscience holds that the cortex operates as\\na predictive machine, continually comparing incoming sensory data with generated expectations [ 23,19]. The world\\nmodel Mwm\\ntreflects this idea by maintaining an internal representation of how the environment evolves over time. Just\\nas cortical circuits integrate multisensory data to update these internal models, our framework allows Mwm\\ntto be refined\\nupon each new observation and relevant reward or emotional cues, offering a Bayesian or free-energy perspective on\\nenvironmental dynamics.\\nEmotion (Limbic System). Emotions, mediated by structures like the amygdala, hypothalamus, and limbic system,\\nsignificantly modulate attention, learning rates, and decision-making thresholds [ 24,25]. By introducing an emotion\\ncomponent Memo\\nt, our model captures how internal valence or arousal states can shift an agent’s focus and behavior.\\nAlthough computational “emotions” are neither fully analogous to biological affect nor conscious feelings, they can\\nguide adaptive heuristics—such as prioritizing urgent goals or responding quickly to perceived threats.\\nGoals and Reward (Prefrontal & Subcortical Circuits). Humans excel at forming abstract, long-term goals, an ability\\noften associated with prefrontal cortex function [ 26,27]. In parallel, subcortical circuits—particularly dopaminergic\\npathways—drive reinforcement signals that shape motivation and habit learning [ 28]. Our agent includes Mgoal\\nt\\nfor storing objectives and Mrew\\ntfor encoding reward signals, thus enabling a continuous feedback loop where goal\\nformation and reward-based adaptation reinforce each other. This mechanism allows for planned action sequences, tool\\nusage, and more nuanced social interactions.\\nReasoning, Planning, and Decision-Making (Prefrontal Cortex). Finally, the human prefrontal cortex integrates\\ninformation from memory, sensory inputs, emotions, and reward pathways to carry out higher-order cognitive pro-\\ncesses—such as logical reasoning, planning, and executive control [ 29,30]. In our agent framework, these capabilities\\nare subsumed by the reasoning sub-function, which—through modules like PlanFn andDecide —selects and executes\\nactions (whether physical or purely mental). By distinguishing planning from on-the-fly decision-making, we capture\\nhow the agent can simulate future scenarios, weigh outcomes, and then commit to a course of action, akin to the flexible\\norchestration observed in prefrontal circuits.\\n1.3.3 Connections to Existing Theories\\nBeyond these explicit neurobiological parallels, our architecture resonates with several important theories in AI,\\ncognitive science, and neuroscience.\\nClassic Perception–Cognition–Action Cycle. We extend the traditional sense–think–act cycle outlined by [ 20],\\nincorporating explicit mechanisms for attention (in P), learning and emotion (in C), and reward signals that persist over\\ntime. This explicitness makes it easier to analyze how an agent’s internal states and prior actions shape subsequent\\nperception and cognition.\\nMinsky’s “Society of Mind”. [17] argued that intelligence arises from an ensemble of specialized “agents” within\\na mind. Our submodules— Cmem,Cwm,Cemo,Cgoal,Crew—echo this decomposition, distributing key functions\\n(memory, prediction, emotional evaluation, goal-setting, etc.) across separate yet interacting components. In a broader\\n“society” context, each agent (or sub-agent) could coordinate cooperatively or competitively, much like Minsky’s\\ninternal agencies. Recent work on natural language-based societies of mind [ 31] supports that agentic systems can be\\nrepresented using the original society-of-mind theory, and could incorporate social structures and economic models\\namong agents.\\nBuzsáki’s Inside-Out Perspective. Neuroscientists [ 18] contend that the brain actively constructs and updates its\\nperception instead of merely receiving inputs. In our model, Mt−1—including emotional states, reward signals, and\\ngoals—directly influences the perception map P. This supports the inside-out stance that an agent’s internal context\\ndrives the way it samples and interprets the environment, rather than passively reacting to it.\\n21\\nPartially observable Markov decision process (POMDP). Our framework can be viewed as a generalization of the\\nclassical Partially Observable Markov Decision Process (POMDP) in several ways. First, whereas a POMDP specifies a\\nprobabilistic transition function P(st+1|st, at)over a (possibly finite) state space, we retain an environment transition\\nTwithout restricting it to a purely probabilistic or finite form, allowing for arbitrary or even deterministic mappings.\\nSecond, in the standard POMDP setting, reward is typically defined as a scalar function of (st, at)(possibly discounted\\nover time). By contrast, we place reward signals inside the agent’s mental state ( Mrew\\nt), letting them depend on—and\\nco-evolve with—goals, emotion, and the world model rather than enforcing a single externally defined objective.\\nThird, while POMDP agents generally select actions by maximizing an expected return (value function), our reasoning\\nsub-process is broader. It accounts for memory, emotion, and other mental-state factors, accommodating heuristic\\nor socially driven decisions rather than strictly value-based choices. Finally, a POMDP does not explicitly define\\ncognitive submodules such as memory or emotion—these must be collapsed into a monolithic “belief state”. In our\\nframework, each sub-component (memory, world model, emotion, goals, reward) is explicitly modeled and updated,\\nmirroring biologically inspired views of cognition. Hence, although our approach recovers the POMDP formulation as\\na special case (by enforcing a probabilistic T, a scalar reward, and a minimal mental state), it admits a richer variety of\\nenvironment transitions, internal states, and decision mechanisms.\\nActive Inference and the Bayesian Brain. Active inference, a unifying framework advanced by [ 19], suggests that\\nagents continually update internal generative models to minimize prediction error (or “free energy”). Our use of Mwm\\nandMrew, together with planning and decision-making modules, can be interpreted in Bayesian terms. The agent\\nattempts to reduce surprise by aligning its world model with new data and by choosing actions that conform to predicted\\n(or desired) outcomes.\\nBiological Plausibility & Generality. While the mapping between brain circuits and agent submodules is made at\\na high level, it offers an approach that is at once biologically inspired andmodularly agnostic . Memory, emotion,\\ngoals, and reward can each be implemented by various AI paradigms—symbolic methods, neural networks, or hybrid\\napproaches—thus preserving flexibility. By integrating these key ideas from neuroscience, cognitive science, and AI,\\nwe arrive at a general framework that captures the essential properties of intelligent behavior without overconstraining\\nimplementation details.\\n1.4 Navigating This Survey\\nThis survey is structured to provide a comprehensive, modular, and interdisciplinary examination of intelligent agents,\\ndrawing inspiration from cognitive science, neuroscience, and other disciplines to guide the next wave of advancements\\nin AI. While many existing surveys [ 32,33,34,35,36,37,38,39,40] offer valuable insights into various aspects\\nof agent research, we provide a detailed comparison of their focal points in Table 1.3. Our work distinguishes itself\\nby systematically comparing biological cognition with computational frameworks to identify synergies, gaps, and\\nopportunities for innovation. By bridging these domains, we aim to provide a unique perspective that highlights not\\nonly where agents excel but also where significant advancements are needed to unlock their full potential.\\nTable 1.3: Summary of existing reviews with different focal points. •indicates primary focus while ◦indicates\\nsecondary or minor focus.\\nSurvey Cognition Memory World Model Reward Action Self Evolve MultiAgent Safety\\nZhang et al. [39] • • ◦ ◦ ◦ • ◦ ◦\\nGuo et al. [38] • • ◦ ◦ ◦ • • ◦\\nYu et al. [40] • • ◦ ◦ • ◦ • •\\nWang et al. [35] • • ◦ ◦ • ◦ • ◦\\nMasterman et al. [37] • • ◦ ◦ • ◦ • ◦\\nXi et al. [34] • • ◦ ◦ • • • •\\nHuang et al. [33] • • ◦ • • • • •\\nDurante et al. [32] • • ◦ • • • • •\\nThis Manuscript • • • • • • • •\\nThe survey is divided into four key parts:\\n•InPart I : Modular Design of Intelligent Agents, we introduce the core modules of agents, including the\\ncognition module, which serves as the “brain” of the agent; the perception systems for interpreting sensory\\ninput; as well as the action systems for interacting with the external world. Within the cognition system,\\nwe further discuss the memory, world modeling, emotion, goal, and reward systems, analyzing their current\\nprogress, limitations, and research challenges.\\n22\\n•InPart II : Self-Enhancement in Intelligent Agents, we shift focus to the capability of agents to evolve and\\noptimize themselves. We explore mechanisms like adaptive learning, self-reflection, and feedback-driven\\nimprovement, inspired by the human ability to grow and refine skills over time. This part also addresses the\\nimportance of dynamic memory systems and continuous knowledge integration for agents to remain relevant\\nand effective in changing environments.\\n•InPart III : Collaborative and Evolutionary Intelligent Systems, we examine how agents interact with each\\nother and their environments to solve complex, large-scale problems. We discuss multi-agent systems,\\nhighlighting their applications in fields such as robotics, medical systems and scientific discovery. This\\npart explores multi-agent system topologies and agent protocol, tracing the evolution of communication and\\ncollaboration from static to dynamic frameworks. We align agents with human collaboration paradigms,\\nexamining how interaction patterns shape the co-evolution of intelligence and how multi-agent systems\\nadapt their decision-making in various collaborative settings to solve complex challenges through collective\\nintelligence.\\n•Finally, in Part IV : Building Safe and Beneficial AI, we provide a comprehensive analysis of the security\\nlandscape for LLM-based agents. We introduce a framework categorizing threats as intrinsic or extrinsic.\\nIntrinsic vulnerabilities arise from within the agent’s architecture: the core LLM “brain”, and the perception\\nand action modules that enable interactions with the world. Extrinsic risks stem from the agent’s engagement\\nwith memory systems, other agents, and the broader environment. This part not only formalizes and analyzes\\nthese vulnerabilities, detailing specific attack vectors like jailbreaking and prompt injection, but also reviews a\\nrange of defense mechanisms. Moreover, we explore future directions, including superalignment techniques\\nand the scaling law of AI safety—the interplay between capability and risk.\\nBy weaving together these threads, our survey aims to provide a holistic perspective on the current state of intelligent\\nagents and a forward-looking roadmap for their development. Our unique focus on integrating cognitive science insights\\nwith computational design principles positions this survey as a foundational resource for researchers seeking to design\\nagents that are not only powerful and efficient but also adaptive, ethical, and deeply aligned with the complexities of\\nhuman society.\\n23\\nPart I\\nCore Components of Intelligent Agents\\n24\\nChapter 2\\nCognition\\nHuman cognition represents a sophisticated information processing system that enables perception, reasoning, and\\ngoal-directed behavior through the orchestrated operation of multiple specialized neural circuits [ 98]. This cognitive\\narchitecture operates through mental states, which serve as the foundation where learning and reasoning occur. The\\nremarkable ability to process information across different levels of abstraction and adapt to novel situations is a crucial\\ninspiration for LLM agents [27].\\nThe cognitive system exhibits several fundamental architectural properties reflected in Figure 1.1. First, learning\\nfunctions across different mental state spaces: it can occur holistically across frontal lobes (supporting executive control\\nand cognition) and temporal lobes (responsible for language, memory, and auditory processing), or focus on specific\\naspects for targeted improvement as shown by the varied research levels in the figure. Second, reasoning emerges in\\ndistinct patterns: it can follow structured templates for systematic problem-solving supported by logical reasoning and\\ncognitive flexibility in the frontal lobes, or appear in unstructured forms for flexible thinking, particularly evident in\\ndecision-making and executive control functions. Third, the system demonstrates remarkable adaptability, continuously\\nupdating its mental states through experience while leveraging both supervised feedback (as in adaptive error correction\\nin the cerebellum) and unsupervised environmental statistics, reflected in the different exploration stages of various\\ncognitive functions shown in the figure [99].\\nThese cognitive processes are supported by a modular organization, composed of distinct but interconnected components\\nthat form a cohesive system [ 100]. These modules include perception systems that transform raw sensory data into\\nmeaningful representations, memory systems that provide the substrate for storing and retrieving information, world\\nmodels that support future scenario simulation, reward signals that guide refinement of behavior through reinforcement,\\nemotion systems that modulate attention and resource allocation, reasoning systems that formulate decisions, and action\\nsystems that translate decisions into environmental interactions.\\nWhile human cognition implements these properties through complex neural architectures shaped by evolution,\\nLLM agents attempt to approximate similar functions using large-scale neural models and algorithmic techniques.\\nUnderstanding this biological-artificial parallel is crucial for developing more capable agents [ 101], as it highlights both\\nthe achievements and limitations of current systems compared to human cognition. Significant differences remain in\\nareas such as adaptability, generalization, and contextual understanding.\\nIn this section, we first explore Learning , examining both the spaces where it occurs within mental states and the\\nspecific objectives it serves. Subsequently, we investigate Reasoning , analyzing both structured and unstructured\\napproaches, before concluding with a dedicated exploration of planning capabilities as a special reasoning action.\\n2.1 Learning\\nLearning represents the fundamental process through which intelligent agents transform experiences into knowledge\\nwithin their mental states. This transformation occurs across different cognitive spaces, from holistic updates across\\nthe full mental state to refinement of specific cognitive components. The scope of learning encompasses remarkable\\ncapacities that serve different objectives: enhancing perceptual understanding, improving reasoning capabilities, and\\ndeveloping richer world understanding.\\n25\\nCognitionLearning Space FullSFT [ 41], PEFT [ 42],\\nRLHF [ 43], ReFT [ 44],\\nAgentic Models [ 45]\\nPartialCoT [ 46], V oyager [ 47],\\nReflexion [ 48], ActRe [ 49],\\nGenerative Agents [ 50]\\nObjectivePerceptionCLIP [ 51], LLaV A [ 52],\\nCogVLM [ 53], Qwen-Audio [ 54],\\nR1-Searcher [ 45], Search-R1 [ 55]\\nReasoningSKY-32B [ 56], Open Thoughts [ 57],\\nLIMO [ 58], STaR [ 59], ReST [ 60],\\nOpenR [ 61], LLaMA-Berry [ 62],\\nRAGEN [ 63], OpenR1 [ 64]\\nWorldInner Monologue [ 65], DESP [ 66],\\nSelf-refine [ 67], CRITIC [ 68],\\nReflexion [ 48], ExpeL [ 69]\\nReasoningStructuredDynamicReAct [ 70], MCoT [ 71],\\nToT [ 72], LATS [ 73], RAP [ 74],\\nGoT [ 75], PoT [ 76], DoT [ 77]\\nStaticSelf-Consistency [ 78],\\nSelf-refine [ 67], PHP [ 79],\\nSelf-Verification [ 80], CoVe [ 81]\\nDomainMathPrompter [ 82], PedCoT [ 83],\\nPhysics Reasoner [ 84]\\nUnstructuredPromptCoT [ 46], Step-Back, [ 85]\\nAsk Me Anything [ 86], CoK [ 87], SEK [ 88]\\nModelDeepSeek-R1 [ 89],\\nClaude 3.7 Sonnet [ 9], o1 [ 90]\\nImplicit Quiet-STaR [ 91], Coconut [ 92]\\nPlanningDEPS [ 66], ProgPrompt [ 93],\\nADaPT [ 94], ToT [ 72], RAP [ 74],\\nTravelPlanner [ 95], PDDL [ 96],\\nMind2Web [ 97]\\nFigure 2.1: Illustrative Taxonomy of Cognition system, including learning and reasoning paradigm.\\nHuman learning operates across multiple spaces and objectives through the brain’s adaptable neural networks. The\\nbrain coordinates learning across its entire network through integrated systems: the hippocampus facilitates rapid\\nencoding of episodic experiences, the cerebellum supports supervised learning for precise motor skills, the basal ganglia\\nenable reinforcement learning through dopaminergic reward signals, and cortical regions facilitate unsupervised pattern\\nextraction [ 99]. At more focused levels, specific neural circuits can undergo targeted adaptation, allowing for specialized\\nskill development and knowledge acquisition. These systems work together on different timescales, ranging from\\nimmediate responses to lifelong development, while being influenced by factors like attention, emotions, and social\\nenvironment [27].\\nLLM agents, while fundamentally different in architecture, implement analogous learning processes across their\\nmental state spaces. At the comprehensive level, they acquire broad knowledge through pre-training on massive\\ndatasets, demonstrating a form of unsupervised learning. At more focused levels, they refine specific capabilities\\nthrough parameter-updating mechanisms like supervised fine-tuning and reinforcement learning. Uniquely, they also\\ndemonstrate in-context learning capabilities, adapting to novel tasks without parameter changes by leveraging context\\n26\\nwithin their attention window: a capability that mirrors aspects of human working memory but operates through\\nfundamentally different mechanisms.\\nThe comparison between human and artificial learning systems provides valuable insights for developing more\\ncapable, adaptive agents. Human learning demonstrates notable characteristics in efficiency, contextualization, and\\nintegration with emotional systems, while LLM-based approaches show distinct capabilities in processing large datasets,\\nrepresenting formal knowledge, and synthesizing information across domains. These complementary strengths suggest\\nproductive directions for research. As we explore the foundations of learning, we first examine the spaces where\\nlearning occurs within mental states, followed by an analysis of the specific objectives that drive learning processes.\\nTable 2.1: Summary of Learning Methods with Different State Modifications. •indicates primary impact while ◦\\nindicates secondary or no direct impact.\\nMethod Model Perception Reasoning Memory Reward World Model\\nV oyager [47] ◦ ◦ ◦ • ◦ ◦\\nGenerative Agents [50] ◦ ◦ ◦ • ◦ ◦\\nLearn-by-interact [102] • ◦ ◦ • ◦ ◦\\nRAGEN [63] • ◦ • ◦ • ◦\\nDigiRL [103] • ◦ • ◦ • ◦\\nR1-Searcher [45] • • • ◦ • ◦\\nRewardAgent [104] • ◦ ◦ ◦ • ◦\\nText2Reward [105] ◦ ◦ ◦ ◦ • ◦\\nARAMP [106] • ◦ ◦ ◦ • ◦\\nActRe [49] • ◦ • ◦ ◦ •\\nWebDreamer [107] ◦ ◦ ◦ ◦ ◦ •\\nRAP [74] ◦ ◦ ◦ ◦ ◦ •\\nAutoManual [108] ◦ ◦ ◦ • ◦ •\\n2.1.1 Learning Space\\nThe learning approaches in LLM agents represent a structured, data-driven paradigm in contrast to the exploratory,\\nemotionally-driven learning observed in humans. While human learning often involves active curiosity, motivation, and\\nemotional reinforcement, LLM-based agents typically learn through more formalized processes, such as parameter\\nupdates during training or structured memory formation during exploration. Current agent architectures attempt to\\nbridge this gap by implementing mechanisms that simulate aspects of human learning while leveraging the strengths of\\ncomputational systems.\\nLearning within an intelligent agent occurs across different spaces, encompassing both the underlying model θand\\nmental states M, where the former fundamentally supports the capabilities and limitations of the latter. Formally, we\\ndefine an intelligent agent’s internal state as a tuple I= (θ, M)that includes both the model parameters and mental\\nstate components. The mental state can be further decomposed into different structures as we illustrated in 1.2:\\nM={Mmem, Mwm, Memo, Mgoal, Mrew} (2.1)\\nwhere Mmemrepresents memory, Mwmdenotes world model, Memoindicates emotional state, Mgoalrepresents\\ngoals, and Mrewrepresents reward signals.\\nModifications to the underlying model can be viewed as full mental state learning, as they fundamentally alter the\\nagent’s capabilities. While model-level modifications can affect different mental states to varying degrees, changes\\nto the model’s context window or external structures tend to focus on specific mental state components. For instance,\\nlearning experiences and skills from the environment primarily influence memory, while leveraging the LLM’s inherent\\npredictive capabilities enhances the world model.\\nFull Mental State Learning Full mental state learning enhances the capabilities of an agent through comprehensive\\nmodifications to the underlying model θ, which in turn affects all components of the mental state M. This process\\nbegins with pre-training, which establishes the foundation of language models by acquiring vast world knowledge,\\nanalogous to how human babies absorb environmental information during development, though in a more structured\\nand extensive manner.\\nPost-training techniques represent the cornerstone for advancing agent capabilities. Similar to how human brains are\\nshaped by education, these techniques while affecting the entire model, can emphasize different aspects of cognitive\\n27\\ndevelopment. Specifically, various forms of tuning-based learning enable agents to acquire domain-specific knowledge\\nand logical reasoning capabilities. Supervised Fine-Tuning (SFT) [ 41] serves as the fundamental approach where\\nmodels learn from human-labeled examples, encoding knowledge directly into the model’s weights. For computational\\nefficiency, Parameter-Efficient Fine-Tuning (PEFT) methods have emerged. Adapter-BERT [ 42] introduced modular\\ndesigns that adapt models to downstream tasks without modifying all parameters, while Low-Rank Adaptation (LoRA)\\n[109] achieves similar results by decomposing weight updates into low-rank matrices, adjusting only a small subset of\\neffective parameters.\\nSome agent capabilities are closely connected to how well they align with human preferences, with alignment-based\\nlearning approaches modifying the model to reshape aspects of the agent’s underlying representations. Reinforcement\\nlearning from human feedback (RLHF) [ 110] aligns models with human values by training a reward model on\\ncomparative judgments and using this to guide policy optimization. InstructGPT [ 43] demonstrated how this approach\\ncould dramatically improve consistency with user intent across diverse tasks. Direct Preference Optimization (DPO)\\n[111] has further simplified this process by reformulating it as direct preference learning without explicit reward\\nmodeling, maintaining alignment quality while reducing computational complexity.\\nReinforcement learning (RL) presents a promising pathway for specialized learning in specific environments. RL\\nhas shown particular promise in enhancing reasoning capabilities, essentially enabling the agent’s underlying model\\nto learn within the space of thought. Foundational works such as Reinforcement Fine-Tuning (ReFT) [ 44] enhance\\nreasoning through fine-tuning with automatically sampled reasoning paths under online reinforcement learning rewards.\\nDeepSeek-R1 [ 89] advances this approach through rule-based rewards and Group Relative Policy Optimization (GRPO)\\n[112], while Kimi k1.5 [ 113] combines contextual reinforcement learning with optimized chain-of-thought techniques\\nto improve both planning processes and inference efficiency. In specific environments, modifying models to enhance\\nagents’ understanding of actions and external environments has proven effective, as demonstrated by DigiRL [ 103],\\nwhich implements a two-stage reinforcement learning approach enabling agents to perform diverse commands on\\nreal-world Android device simulators.\\nRecent works have attempted to integrate agent action spaces directly into model training [ 45,55], enabling learning\\nof appropriate actions for different states through RL or SFT methods. This integration fundamentally affects the\\nagent’s memory, reward understanding, and world model comprehension, pointing toward a promising direction for the\\nemergence of agentic models.\\nPartial Mental State Learning While full mental state learning through model modifications provides comprehensive\\ncapability updates, learning focused on particular components of an agent’s mental state Mrepresents another essential\\nand often more efficient approach. Such partial mental state learning can be achieved either through targeted model\\nupdates or through in-context adaptation without parameter changes.\\nIn-Context Learning (ICL) illustrates how agents can effectively modify specific mental state components without\\nmodifying the entire model. This mechanism allows agents to adapt to new tasks by leveraging examples or instructions\\nwithin their context window, paralleling human working memory’s role in rapid task adaptation. Chain-of-Thought\\n(CoT) [ 46] demonstrates the effectiveness of this approach, showing how agents can enhance specific cognitive\\ncapabilities while maintaining their base model parameters unchanged.\\nThe feasibility of partial mental state learning is evidenced through various approaches targeting different components\\nsuch as memory ( Mmem), reward ( Mrew), and world model ( Mwm). Through normal communication and social\\ninteraction, Generative Agents [ 50] demonstrate how agents can accumulate and replay memories, extracting high-\\nlevel insights to guide dynamic behavior planning. In environmental interaction scenarios, V oyager [ 47] showcases\\nhow agents can continuously update their skill library through direct engagement with the Minecraft environment,\\naccumulating procedural knowledge without model retraining. Learn-by-Interact [ 102] further extends this approach by\\nsynthesizing experiential data through direct environmental interaction, eliminating the need for manual annotation or\\nreinforcement learning frameworks. Additionally, agents can learn from their mistakes and improve through reflection,\\nas demonstrated by Reflexion [ 48], which guides agents’ future thinking and actions by obtaining textual feedback from\\nrepeated trial and error experiences.\\nModifications to reward and world models provide another example of partial mental state learning. ARMAP [ 106]\\nrefines environmental reward models by distilling them from agent action trajectories, providing a foundation for further\\nlearning. AutoMC [ 114] constructs dense reward models through environmental exploration to support agent behavior.\\nMeanwhile, [ 107] explicitly leverages LLMs as world models to predict the impact of future actions, effectively\\nmodifying the agent’s world understanding ( Mwm). ActRe[ 49] builds upon the language model’s inherent world\\nunderstanding to construct tasks from trajectories, enhancing the agent’s capabilities as both a world model and\\nreasoning engine through iterative training.\\n28\\n2.1.2 Learning Objective\\nThe learning process of intelligent agents manifests across all aspects of their interaction with the environment. At the\\ninput level, agents learn to better perceive and parse environmental information; at the processing level, agents learn\\nhow to conduct effective reasoning based on existing knowledge or reasoning capabilities; at the comprehension level,\\nagents form and optimize their understanding of the world through continuous interaction. This multi-level learning\\nobjective framework enables agents to evolve continuously across different dimensions, allowing them to better handle\\ncomplex and dynamic task environments.\\nLearning for Better Perception The ability to effectively perceive and process information from the environment is\\nfundamental to agent intelligence. To enhance perceptual capabilities, agents employ two primary learning approaches:\\nexpanding multimodal perception and leveraging retrieval mechanisms.\\nMultimodal perception learning enables agents to process and integrate diverse sensory inputs, similar to human\\nmulti-sensory integration but unconstrained by biological limitations. This capability has evolved significantly through\\nadvances like CLIP [ 51], which pioneered the alignment of visual and linguistic representations in shared embedding\\nspaces. Building on this foundation, models like LLaV A [ 52] enhanced visual perception by training specialized\\nprojectors on image-text pairs, while CogVLM [ 53] advanced visual reasoning through unified representational\\narchitectures.\\nThe expansion of perceptual modalities continues across multiple sensory domains. In audio processing, Qwen-\\nAudio [ 54] demonstrates the unified encoding of diverse acoustic information, from speech to environmental sounds.\\nRecent work by [ 115] has even ventured into tactile perception, developing datasets that align touch, vision, and\\nlanguage representations. These advances enable agents to engage more comprehensively with both physical and digital\\nenvironments.\\nAgents also learn to enhance their observational capabilities through retrieval mechanisms. Unlike human perception,\\nwhich is constrained by immediate sensory input, agents can learn to access and integrate information from vast\\nexternal knowledge repositories. Retrieval-augmented approaches like RAG [ 116] enhance perceptual understanding by\\nconnecting immediate observations with relevant stored knowledge.\\nRecent work on retrieval-based agents demonstrates the potential for enhancing active information acquisition ca-\\npabilities. Search-o1 [ 117] guides reasoning models to learn active retrieval through prompting, thereby expanding\\ntheir knowledge boundaries. Taking this further, R1-Searcher [ 45] and Search-R1 [ 55] directly incorporate retrieval\\ncapabilities into the model, enabling autonomous information retrieval during the reasoning process. These advances\\nsuggest a promising direction for improving agent perception: enhancing model-level active perception capabilities\\nto enrich the foundation for decision-making. This approach may represent a significant avenue for future agent\\ndevelopment.\\nLearning for Better Reasoning Reasoning serves as a critical bridge between an agent’s mental state and its actions,\\nmaking the ability to reason effectively and the development of reasoning capabilities essential for intelligent agents.\\nThe foundation of reasoning in modern agents stems from two key elements: the rich world knowledge embedded in\\ntheir underlying models, and the robust logical frameworks supported either internally or through context structuring.\\nThis makes learning for better reasoning a vital objective in agent development.\\nThe development of reasoning capabilities is demonstrated through several key phenomena. First, high-quality reasoning\\ndata directly enhances model reasoning ability; second, such high-quality data often requires verification or reward\\nmodels for effective curation; and third, direct reinforcement learning on foundation models can spontaneously manifest\\nreasoning capabilities.\\nThe importance of reasoning in agent development has been re-emphasized following the release of the o1 series. A\\ncommon approach involves collecting and distilling data from open/closed-source reasoning models. For instance,\\nSKY-32B [ 56] distilled data from QWQ-32B [ 118] to train a 32B reasoning model at a cost of $450. Similarly, Open\\nThoughts [ 57] trained Bespoke-Stratos-32B at a low cost by distilling and synthesizing datasets from R1. These studies\\ndemonstrate that even without complex algorithmic design, using reasoning data to perform Supervised Fine-Tuning\\n(SFT) on base models can effectively activate reasoning capabilities.\\nAnother crucial insight regarding data quality is that highly structured reasoning data more effectively enables agents\\nand language models to learn reasoning processes. Notably, LIMO [ 58] demonstrated that powerful reasoning models\\ncould be built with extremely few data samples by constructing long and effective reasoning chains for complex\\nreasoning tasks. This insight stems from their observation that language models inherently possess sufficient knowledge\\nfor reasoning but require high-quality reasoning paths to activate these capabilities. Supporting this view, Li et al.\\n29\\n[119] revealed that both Long CoT and Short CoT fundamentally teach models to learn reasoning structures rather than\\nspecific content, suggesting that automated selection of high-quality reasoning data may become an important future\\ndirection.\\nOne viable exploration approach involves first conducting extensive searches, and then using verifiable environments\\nor trainable reward models to provide feedback on reasoning trajectories, thereby filtering out high-quality reasoning\\ndata. This approach has led to several families of techniques that leverage different feedback mechanisms to improve\\nreasoning capabilities.\\nThe first category follows the bootstrap paradigm exemplified by STaR [ 59] and its variants, which implement techniques\\nwhere models generate step-by-step rationales and iteratively improve through fine-tuning on successful reasoning\\npaths. This family includes Quiet-STaR [ 91], V-STaR [ 120], and rStar-Math [ 121], with the latter specifically enhancing\\nmathematical reasoning through reinforcement learning principles. By iteratively selecting correct reasoning paths for\\ntraining, these methods achieve self-improvement through successive refinement cycles.\\nThe second category extends this paradigm by more explicitly incorporating reinforcement learning principles. The\\nReST family, beginning with the original ReST [ 60] introducing reinforced self-training, performs multiple attempts\\n(typically 10) per sample and creates new training datasets from successful reasoning instances. ReST-EM [ 122]\\nenhances the approach with expectation maximization, while ReST-MCTS [ 122] further integrates Monte Carlo Tree\\nSearch to enable improved reasoning capabilities through more sophisticated exploration strategies.\\nSeveral approaches have introduced Policy Reward Models (PRMs) to provide quality feedback on reasoning paths.\\nMethods like OpenR [ 61] and LLaMA-Berry [ 62] model reasoning tasks as Markov Decision Processes (MDPs) and\\nleverage tree search to explore diverse reasoning paths while using PRMs for quality assessment. In domain-specific\\napplications, methods like rStar-Math [ 121] and DeepSeekMath [ 112] have demonstrated success in mathematical\\nproblem-solving through multi-round self-iteration and balanced exploration-exploitation strategies. For code generation,\\no1-Coder [ 123] leverages MCTS to generate code with reasoning processes, while Marco-o1 [ 123] extends this approach\\nto open-ended tasks. These implementations highlight how the synergy between MCTS and PRM achieves effective\\nreasoning path exploration while maintaining solution quality through fine-grained supervision.\\nBeyond data-driven approaches, reinforcement learning (RL) has demonstrated remarkable success in enhancing\\nlanguage models’ reasoning capabilities, as evidenced by recent breakthroughs like DeepSeek R1 [ 89] and Kimi-K-1.5\\n[113]. The foundation of RL for LLMs can be traced to several pioneering frameworks: ReFT [ 44] introduced a\\ncombination of supervised fine-tuning and online reinforcement learning, while VeRL [ 124] established an open-\\nsource framework supporting various RL algorithms for large-scale models up to 70B parameters. RFT [ 125] further\\ndemonstrated the effectiveness of reward-guided optimization in specific reasoning tasks.\\nBuilding upon these foundations, subsequent works have explored diverse applications and improvements. OpenR1 [ 64]\\nand RAGEN [ 63] extended RL techniques to enhance general reasoning capabilities, while specialized implementations\\nlike SWE-Gym [ 126] demonstrated success in software engineering tasks. Notably, DigiRL [ 103] introduced novel\\napproaches for digital-world agent enhancement.\\nRecent advances have further integrated RL with tool usage and reasoning. Qwen-QwQ-32B [ 118] employs rein-\\nforcement learning and a general reward mechanism to incorporate tool calling into the reasoning process, enabling\\nthe seamless use of arbitrary tools during reasoning and achieving agent-like capabilities directly within the model.\\nSimilarly, RAGEN [ 63] focuses on multi-step agentic scenarios, establishing a framework for agent reinforcement\\nlearning in complex environments. These developments suggest an increasing convergence between model training\\nand agent development, potentially leading to more integrated and capable intelligent systems. These implementations\\nhighlight how RL can effectively improve model performance while reducing dependence on large-scale annotated\\ndatasets, particularly in complex reasoning scenarios.\\nLearning for World Understanding A critical aspect of agent intelligence is the ability to understand how the world\\noperates through direct interaction and experience accumulation. This understanding encompasses how the environment\\nresponds to different actions and the consequences these actions bring. Through continuous interaction with their\\nenvironment, agents can build and refine their memory ,reward understanding , and world model , learning from both\\nsuccesses and failures to develop a more comprehensive grasp of their operational domain.\\nRecent research has revealed diverse approaches to experiential learning for world understanding. At the foundational\\nlevel, Inner Monologue [ 65] demonstrates how agents can accumulate basic environmental knowledge through con-\\ntinuous interaction. Similarly, Learn-by-Interact [ 102] shows that meaningful understanding can emerge from direct\\nenvironmental engagement without explicit reward mechanisms. More sophisticated approaches are exemplified by\\nDESP [ 66] and V oyager [ 47] in the Minecraft environment, where agents not only gather experiences but also actively\\nprocess them: DESP through outcome analysis and V oyager through dynamic skill library expansion.\\n30\\nThe processing and utilization of accumulated experiences have been further systematized through advanced frameworks.\\nGenerative Agents [ 50] introduces sophisticated memory replay mechanisms, enabling agents to extract high-level\\ninsights from past interactions. This systematic approach is enhanced by Self-refine [ 67] and Critic [ 68], which\\nimplement structured cycles of experience evaluation and refinement.\\nThe optimization of reward understanding through environmental interaction has emerged as another crucial aspect\\nof world understanding. Text2Reward [ 105] demonstrates how agents can continuously refine reward functions\\nthrough human feedback, better aligning them with task objectives and environmental characteristics. Similarly,\\nAutoManual [ 108] builds behavioral guidelines through sustained interaction, developing reward-verified protocols\\nthat provide a foundation for understanding environmental rewards and decision-making. These interaction-based\\noptimization mechanisms enable agents to better comprehend environmental dynamics and generate more precise reward\\nsignals, ultimately enhancing their adaptability and decision-making capabilities in complex, dynamic environments.\\nBuilding on these foundations, RAP [ 74] represents a significant advancement by conceptualizing reasoning as planning\\nwith a world model. By repurposing LLMs as both reasoning agents and world models, RAP enables agents to simulate\\nthe outcomes of potential actions before committing to them, facilitating more effective planning through Monte Carlo\\nTree Search. This approach allows agents to strategically explore the reasoning space with a proper balance between\\nexploration and exploitation.\\nFurther innovations in leveraging world models for agent learning include ActRe [ 127], which reverses the typical\\nreasoning-action sequence by first performing actions and then generating post-hoc explanations. This capability to\\nrationalize actions demonstrates LLMs’ inherent understanding of world dynamics, enabling autonomous trajectory\\nannotation and facilitating contrastive self-training.\\nThe importance of cognitive maps in world understanding is highlighted by [ 128], who show that structured mental rep-\\nresentations inspired by human cognition significantly enhance LLMs’ extrapolation capabilities in novel environments.\\nThese cognitive maps not only improve planning but also exhibit human-like characteristics such as structured mental\\nsimulation and rapid adaptation.\\nIn web-based environments, recent work by [ 107] and [ 129] demonstrates that LLMs can function as effective world\\nmodels for anticipating the outcomes of web interactions. By simulating potential state changes before executing\\nactions, these approaches enable safer and more efficient decision-making, particularly in environments where actions\\nmay be irreversible.\\nThrough systems like Reflexion [ 48] and ExpeL [ 69], agents have advanced experiential learning by autonomously\\nmanaging the full cycle of experience collection, analysis, and application, enabling them to learn effectively from both\\nsuccesses and failures.\\nThese developments collectively illustrate how world models are becoming increasingly central to agent learning\\nsystems, providing a foundation for understanding environmental dynamics and enabling more effective planning,\\nreasoning, and decision-making in complex, interactive environments.\\n2.2 Reasoning\\nReasoning represents the key to intelligent behavior, transforming raw information into actionable knowledge that drives\\nproblem-solving and decision-making. For both humans and artificial agents, it enables logical inference, hypothesis\\ngeneration, and purposeful interaction with the world. In human cognition, reasoning emerges through multiple\\nstrategies: deductive reasoning applies general rules to specific cases, inductive reasoning builds generalizations from\\nparticular instances, and abductive reasoning constructs plausible explanations from incomplete data [ 130,131]. These\\nprocesses are augmented by heuristics—mental shortcuts that streamline decision-making under uncertainty—and are\\ncontinuously refined through environmental feedback, ensuring that reasoning remains grounded in reality and adaptive\\nto change.\\nFor LLM-based agents, reasoning serves a parallel role, elevating them beyond reactive systems to proactive entities\\ncapable of sophisticated cognition. Through reasoning, these agents process multimodal inputs, integrate diverse\\nknowledge sources, and formulate coherent strategies to achieve objectives. The environment plays a dual function:\\nsupplying information that fuels reasoning and serving as the proving ground where reasoned actions are tested, creating\\na feedback loop that enables agents to validate inferences and learn from errors.\\nIn LLM-based agents, reasoning can be formally defined as the process of action selection based on mental states,\\nrepresenting a crucial bridge between perception and action. More precisely, given a mental state Mt at time t, reasoning\\ncan be formalized as a function R(Mt) →at, where at represents the selected action. This process operates across\\n31\\nStructured Reasoning Unstructured Reasoning\\ninput output\\ninput outputPlanning\\ninput input output\\nStatic\\nImplicitinput output\\nDynamicExplicit\\nReasoning Node Planned Node \\nConfirmed Path Potential Path\\nLanguage Space Latent SpaceFigure 2.2: Comparison of reasoning paradigms in LLM-based agents.\\nvarious environments—textual, digital, and physical worlds—where completing a task typically requires either a single\\nreasoning step or a composition of multiple reasoning actions.\\nThe composition of reasoning actions naturally leads to two distinct approaches: structured and unstructured reasoning.\\nStructured reasoning ( Rs) can be formalized as an explicit composition Rs=R1◦R2◦. . .◦Rn, where each Ri\\nrepresents a discrete reasoning step with clear logical dependencies. In contrast, unstructured reasoning ( Ru) takes a\\nmore holistic form Ru=f(Mt), where the composition remains implicit and flexible, allowing for dynamic adaptation\\nto context. This dual framework mirrors human cognition, where structured reasoning parallels our explicit logical\\ndeduction processes, while unstructured reasoning reflects our capacity for intuitive problem-solving and pattern\\nrecognition.\\nThe environment plays a crucial role in this formalization, serving both as a source of observations otthat influence\\nmental state updates ( Mt=L(Mt−1, at−1, ot)) and as a testing ground for reasoning outcomes. This creates a\\ncontinuous feedback loop where reasoning not only drives action selection but also influences how the agent’s mental\\nstate evolves, enabling iterative refinement of reasoning strategies through experience.\\nIn this section, we will examine how these reasoning approaches manifest in practice. We begin with structured\\nreasoning, which emphasizes systematic problem decomposition and multi-step logical chains. We then explore\\nunstructured reasoning, which allows for flexible response patterns and parallel solution exploration. Finally, we\\ninvestigate planning as a specialized form of reasoning that combines both structured and unstructured approaches for\\ntackling complex, long-horizon tasks.\\n2.2.1 Structured Reasoning\\nStructured reasoning represents a methodical approach to problem-solving that employs explicit organizational frame-\\nworks to guide the reasoning process. Unlike unstructured approaches, structured reasoning makes the composition of\\nreasoning steps explicit, which can be formalized as Rs=R1◦R2◦. . .◦Rn, where each Rirepresents a discrete\\nreasoning step with clear logical dependencies. In this formulation, each reasoning node is an explicitly executed\\ncomputational unit, and the connections between nodes represent definite information flow paths. This approach\\nenables more systematic exploration of solution spaces and facilitates more robust decision-making through deliberate\\nstep-by-step analysis, providing high interpretability and traceability throughout the reasoning process.\\n2.2.1.1 Dynamic Reasoning Structures\\nDynamic reasoning structures allow for the adaptive construction of reasoning paths during problem-solving, creating\\nversatile frameworks that can adjust based on intermediate results and insights.\\nLinear Sequential Reasoning Linear structures frame reasoning as a series of sequential steps, where each step builds\\non the one before. ReAct [ 70] illustrates this by combining reasoning traces with task-specific actions in an alternating\\nfashion. This combination allows for reasoning traces to guide and modify action plans while actions can access\\nexternal sources for further information. This mutual interaction improves both reasoning integrity and environmental\\nadaptation.\\n32\\nReasoning via Planning (RAP) [ 74] extends the linear reasoning paradigm by formulating LLM reasoning as a Markov\\ndecision process, though it was limited by states specifically designed for particular problems. The Markov Chain of\\nThought (MCoT) [ 71] extended this paradigm by conceptualizing each reasoning step as a Markovian state accompanied\\nby executable code. This approach enables efficient next-step inference without requiring a lengthy context window by\\ncompressing previous reasoning into a simplified math question. Atom of Thoughts [ 132] explicitly defined problems\\nas state representations and designed a general decomposition-contraction two-phase state transition mechanism to\\nconstruct Markovian reasoning processes, transforming complex problems into a series of atomic questions.\\nTree-Based Exploration Tree-based approaches expand beyond linear structures by organizing reasoning into hierar-\\nchical frameworks that support branching exploration. Tree of Thoughts (ToT) [ 72] introduces a structured approach\\nwhere complex problems are decomposed into intermediate steps, enabling breadth-first or depth-first search through\\nthe solution space. This allows the model to consider multiple reasoning paths simultaneously and systematically\\nexplore alternatives.\\nLanguage Agent Tree Search (LATS) [ 73] advances this paradigm by integrating Monte Carlo Tree Search (MCTS)\\nwith LLMs, using the environment as an external feedback mechanism. This approach enables more deliberate and\\nadaptive problem-solving by balancing exploration and exploitation through a sophisticated search process guided by\\nLLM-powered value functions and self-reflection.\\nReasoning via Planning (RAP) [ 74] further enhances tree-based reasoning by repurposing LLMs as both reasoning\\nagents and world models. Through this dual role, RAP enables agents to simulate the outcomes of potential reasoning\\npaths before committing to them, creating a principled planning framework that balances exploration with exploitation\\nin the reasoning space.\\nGraph-Based Reasoning Graph structures offer even greater flexibility by allowing non-hierarchical relationships\\nbetween reasoning steps. Graph of Thoughts (GoT) [ 75] extends tree-based approaches to arbitrary graph structures,\\nenabling more complex reasoning patterns that can capture interdependencies between different steps. This approach\\nallows for connections between seemingly disparate reasoning branches, facilitating more nuanced exploration of the\\nsolution space.\\nPath of Thoughts (PoT) [ 76] addresses relation reasoning challenges by decomposing problems into three key stages:\\ngraph extraction, path identification, and reasoning. By explicitly extracting a task-agnostic graph that identifies\\nentities, relations, and attributes within the problem context, PoT creates a structured representation that facilitates\\nthe identification of relevant reasoning chains, significantly improving performance on tasks requiring long reasoning\\nchains.\\nDiagram of Thought (DoT) [ 77] models iterative reasoning as the construction of a directed acyclic graph (DAG),\\norganizing propositions, critiques, refinements, and verifications into a unified structure. This approach preserves\\nlogical consistency while enabling the exploration of complex reasoning pathways, providing a theoretically sound\\nframework grounded in Topos Theory.\\n2.2.1.2 Static Reasoning Structures\\nStatic reasoning structures employ fixed frameworks that guide the reasoning process without dynamically adjusting the\\nstructure itself, focusing instead on improving the content within the established framework.\\nEnsemble Methods. Ensemble approaches leverage multiple independent reasoning attempts to improve overall\\nperformance through aggregation. Self-Consistency [ 78] pioneered this approach by sampling multiple reasoning paths\\nrather than relying on single greedy decoding, significantly improving performance through majority voting among the\\ngenerated solutions.\\nMedPrompt [ 133] demonstrates how domain-specific ensemble techniques can enhance performance by carefully\\ncrafting prompts that elicit diverse reasoning approaches, achieving state-of-the-art results on medical benchmarks\\nthrough systematic composition of prompting strategies.\\nLLM-Blender [ 134] introduces a sophisticated ensembling framework that leverages the diverse strengths of multiple\\nLLMs through pairwise comparison (PairRanker) and fusion (GenFuser) of candidate outputs. This approach enables\\nthe system to select the optimal model output for each specific example, creating responses that exceed the capabilities\\nof any individual model.\\nProgressive Improvement. Progressive improvement frameworks focus on iteratively refining reasoning through\\nstructured feedback loops. Self-Refine [ 67] implements an iterative approach where the model generates initial output,\\nprovides self-feedback, and uses that feedback to refine itself. This mimics human revision processes without requiring\\nadditional training or reinforcement learning, resulting in significant improvements across diverse tasks.\\n33\\nReflexion [ 48] extends this concept by integrating environmental feedback, enabling agents to verbally reflect on task\\nfeedback signals and maintain reflective text in an episodic memory buffer. This approach guides future decision-making\\nby incorporating insights from previous attempts, significantly enhancing performance in sequential decision-making,\\ncoding, and reasoning tasks.\\nProgressive-Hint Prompting (PHP) [ 79] further develops this paradigm by using previously generated answers as hints\\nto progressively guide the model toward correct solutions. This approach enables automatic multiple interactions\\nbetween users and LLMs, resulting in significant accuracy improvements while maintaining high efficiency.\\nError Correction. Error correction frameworks focus specifically on identifying and addressing mistakes in the\\nreasoning process. Self-Verification [ 80] introduces a self-critique system that enables models to backward-verify\\ntheir conclusions by taking the derived answer as a condition for solving the original problem, producing interpretable\\nvalidation scores that guide answer selection.\\nRefiner [ 135] addresses the challenge of scattered key information by adaptively extracting query-relevant content and\\nrestructuring it based on interconnectedness, highlighting information distinction and effectively aligning downstream\\nLLMs with the original context.\\nChain-of-Verification (CoVe) [ 81] tackles factual hallucinations through a structured process where the model drafts\\nan initial response, plans verification questions, independently answers those questions, and generates a final verified\\nresponse. This deliberate verification process significantly reduces hallucinations across a variety of tasks.\\nRecursive Criticism and Improvement (RCI) [ 128] enables LLMs to execute computer tasks by recursively criticizing\\nand improving their outputs, outperforming existing methods on the MiniWoB++ benchmark with only a handful of\\ndemonstrations per task and without task-specific reward functions.\\nCritic [ 68] extends this approach by integrating external tools for validation, enabling LLMs to evaluate and progressively\\namend their outputs like human interaction with tools. This framework allows initially “black box” models to engage in\\na continuous cycle of evaluation and refinement, consistently enhancing performance across diverse tasks.\\n2.2.1.3 Domain-Specific Reasoning Frameworks\\nDomain-specific reasoning frameworks adapt structured reasoning approaches to the unique requirements of particular\\ndomains, leveraging specialized knowledge and techniques to enhance performance in specific contexts.\\nMathPrompter [ 82] addresses arithmetic reasoning challenges by generating multiple algebraic expressions or Python\\nfunctions to solve the same math problem in different ways. This approach improves confidence in the output results by\\nproviding multiple verification paths, significantly outperforming state-of-the-art methods on arithmetic benchmarks.\\nPhysics Reasoner [ 84] addresses the unique challenges of physics problems through a knowledge-augmented framework\\nthat constructs a comprehensive formula set and employs detailed checklists to guide effective knowledge applica-\\ntion. This three-stage approach—problem analysis, formula retrieval, and guided reasoning—significantly improves\\nperformance on physics benchmarks by mitigating issues of insufficient knowledge and incorrect application.\\nPedagogical Chain-of-Thought (PedCoT) [ 83] leverages educational theory, particularly the Bloom Cognitive Model, to\\nguide the identification of reasoning mistakes in mathematical contexts. This approach combines pedagogical principles\\nfor prompt design with a two-stage interaction process, providing a foundation for reliable mathematical mistake\\nidentification and automatic answer grading.\\nThe evolution of structured reasoning in LLM agents reflects a growing understanding of how to enhance reasoning\\ncapabilities through explicit organizational frameworks. From linear sequences to complex graphs, and ensemble\\nmethods to specialized domain frameworks, these approaches demonstrate the power of structural guidance in improving\\nreasoning performance across diverse tasks and domains.\\n2.2.2 Unstructured Reasoning\\nIn contrast to structured reasoning approaches that explicitly organize reasoning steps, unstructured reasoning ( Ru) takes\\na holistic form Ru=f(Mt), where the composition remains implicit and flexible. In this mode, the reasoning process\\nis encapsulated within a single function mapping, without explicitly defining intermediate steps or state transitions. This\\napproach leverages the inherent capabilities of language models to generate coherent reasoning without enforcing rigid\\nstructural constraints, with intermediate reasoning processes occurring explicitly in the language space or implicitly\\nin the latent space. Unstructured reasoning methods have demonstrated remarkable effectiveness across diverse tasks\\nwhile maintaining simplicity and efficiency in implementation.\\n34\\n2.2.2.1 Prompting-Based Reasoning\\nThe most accessible way to elicit reasoning in LLM agents lies in carefully crafted prompts. By providing appropriate\\nreasoning demonstrations or instructing LLMs to perform inferential steps, agents can leverage their logical deduction\\ncapabilities to solve problems through flexible reasoning processes.\\nChain-of-Thought Variants. The cornerstone of prompting-based reasoning is Chain-of-Thought (CoT) prompt-\\ning [46], which operationalizes reasoning through few-shot examples with explicit generation of intermediate rational-\\nization steps. This foundational technique has inspired several evolutionary variants that enhance its basic approach.\\nZero-shot CoT [ 136] eliminates the need for demonstration examples through strategic prompting (e.g., “Let’s think\\nstep by step”), making the approach more accessible while maintaining effectiveness. Auto-CoT [ 137] automates the\\ncreation of effective demonstrations by clustering diverse questions and generating reasoning chains for representative\\nexamples from each cluster. Least-to-Most Prompting [ 138] addresses complex reasoning by decomposing problems\\ninto sequential sub-problems, enabling a progressive planning process that facilitates easy-to-hard generalization.\\nComplex CoT [ 139] further enhances reasoning depth by specifically selecting high-complexity exemplars as prompting\\ntemplates, better-equipping models to tackle intricate problems.\\nProblem Reformulation Strategies. Advanced prompting strategies demonstrate architectural innovations in reasoning\\nguidance by reformulating the original problem. Step-Back Prompting [ 85] implements abstraction-first reasoning\\nthrough conceptual elevation, enabling models to derive high-level concepts and first principles before addressing\\nspecific details. Experimental results demonstrate substantial performance gains on various reasoning-intensive\\ntasks, with improvements of 7-27% across physics, chemistry, and multi-hop reasoning benchmarks. Rephrase and\\nRespond [ 140] employ semantic expansion to transform original questions into more tractable forms, allowing models\\nto approach problems from multiple linguistic angles and identify the most effective problem formulation.\\nAbstraction-of-Thought [ 141] introduces a novel structured reasoning format that explicitly requires varying levels of\\nabstraction within the reasoning process. This approach elicits language models to first contemplate at the abstract level\\nbefore incorporating concrete details, a consideration overlooked by step-by-step CoT methods. By aligning models\\nwith the AoT format through finetuning on high-quality samples, the approach demonstrates substantial performance\\nimprovements across a wide range of reasoning tasks compared to CoT-aligned models.\\nEnhanced Prompting Frameworks. Several frameworks extend the basic prompting paradigm to create more\\nsophisticated reasoning environments. Ask Me Anything [ 86] constrains open-ended generation by reformulating tasks\\ninto structured question-answer sequences, enforcing focused reasoning trajectories. This approach recursively uses the\\nLLM itself to transform task inputs to the effective QA format, enabling open-source GPT-J-6B to match or exceed the\\nperformance of few-shot GPT3-175B on 15 of 20 popular benchmarks.\\nAlgorithm of Thoughts [ 142] proposes a novel strategy that propels LLMs through algorithmic reasoning pathways by\\nemploying algorithmic examples fully in context. This approach exploits the innate recurrence dynamics of LLMs,\\nexpanding their idea exploration with merely one or a few queries. The technique outperforms earlier single-query\\nmethods and even more recent multi-query strategies while using significantly fewer tokens, suggesting that instructing\\nan LLM using an algorithm can lead to performance surpassing that of the algorithm itself.\\nChain-of-Knowledge (CoK) [ 87] augments LLMs by dynamically incorporating grounded information from heteroge-\\nneous sources, resulting in more factual rationales and reduced hallucination. CoK consists of three stages: reasoning\\npreparation, dynamic knowledge adapting, and answer consolidation, leveraging both unstructured and structured\\nknowledge sources through an adaptive query generator. This approach corrects rationales progressively using preceding\\ncorrected rationales, minimizing error propagation between reasoning steps.\\nSelf-Explained Keywords (SEK) [ 88] addresses the challenge of low-frequency terms in code generation by extracting\\nand explaining key terms in problem descriptions with the LLM itself and ranking them based on frequency. This\\napproach significantly improves code generation performance across multiple benchmarks, enabling models to shift\\nattention from low-frequency keywords to their corresponding high-frequency counterparts.\\n2.2.2.2 Reasoning Models\\nRecent advances in language models have led to the development of specialized reasoning models designed explicitly\\nfor complex inferential tasks. These models are fine-tuned or specially trained to optimize reasoning capabilities,\\nincorporating architectural and training innovations that enhance their performance on tasks requiring multi-step logical\\ninference.\\nReasoning models like DeepSeek’s R1 [ 89], Anthropic’s Claude 3.7 Sonnet [ 9], and OpenAI’s o series models [ 90]\\nrepresent the frontier of reasoning capabilities, demonstrating remarkable proficiency across diverse reasoning bench-\\n35\\nmarks. These models are trained with specialized methodologies that emphasize reasoning patterns, often incorporating\\nsignificant amounts of human feedback and reinforcement learning to enhance their inferential abilities.\\nThe emergence of dedicated reasoning models reflects a growing understanding of the importance of reasoning\\ncapabilities in language models and the potential benefits of specialized training for these tasks. By concentrating\\non reasoning-focused training data and objectives, these models achieve performance levels that significantly exceed\\nthose of general-purpose language models, particularly on tasks that require complex logical inference, mathematical\\nreasoning, and multi-step problem-solving.\\n2.2.2.3 Implicit Reasoning\\nBeyond explicit reasoning approaches, recent research has explored the potential of implicit reasoning methods that\\noperate without overtly exposing the reasoning process. These approaches aim to improve efficiency by reducing the\\nnumber of tokens generated while maintaining or enhancing reasoning performance.\\nQuiet-STaR [ 91] generalizes the Self-Taught Reasoner approach by teaching LMs to generate rationales at each\\ntoken to explain the future text, improving their predictions. This approach addresses key challenges including\\ncomputational cost, the initial unfamiliarity with generating internal thoughts, and the need to predict beyond individual\\ntokens. Experimental results demonstrate zero-shot improvements in mathematical reasoning (5.9% →10.9%) and\\ncommonsense reasoning (36.3% →47.2%) after continued pretraining, marking a step toward LMs that learn to reason\\nin a more general and scalable way.\\nChain of Continuous Thought (Coconut) [ 92] introduces a paradigm that enables LLM reasoning in an unrestricted\\nlatent space instead of using natural language. By utilizing the last hidden state of the LLM as a representation of\\nthe reasoning state and feeding it back as the subsequent input embedding directly in the continuous space, Coconut\\ndemonstrates improved performance on reasoning tasks with fewer thinking tokens during inference. This approach\\nleads to emergent advanced reasoning patterns, including the ability to encode multiple alternative next reasoning steps,\\nallowing the model to perform a breadth-first search rather than committing to a single deterministic path.\\nRecent analysis [ 143] of implicit reasoning in transformers reveals important insights into its limitations. While\\nlanguage models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain\\ntests via implicit reasoning when trained on fixed-pattern data, implicit reasoning abilities emerging from training on\\nunfixed-pattern data tend to overfit specific patterns and fail to generalize further. These findings suggest that language\\nmodels acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns\\nwhile lacking broader generalization capabilities.\\nThe evolution of unstructured reasoning approaches demonstrates the remarkable adaptability of language models\\nto different reasoning paradigms. From simple prompting techniques to sophisticated implicit reasoning methods,\\nthese approaches leverage the inherent capabilities of LLMs to perform complex logical inferences without requiring\\nexplicit structural constraints. This flexibility enables more intuitive problem-solving while maintaining efficiency and\\neffectiveness across diverse reasoning tasks.\\n2.2.3 Planning\\nPlanning is a fundamental aspect of human cognition, enabling individuals to organize actions, anticipate outcomes,\\nand achieve goals in complex, dynamic environments [ 144]. Formally, planning can be described as the process of\\nconstructing potential pathways from an initial state to a desired goal state, represented as P:S0→ {a1, a2, . . . , a n} →\\nSg, where S0is the starting state, {a1, a2, . . . , a n}denotes a sequence of possible actions, and Sgis the goal state.\\nUnlike direct reasoning, planning involves generating hypothetical action sequences before execution, functioning as\\ncomputational nodes that remain inactive until deployed. This cognitive ability emerges from the interplay of specialized\\nneural circuits, including the prefrontal cortex, which governs executive control, and the hippocampus, which supports\\nepisodic foresight and spatial mapping. Insights from decision theory, psychology, and cybernetics—such as rational\\nframeworks, prospect theory, and feedback loops—demonstrate how planning allows humans to transcend reactive\\nbehavior, actively shaping their futures through deliberate intent and adaptive strategies. This capacity not only\\nunderpins intelligent behavior but also serves as a model for developing LLM-based agents that seek to replicate and\\nenhance these abilities computationally [145, 146].\\nIn human cognition, planning operates as a hierarchical process, integrating immediate decisions with long-term\\nobjectives. This reflects the brain’s modular architecture, where neural systems collaborate to balance short-term\\ndemands with future possibilities—a dynamic informed by control theory’s principles of stability and optimization.\\nSimilarly, LLM-based agents employ planning by leveraging their extensive linguistic knowledge and contextual\\nreasoning to transform inputs into actionable steps. Whether addressing structured tasks or unpredictable challenges,\\n36\\nthese agents emulate human planning by decomposing objectives, evaluating potential outcomes, and refining their\\nstrategies—blending biological inspiration with artificial intelligence. This section examines the theoretical foundations\\nand practical techniques of planning, from sequential approaches to parallel exploration, highlighting its critical role in\\nintelligent systems.\\nDespite the potential of LLMs in automated planning, their performance faces limitations due to gaps in world\\nknowledge [ 147]. LLMs often lack deep comprehension of world dynamics, relying on pattern recognition rather than\\ngenuine causal reasoning, which hinders their ability to manage sub-goal interactions and environmental changes [ 148].\\nAdditionally, their reliance on static pre-training data restricts adaptability in real-time scenarios, limiting their\\ngeneralization in dynamic planning tasks [ 149]. The absence of an intrinsic System 2 reasoning mechanism further\\ncomplicates their ability to independently generate structured, optimal plans [ 150]. However, researchers have proposed\\nstrategies such as task decomposition, search optimization, and external knowledge integration to mitigate these\\nchallenges.\\nTask Decomposition Task decomposition enhances LLM planning by breaking complex goals into smaller, manageable\\nsubtasks, reducing problem complexity and improving systematic reasoning. The Least-to-Most Prompting method\\n[138] exemplifies this approach, guiding LLMs to solve subproblems incrementally. ADaPT [ 151] further refines\\nthis strategy by dynamically adjusting task decomposition based on complexity and model capability, particularly in\\ninteractive decision-making scenarios. These methods also facilitate parallel subtask processing, backward error tracing,\\nand independence determination [132], providing a structured framework for reasoning.\\nIn LLM planning, tasks function as executable units—distinct from static state descriptions in formal mod-\\nels—emphasizing structured sequences that achieve intended outcomes [ 66]. These tasks vary in nature: some\\nare subproblems requiring specific solutions (e.g., solving equations within broader challenges), while others involve\\ntool invocation (e.g., querying APIs for weather data in travel planning) [ 152,153]. Alternatively, tasks may be\\nrepresented as graph nodes mapping dependencies, such as prioritizing objectives in project management [ 154]. By\\ndefining clear, modular goals, these formulations enhance reasoning and action efficiency, guiding agents through\\ncomplex problem spaces with greater precision [93].\\nSearching Given the stochastic nature of LLMs [ 155], parallel sampling combined with aggregated reasoning can\\nimprove inference performance. Task decomposition structures individual solution trajectories, enabling the construction\\nof a solution space that includes multiple pathways to a goal and their interrelationships [ 72,156]. This space allows\\nsampling diverse potential solutions [ 157], facilitating exploration through techniques like reflection, review, and\\nparallel sampling informed by existing knowledge [158].\\nComputational constraints often preclude exhaustive evaluation, making efficient navigation of the solution space\\nessential. Methods include tree search algorithms like LATS [ 159], heuristic approaches such as PlanCritic’s genetic\\nalgorithms [ 160], and CoT-SC, which identifies recurring solutions via self-consistency checks [ 78]. Reward-based\\nmodels like ARMAP assess intermediate and final outcomes to optimize planning [ 106]. This iterative exploration and\\nrefinement process enhances adaptability, ensuring robust strategies for complex problems.\\nWorld Knowledge Effective planning requires agents to navigate dynamic environments, anticipate changes, and\\npredict outcomes, underscoring the importance of world knowledge. RAP [ 74] examines the interplay between LLMs,\\nagent systems, and world models, positioning LLMs as dual-purpose entities: as world models, they predict state\\nchanges following actions [ 107,161]; as agents, they select actions based on states and goals [ 70]. This framework\\nmirrors human cognition—simulating action consequences before selecting optimal paths—and unifies language\\nmodels, agent models, and world models as pillars of machine reasoning [162].\\nAgents augment LLM capabilities by integrating external knowledge, addressing gaps in world understanding. ReAct\\nemploys an action-observation loop to gather environmental feedback, combining real-time data with linguistic\\nknowledge to improve decision-making in complex scenarios [ 70]. This enables LLMs to iteratively refine their\\nworld models during action execution, supporting adaptive planning. Conversely, LLM+P [ 163] integrates LLMs with\\nthe PDDL planning language, converting natural language inputs into formalized representations solved by classical\\nplanners [ 164,165]. This hybrid approach compensates for LLMs’ limitations in structured planning, merging their\\nlinguistic flexibility with the reliability of traditional systems.\\nFurther advancements enhance LLM planning through world knowledge integration. CodePlan [ 166] uses code-form\\nplans—pseudocode outlining logical steps—to guide LLMs through complex tasks, achieving notable performance\\nimprovements across benchmarks [ 167]. The World Knowledge Model (WKM) equips LLMs with prior task knowledge\\nand dynamic state awareness, reducing trial-and-error and hallucinations in simulated environments [ 168]. A neuro-\\nsymbolic approach combining Linear Temporal Logic with Natural Language (LTL-NL) integrates formal logic with\\n37\\nLLMs, leveraging implicit world knowledge to ensure reliable, adaptive planning [ 169]. Together, these methods\\nillustrate how structured frameworks and environmental understanding can transform LLMs into effective planners.\\n38\\nChapter 3\\nMemory\\nMemory is fundamental to both human and artificial intelligence. For humans, it serves as the bedrock of cognition, a\\nvast repository of experiences and knowledge that empowers us to learn, adapt, and navigate the complexities of the\\nworld. From infancy, our capacity to encode, store, and retrieve information underpins our ability to acquire language,\\nmaster skills, and build relationships. Decades of research in neuroscience and cognitive psychology have illuminated\\nthe multifaceted role of memory, revealing its influence on our sense of self, creative endeavors, and decision-making\\nprocesses. Similarly, in the burgeoning field of artificial intelligence, memory is increasingly recognized as a cornerstone\\nof intelligent behavior. Just as humans rely on past experiences to inform present actions, AI agents require robust\\nmemory mechanisms to tackle intricate tasks, anticipate future events, and adjust to dynamic environments. Therefore,\\na deep understanding of human memory – its organization, processes, and limitations – provides invaluable insights for\\nthe development of more capable and adaptable AI systems. This section will first provide a concise overview of human\\nmemory, focusing on the key stages of encoding, consolidation, and retrieval. We will then transition to exploring the\\ndiverse approaches employed in designing AI agent memory systems, ranging from traditional symbolic representations\\nto cutting-edge neural network-based methods. A critical comparison between these artificial memory systems and\\ntheir human counterparts will highlight existing gaps in areas such as adaptability, contextual understanding, and\\nresilience. Finally, we will consider how principles derived from neuroscience and cognitive psychology can inform\\nfuture research, suggesting directions that may lead to the creation of artificial memory systems that exhibit greater\\nrobustness, nuance, and ultimately, a closer resemblance to the remarkable capabilities of human memory.\\n3.1 Overview of Human Memory\\n3.1.1 Types of Human Memory\\nHuman memory is often conceptualized as a multi-tiered system that captures, stores, and retrieves information at\\ndifferent levels of processing and timescales. Researchers from the fields of cognitive science, neuroscience, and\\npsychology have proposed various models to describe these levels. A commonly accepted hierarchy distinguishes\\nbetween sensory memory, short-term memory (including working memory), and long-term memory [ 170,171]. Within\\nlong-term memory, explicit (declarative) and implicit (non-declarative) forms are further delineated [ 172]. Figure 3.1\\nillustrates one such hierarchical framework:\\n•Sensory Memory. Sensory memory is the initial, brief store of raw sensory information. It maintains inputs\\nfrom the environment for a duration ranging from milliseconds to a few seconds, allowing subsequent processes\\nto determine which portions of the stimulus are relevant for further analysis [ 173]. Iconic memory (for visual\\ninput) [174] and echoic memory (for auditory input) [175] are two well-known subtypes.\\n•Short-Term Memory and Working Memory. Short-term memory (STM) involves holding a limited amount\\nof information in an easily accessible state for seconds to under a minute. The term working memory is often\\nused to emphasize the manipulation of that information rather than mere maintenance. While some models\\ntreat working memory as a subset of STM, others view it as a distinct system that manages both the storage and\\nactive processing of data (for instance, performing arithmetic in one’s head) [ 176,177]. The capacity of STM\\nor working memory is finite, typically cited as around seven plus or minus two chunks of information [ 98],\\nthough individual differences and task factors can modulate this figure.\\n39\\nHuman\\nMemory\\nLong-Term\\nMemory\\nDeclarative\\n(Explicit) Memory\\nSemantic\\nMemoryEpisodic\\nMemory\\nAutobiographical\\nMemoryNon-Declarative\\n(Implicit) Memory\\nProcedural\\nMemoryPriming Classical\\nConditioningNon-Associative\\nMemorySensory\\nMemoryShort-Term\\nMemory\\nWorking\\nMemory\\nFigure 3.1: The hierarchical taxonomy of human memory system.\\n•Long-Term Memory (LTM). Long-term memory accommodates the more durable storage of information that\\ncan persist from hours to decades [ 178,179]. This repository supports the learning of skills, the acquisition of\\nfactual knowledge, and the recollection of personal experiences. Although long-term memory is sometimes\\ndescribed as having a vast or near-unlimited capacity, factors such as decay, interference, and retrieval cues\\ninfluence the extent to which stored information can be accessed [180].\\n–Declarative (Explicit) Memory. Declarative memory encompasses memories that can be consciously\\nrecalled and articulated [181]. Within this broad category, researchers often discuss:\\n*Semantic Memory: Factual knowledge about the world, including concepts, words, and their\\nrelationships [ 182]. Examples include recalling the meaning of vocabulary terms or knowing the\\ncapital city of a country.\\n*Episodic Memory: Personally experienced events that retain contextual details such as time, place,\\nand the people involved [ 183]. This form of memory allows individuals to mentally travel back in\\ntime to relive past experiences.\\n*Autobiographical Memory: A form of episodic memory focusing on events and experiences related\\nto one’s personal history [ 184]. While sometimes treated as a sub-category of episodic memory,\\nautobiographical memory places particular emphasis on the self and its evolving life narrative.\\n–Non-Declarative (Implicit) Memory. Non-declarative memory refers to memories that influence\\nbehavior without the need for conscious awareness [185]. Key subtypes include:\\n*Procedural Memory: The gradual acquisition of motor skills and habits (e.g., riding a bicycle, typing\\non a keyboard) that become automatic with repetition [186, 187].\\n*Priming: The phenomenon in which prior exposure to a stimulus influences subsequent responses,\\noften without explicit recognition of the previous encounter [188].\\n*Classical Conditioning: The learned association between two stimuli, where one stimulus comes to\\nelicit a response originally produced by the other [189].\\n*Non-Associative Memory: Adaptive modifications in behavior following repeated exposure to a\\nsingle stimulus. Habituation (reduced response to a repeated, harmless stimulus) and sensitization\\n(increased response after exposure to a noxious or intense stimulus) are representative examples [ 190,\\n191].\\nDespite the orderly appearance of these categories, human memory processes often overlap. For example, autobiograph-\\nical memory is typically nested within episodic memory, yet its particular focus on self-relevant experiences leads some\\ntheorists to treat it as a slightly different category. Similarly, the boundary between short-term and working memory\\ncan differ depending on the theoretical perspective. Some scholars prefer a more functional, process-oriented view of\\nworking memory, while others employ a strictly capacity-oriented concept of short-term storage. In each case, these\\ndifferent perspectives on memory highlight the complexity and nuance of human cognition.\\n40\\n3.1.2 Models of Human Memory\\nHuman memory has inspired a wide range of theoretical models, each offering different insights into how information\\nis acquired, organized, and retrieved. Although no single framework commands universal agreement, several influential\\nperspectives have shaped the discourse in cognitive science, neuropsychology, and AI research. The following content\\nhighlights some of the most prominent models and architectures used to explain memory’s multiple facets.\\nSensoryMemoryShort-Term /Working MemoryLong-Term MemoryEnvironmentalstimuliAttentionRetrievalTransfer\\nRehearsal \\nUnattended information is lostUnrehearsed information is lostSome information is forgotten over time (decay or retrieval failure)\\nFigure 3.2: Atkinson-Shiffrin three-stage model of human memory [170].\\nThe Multi-Store (Modal) Model. A seminal proposal by Atkinson and Shiffrin [ 170] introduced the multi-store or\\n“modal” model, which posits three main stores for incoming information: sensory memory ,short-term memory , and\\nlong-term memory . Control processes (e.g., attention, rehearsal) regulate how data transitions across these stores.\\nFigure 3.2 illustrates this model of memory. Despite its relative simplicity, this model remains foundational for\\nunderstanding how fleeting sensory impressions eventually form stable, long-lasting representations.\\nLong-Term MemoryCentral ExecutivePhonological LoopVisuospatial SkeptchpadEpisodic BufferLanguageVisual SemanticsShort-term Episodic Memory\\nFigure 3.3: Baddeley’s model of working memory [192].\\nWorking Memory Models. Recognizing that short-term memory also involves active maintenance, Baddeley and\\nHitch [ 192] proposed a working memory framework emphasizing the dynamic manipulation of information. Their\\noriginal model described a central executive that coordinates two subsystems: the phonological loop (verbal) and the\\nvisuospatial sketchpad (visual/spatial). A subsequent refinement introduced the episodic buffer to integrate material\\nfrom these subsystems with long-term memory [ 193]. Figure 3.3 shows the framework of the working memory model.\\nAlternatives such as Cowan’s embedded-processes model [ 194] similarly underscore the role of attention in governing\\nhow information is briefly sustained and manipulated.\\nSerial-Parallel-Independent (SPI) Model. Initial distinctions between episodic, semantic, and procedural memory\\nwere championed by Tulving [ 195], who later refined his ideas into the Serial-Parallel-Independent (SPI) model, as\\nshown in Figure 3.4. In this framework, memory is divided into two overarching systems. The cognitive representation\\nsystem handles perceptual input and semantic processes, encompassing facts, concepts, and contextual (episodic)\\nknowledge. The action system , by contrast, underpins procedural skills such as dance routines, driving maneuvers, or\\ntyping proficiency. Tulving’s SPI model posits that memory formation can occur at multiple levels: strictly perceptual\\nencoding can support rudimentary episodic memories, while richer episodic representations benefit from semantic\\n41\\nProcedural MemoryPerceptual Representation SystemsSemantic MemoryWorking MemoryEpisodic Memory\\nAction SystemCognitiveRepresentationSystemsFigure 3.4: The Serial-Parallel Independent (SPI) model of human memory [195].\\nmediation. For instance, patients with semantic dementia, who struggle to retain word meanings, can still form some\\nepisodic memories but often lack the full contextual detail conferred by intact semantic networks. By highlighting the\\nrole of procedural memory and its automatic, intuitive nature, the SPI model aims to integrate structure (the content of\\nmemory) and function (how memory is used), surpassing earlier accounts that largely focused on explicit storage and\\nretrieval. Despite these strengths, critics note that the model under-specifies how working memory operates within the\\nbroader system, and the feedback mechanisms connecting cognitive and action subsystems remain loosely defined.\\nGlobal Workspace Theory (GWT) and the IDA/LIDA Framework. Global Workspace Theory (GWT), developed\\nby Baars [ 196], conceptualizes consciousness and working memory as a “broadcast” mechanism that distributes\\ninformation to specialized processors. Building on GWT, Franklin [ 197,198] proposed the IDA (Intelligent Distribution\\nAgent) model, later extended to LIDA (Learning IDA) , as a comprehensive cognitive architecture. In these frameworks,\\nmultiple memory systems (e.g., perceptual, episodic, procedural) interact through iterative “cognitive cycles”, with\\na global workspace functioning as a hub for attention and decision-making. From an AI standpoint, IDA/LIDA\\ndemonstrates how human-like memory processes can be operationalized to guide an agent’s perception, action selection,\\nand learning.\\nACT-R and Cognitive Architectures. ACT-R (Adaptive Control of Thought—Rational) [ 199] is a comprehensive\\ncognitive architecture that integrates memory, perception, and motor processes into a unified theoretical framework. It\\nhas been applied extensively across diverse domains, including learning and memory, problem-solving, decision-making,\\nlanguage comprehension, perception and attention, cognitive development, and individual differences. Figure 3.5\\nillustrates the processes of ACT-R. At the core of ACT-R are distinct modules (e.g., visual, manual, declarative,\\nprocedural) that interact with the system through dedicated buffers . Declarative memory stores factual “chunks,”\\nwhile procedural memory encodes if–then production rules for actions and strategies. Cognition unfolds via a pattern\\nmatcher that selects a single production to fire based on the current buffer contents. This symbolic production system is\\naugmented by subsymbolic processes, guided by mathematical equations that dynamically regulate activations, retrieval\\nlatencies, and production utilities. By combining symbolic and subsymbolic levels, ACT-R provides a mechanistic\\naccount of how individuals acquire, retrieve, and apply knowledge—thus shedding light on empirical phenomena such\\nas reaction times, error patterns, and the shaping of learning over time.\\nEach of the aforementioned models illuminates different aspects of memory. The multi-store model provides a\\nstraightforward introduction to storage stages, working memory models emphasize active maintenance and manipulation,\\nand frameworks such as IDA/LIDA or ACT-R embed memory within a comprehensive view of cognition. In practice,\\nresearchers often draw upon multiple perspectives, reflecting the intricate nature of human memory and its integral role\\nin perception, learning, and adaptive behavior.\\n3.2 From Human Memory to Agent Memory\\nHaving established the fundamentals of human memory, we now focus on how Large Language Model (LLM)-based\\nagents manage and store information. Memory is not merely a storage mechanism but is fundamental to human and\\n42\\nSensory ModulesAuditoryVisualMotor ModuleSensory BuffersMotor BufferGoal BufferRetrieval BufferMatchingSelectionExecutionProductionSystemIntentional ModuleDeclarative Module\\nAuditory WorldPhysical WorldModulesBuﬀers\\nEnvironmentFigure 3.5: An abstraction of the most important processes in the ACT-R model [199].\\nartificial intelligence. Memory underpins cognition, enabling learning, adaptation, and complex problem-solving for\\nhumans. Similarly, for LLM-based agents, memory provides the crucial scaffolding for maintaining context, learning\\nfrom experience, and acting coherently over time. Without memory, even a highly capable LLM would struggle to\\nadapt to changing circumstances or maintain focus during extended interactions.\\nWhile LLM-based agents and biological systems differ fundamentally, the principles guiding human memory—context\\nretention, selective forgetting, and structured retrieval—are highly relevant to agent design. Therefore, examining the\\nparallels and distinctions between human and artificial memory is beneficial. Functionally, we can draw analogies: an\\nagent’s short-term memory buffer resembles the prefrontal cortex’s role in working memory, while long-term storage in\\na vector database is akin to the hippocampus’s function in consolidating episodic memories. Agent memory design\\ncan benefit from emulating human memory’s mechanisms, including selective attention, prioritized encoding, and\\ncue-dependent retrieval. However, crucial differences exist.\\nHuman memory, built upon biological neural networks, integrates storage and computation within neurons’ connections\\nand activity patterns. This offers a high degree of parallelism and adaptability. In contrast, current agent memory\\nsystems predominantly rely on digital storage and algorithms, using symbolic representations and logical operations,\\nthus separating storage and computation. This impacts information processing: human memory is associative and\\ndynamic, capable of fuzzy matching and creative leaps, while current agent memory relies on precise matching and\\nvector similarity, struggling with ambiguity. Although digital storage capacity is vast, it cannot yet replicate the\\ncomplexity and dynamism of human memory, particularly in nuanced pattern recognition and long-term stability.\\nHuman memory, while imperfect, excels at extracting crucial information from noisy data. Agent memory systems, in\\ntheir current stage, are still nascent compared to the intricacies of human memory, facing limitations in organization,\\nintegration, adaptive forgetting, and knowledge transfer.\\nThe need for a dedicated memory module in LLM-based agents is paramount. While external knowledge bases\\n(databases, search engines, APIs) [ 200] provide valuable information, they do not capture the agent’s internal reasoning,\\npartial inferences, or task-specific context. An agentic memory system internalizes interim steps, evolving objectives,\\nand historical dialogue, enabling self-referential exploration and adaptation. This is crucial for tasks requiring the agent\\nto build upon prior judgments or maintain a personalized understanding of user goals.\\n43\\nEarly approaches to agent memory, such as appending conversation history to the input prompt (a rudimentary form of\\nworking memory) [ 201], have evolved. Modern architectures employ more sophisticated techniques, including vector\\nembeddings for rapidly retrieving memories [ 202] and selective incorporation of reasoning chains into subsequent\\ninference steps [ 203,204]. These diverse methods share the common goal of managing a large information reservoir\\nwithout compromising system responsiveness.\\nHowever, compared to the sophistication of human memory, current agentic methods have limitations. Many systems\\nlack coherent strategies for long-term memory consolidation, leading to cluttered logs or abrupt information loss.\\nThe flexible, bidirectional interplay between stored knowledge and ongoing processing, characteristic of human\\nworking memory, is often absent. Metacognitive oversight—selective recall, forgetting, and vigilance against outdated\\ninformation—is also underdeveloped in LLM-based agents. Balancing comprehensive recall with practical efficiency,\\nas humans do, remains a key challenge.\\nBuilding robust and adaptable memory for LLM-based agents involves addressing three core research questions: First,\\nhow should memory be represented to capture diverse information types and facilitate efficient access? Second, how\\ncan agent memory evolve, incorporating new experiences, adapting to changing contexts, and maintaining consistency?\\nFinally, how can the stored memories effectively enhance reasoning, decision-making, and overall agent performance?\\nThe following sections delve into these crucial areas, exploring current approaches, limitations, and potential future\\ndirections.\\n3.3 Representation of Agent Memory\\nInspired by human cognitive systems [ 285], current memory architecture in intelligent agents adopts a hierarchical\\nframework that integrates perception through sensory memory [ 205], real-time decision-making via short-term mem-\\nory [ 286,287], and sustained knowledge retention through long-term memory [ 288,289,48]. This multi-layered\\nstructure equips agents to manage immediate tasks while maintaining a broader contextual understanding, fostering\\nadaptability and seamless continuity across diverse interactions.\\nSpecifically, the memory system transforms raw environmental inputs into structured, actionable representations.\\nSensory memory acts as the gateway, capturing and selectively filtering perceptual signals to provide a foundation for\\ncognitive processing. Short-term memory bridges these immediate perceptions with task-level understanding, buffering\\nrecent interactions and enabling dynamic adaptation through experience replay and state management. Long-term\\nmemory then consolidates and stores information over extended periods, facilitating cross-task generalization and the\\naccumulation of enduring knowledge.\\nTogether, these memory components form a cohesive cycle of perception, interpretation, and response. This cycle\\nsupports real-time decision-making and enables agents to learn and evolve continuously, reflecting an intricate balance\\nbetween responsiveness and growth. The following delves into the formulation of each memory type, exploring their\\nunique roles and interactions within the agent’s cognitive architecture.\\n3.3.1 Sensory Memory\\nIn human cognitive systems, sensory memory serves as a mechanism for collecting information through the\\nsenses—touch, hearing, vision, and others—and is characterized by its extremely brief lifespan. Analogously, sensory\\nmemory functions as the embedded representation of inputs such as text, images, and other perceptual data in intelligent\\nagents. It represents the initial phase of environmental information processing, acting as a gateway for transforming raw\\nobservations into meaningful representations for further cognitive processing.\\nSensory memory in intelligent agents transcends passive information reception. It dynamically encodes and filters\\nperceptual signals, bridging immediate sensory inputs with the agent’s internal state, objectives, and prior knowledge.\\nThis adaptive process facilitates rapid perception of environmental changes, task continuity, and real-time context-aware\\ninformation processing. Sophisticated attention mechanisms are employed to ensure relevance and focus in the sensory\\nmemory layer, forming a critical foundation for decision-making and adaptation.\\nFormally, sensory memory formation consists of three sequential steps: perceptual encoding ,attentional selection ,\\nandtransient retention . First, perceptual encoding transforms raw sensory signals into processable representations,\\nmathematically expressed as:\\nϕ(ot) =Encode (ot, st) (3.1)\\nwhere otis the sensory input at time t, and strepresents the agent’s state. For instance, RecAgent [ 205] employs an\\nLLM-based sensory memory module to encode raw observations while filtering noise and irrelevant content. Extending\\n44\\nMemoryRepresentationSensory Text-basedRecAgent [ 205] CoPS [ 206] Memo-\\nryBank [ 207] Memory Sandbox [ 208]\\nMulti-modalVideoAgent [ 209] WorldGPT [ 210] Agent\\nS [211] OS-Copilot [ 212] MuLan [ 213]\\nShort-term ContextMemGPT [ 214] KARMA [ 215]\\nLSFS [ 216] OSCAR [ 217] RCI [ 128]\\nWorkingGenerative Agent [ 50] RLP [ 218]\\nCALYPSO [ 219] HiAgent [ 220]\\nLong-termSemantic AriGraph [ 221] RecAgent [ 205] HippoRAG [ 222]\\nEpisodicMobileGPT [ 223] MemoryBank [ 207]\\nEpisodic Verbalization [ 224] MrSteve [ 225]\\nProcedural AAG [ 226] Cradle [ 227] JARVIS-1 [ 228] LARP [ 229]\\nLifecycleAcquisitionInformation\\nCompressionHiAgent [ 220] LMAgent [ 230] ReadAgent [ 231] M2WF [ 232]\\nExperience\\nConsolidationExpeL [ 69] MindOS [ 233] [234] [235]\\nEncodingSelective\\nAttentionAgentCorrd [ 236] MS [ 237] GraphVideoA-\\ngent [ 238] A-MEM [ 239] [240]\\nMulti-modal\\nFusionOptimus-1 [ 241] Optimus-2 [ 242] JARVIS-1 [ 228]\\nDerivationReflectionAgent S [ 211] OSCAR [ 217]\\nR2D2 [ 243] Mobile-Agent-E [ 244]\\nSummarization SummEdits [ 245] SCM [ 246] Healthcare Copilot [ 247] [248]\\nKnowledge\\nDistillationKnowagent [ 249] AoTD [ 250] LDPD [ 251]\\nSub-goal Distillation [ 252] MAGDi [ 253]\\nSelective\\nForgettingLyfe Agent [ 254] TiM [ 203] Mem-\\noryBank [ 207] S3[255] [235]\\nRetrieval IndexingHippoRAG [ 222] TradingGPT [ 256]\\nLongMemEval [ 257] SeCom [ 258]\\nMatching Product Keys [ 259] OSAgent [ 260] [261] [235]\\nNeural\\nMemoryAssociative\\nMemoryHopfield Networks [ 262,263] Neural Turing Machines [ 264]\\nParameter\\nIntegrationMemoryLLM [ 265] SELF-PARAM [ 266] Mem-\\noRAG [ 267] TTT-Layer [ 268] Titans [ 269] R3Mem [ 270]\\nUtilizationRAG RAGLAB [ 271] Adaptive Retrieval [ 272] Atlas [ 273] [274]\\nLong-context\\nModelingRMT [ 275,276] AutoCompressor [ 277]\\nICAE [ 278] Gist [ 279] CompAct [ 280]\\nAlleviating\\nHallucinationLamini [ 281] Memoria [ 282] PEER [ 283] [284]\\nFigure 3.6: Tree diagram of the memory module in intelligent agents.\\nbeyond text-based perception, multimodal sensory memory systems such as Jarvis-1 [ 228], VideoAgent [ 209], and\\nWorldGPT [210] integrate multimodal foundation models to process diverse modality inputs.\\n45\\nNext, attentional selection extracts crucial information from the encoded sensory data. This process, guided by an\\nattention mechanism, is represented as:\\nαt=Attention (ϕ(ot), ct) (3.2)\\nwhere ϕ(ot)is the encoded input, and ctdenotes contextual information influencing attention. For example, RecA-\\ngent [ 205] employs an attention mechanism with an importance scoring system that assigns relevance scores to\\ncompressed observations, prioritizing critical inputs such as item-specific interactions while de-emphasizing less\\nsignificant actions. This helps extract high-priority information for memory retention.\\nFinally, transient retention temporarily stores the selected sensory information as sensory memory:\\nMsensory ={αt|t∈[t−τ, t]} (3.3)\\nSeveral strategies have been implemented to manage the time window. For instance, RecAgent [ 205] models retention\\nby associating each observation with the timestamp corresponding to the start of a simulation round in the user behavior\\nsimulation environment, represented as a triplet ⟨observation, importance score, timestamp ⟩. Similarly, CoPS [ 206]\\nemploys a fixed-size sensory memory pool as a time window, which consists of user search requests for personalized\\nsearch, facilitating “re-finding” behavior. When a new query is received, the system first checks the sensory memory for\\nrelevant matches. If a match is found, the query is classified as a re-finding instance, enabling a rapid sensory response.\\n3.3.2 Short-Term Memory\\nShort-term memory in cognition-inspired intelligent agents serves as a transient and dynamic workspace that bridges\\nsensory memory and long-term memory. It is essential for storing and processing task-relevant information and recent\\ninteraction sequences, supporting real-time decision-making and adaptive behavior. Inspired by human short-term\\nand working memory, it temporarily retains information to facilitate complex cognitive tasks, ensuring continuity and\\ncoherence in the agent’s operations.\\nShort-term memory in intelligent agents can be categorized into context memory andworking memory . On the one\\nhand, context memory treats the context window as the short-term memory of LLMs. For example, MemGPT [ 214],\\ninspired by hierarchical memory systems in operating systems, manages different storage tiers to extend context beyond\\nthe LLM’s inherent limitations. [ 290] introduces a neurosymbolic context memory that enhances LLMs by enabling\\nsymbolic rule grounding and LLM-based rule application.\\nOn the other hand, working memory involves fetching and integrating relevant external knowledge to hold essential\\ninformation during an agent’s operation. Generative Agent [ 50] employs short-term memory to retain situational\\ncontext, facilitating context-sensitive decision-making. Reflexion [ 48] utilizes a sliding window mechanism to capture\\nand summarize recent feedback, balancing detailed immediate experiences with high-level abstractions for enhanced\\nadaptability. RLP [ 218] maintains conversational states for speakers and listeners, using them as short-term memory\\nprompts to support dialogue understanding and generation.\\nFor interactive and creative game scenarios, CALYPSO [ 219] assists Dungeon Masters in storytelling for Dungeons\\n& Dragons by constructing short-term memory from scene descriptions, monster details, and narrative summaries,\\nenabling adaptive storytelling and dynamic engagement. Similarly, Agent S [ 211] and Synapse [ 291], designed for\\nGUI-based autonomous computer interaction, define their short-term memory as task trajectories, including actions\\nsuch as button clicks and text inputs. This formulation supports behavioral cloning and enhances adaptation in novel\\nGUI navigation tasks.\\nIn robotics applications, SayPlan [ 292] leverages scene graphs and environmental feedback as short-term memory to\\nguide planning and execution in scalable robotic environments. KARMA [ 215] engages short-term working memory\\nwith an effective and adaptive memory replacement mechanism to dynamically record changes in objects’ positions and\\nstates. LLM-Planner [ 293] iteratively updates short-term memory with environmental observation to prompt an LLM\\nfor dynamic planning.\\n3.3.3 Long-Term Memory\\nLong-term memory in cognition-inspired intelligent agents enables the retention and retrieval of information over\\nextended periods, allowing agents to generalize knowledge and adapt to new contexts effectively. Unlike sensory\\nand short-term memory, which handle transient or immediate data, long-term memory supports cumulative learning\\nand cross-task adaptability. It mirrors human long-term memory by incorporating explicit and implicit components,\\nfacilitating richer contextual understanding and intuitive behavior.\\nOn the one hand, explicit memory involves intentional recollection, analogous to declarative memory in humans. It\\nconsists of semantic memory , which stores general knowledge such as facts and concepts, and episodic memory ,\\n46\\nwhich records specific events and interaction histories. Semantic memory in intelligent agents can be preloaded from\\ndomain knowledge bases or dynamically acquired through interactions. For example, in environments like TextWorld,\\nsemantic memory captures structured facts, such as “ Recipe −contains −Tuna ” or “ Recipe −is on−Table ”. Episodic\\nmemory, in contrast, logs situational context and sequential actions, such as “go from kitchen to living room, then to\\ngarden”. Integrating semantic and episodic memory allows agents to retain static and contextual information, enabling\\nhuman-like adaptability and context-aware responses.\\nOn the other hand, implicit memory shapes agent behavior through procedural memory andpriming . Procedural memory\\nenables agents to perform repetitive tasks efficiently by recalling specific skills and reusable plans. For example, it\\nautomates routine tasks without requiring explicit instructions, improving task execution efficiency. Priming, meanwhile,\\ncaptures state changes and corresponding responses, allowing agents to adapt to similar contexts quickly. Priming\\nenhances fluidity and context-sensitive decision-making by directly matching observations to or continuously chaining\\nactions. Implicit memory, shaped by interactions with cognitive modules, enables rapid adaptation, often after minimal\\nexposure to new stimuli.\\nMost intelligent agents implement both semantic and episodic memory within their memory modules. For instance,\\nAgent S [ 211], designed for GUI automation tasks, incorporates semantic memory to store online web knowledge\\nin natural language form, while episodic memory captures high-level, step-by-step task experiences. Similarly,\\nAriGraph [ 221], targeting embodied simulation tasks, encodes semantic environment knowledge using a fact graph\\nand logs episodic navigation history through an event graph. In AI companion systems like MemoryBank [ 207] for\\nSiliconFriend, semantic memory constructs user portraits in natural language, while episodic memory retains interaction\\nhistories, enhancing personalized and context-aware behavior.\\nFor implementing implicit memory, current agent systems primarily adopt model-friendly memory formats, such\\nas key-value pair storage, executable code, or reusable routines. For example, AAG [ 226] defines and generalizes\\nprocedures through analogy, mapping knowledge from one situation (base) to another (target). This structure can be\\nrepresented as a linear directed chain graph, where the input serves as the root, the output as the leaf node, and each\\nintermediate step as a node in the chain. Similarly, Cradle [ 227] and Jarvis-1 [ 228] implement procedural memory by\\nstoring and retrieving skills in code form, which can be either learned from scratch or pre-defined. Once curated, skills\\ncan be added, updated, or composed within memory. The most relevant skills for a given task and context are then\\nretrieved to support action planning.\\n3.4 The Memory Lifecycle\\nIn this section, we introduce the lifecycle of memory in AI agents, as depicted in Figure 3.7. The lifecycle comprises a\\ndual process of retention and retrieval. Retention includes acquisition, encoding, and derivation, while retrieval involves\\nmemory matching, neural memory networks, and memory utilization.\\n3.4.1 Memory Acquisition\\nMemory Acquisition is the foundational process by which intelligent agents take in raw perceptual information from\\ntheir environment. This initial step is crucial for subsequent learning, adaptation, and decision-making [ 305]. A\\nprimary challenge in acquisition is the sheer volume and complexity of environmental inputs. Agents are constantly\\nbombarded with visual, auditory, textual, and other forms of data, much of which is redundant or irrelevant to the agent’s\\ngoals. Therefore, a core aspect of memory acquisition is not simply capturing data, but also initiating a preliminary\\nfiltering process. This filtering leverages two primary mechanisms: initial information compression andexperience\\nconsolidation .\\nAt this early stage, information compression involves rudimentary techniques to reduce data dimensionality. This\\nmight include downsampling images, extracting key phrases from text using simple heuristics, or identifying significant\\nchanges in audio streams [ 306]. The goal is rapid, lossy compression to prioritize potentially relevant information. For\\nexample, LMAgent [ 230] prompts the LLM to perform information compression, reducing irrelevant and unimportant\\ncontent when constructing sensory memory to enhance operational efficiency. Meanwhile, ReadAgent [ 231] and\\nGraphRead [ 307] respectively employ different strategies for compressing long text, i.e., episode pagination and\\ngraph-based structuring, to maximize information retention while ensuring efficiency.\\nOn the other hand, experience consolidation, even at the acquisition phase, plays a role. The agent doesn’t yet have\\na rich memory, but it can begin to apply previously learned, very general rules or biases. For example, if the agent\\nhas a pre-existing bias towards moving objects, it might prioritize visual data containing motion, even before full\\nencoding [ 308]. To enhance the dynamic consolidation of memory-based experiences, [ 235] define metrics such\\nas contextual relevance and recall frequency to determine whether to update long-term memory in a vector database.\\n47\\nMethod DomainMemory Representation Memory Lifecycle\\nSensory Short-term Long-term Acquisition Encoding Derivation Retrieval Utilization\\nSynapse [291] GUIMulti-\\nmodalContextEpisodic,\\nProceduralUser demo. -Hierarch.\\nDecomp.- -\\nAgent S [211] GUIMulti-\\nmodalContext,\\nWorkingSemantic,\\nEpisodicInfo.\\nCompress.Contrastive\\nLearn.Select.\\nForget.IndexingLong-\\ncontext\\nAutomanual [108] GUIMulti-\\nmodalContextProcedural,\\nEpisodicUser\\nDemo.Hierarch.\\nParseGoal\\nDecomp.Task\\nSearchSubgoal\\nExec.\\nAutoGuide [294] GUIMulti-\\nmodalContext -Screen\\nCapture- Action Plan -Action\\nExec.\\nAgent-Pro [295] GUIMulti-\\nmodalContext -Screen\\nCapture-Hierarch.\\nDecomp.-Action\\nExec.\\nMemGPT [214] Document TextContext,\\nWorking-External\\nData- -Paging,\\nFunc. callDoc.\\ninteract.\\nSeeAct [296] WebMulti-\\nmodalContext -Screen\\nCapture- Action Plan -Web\\nInteract.\\nAutoWebGLM\\n[297]Web Text Context -HTML\\nParseHTML\\nEmbedHTML\\nAnalysis-Web\\nInteract.\\nSteP [298] Web Text Context Task-spec.HTML\\nParseHTML\\nEmbedHTML\\nAnalysisElement\\nRankWeb\\nInteract.\\nAWM [299] Web Text - ProceduralWorkflow\\nExtract.Action\\nSumm.-Sim.\\nlookupWorkflow\\nexec.\\nAriGraph [221] TextWorld Text -Semantic,\\nEpisodicEnv.\\nObserv.Knowl.\\nGraphGraph\\nTraversalAssoc.\\nRetrievalAction\\nplan.\\nMemoryBank [ 207]Dialogue Text - EpisodicDialogue\\nRecord- -Chron.\\norderResp. gen.\\nPromptAgent [300] General Text Context - Prompting -Prompt\\nRefine.Content-\\nbasedPrompt\\nExec.\\nECL [301] EmbodyMulti-\\nmodalContext EpisodicObs.\\nRecordingContrast.\\nLearn.Exper.\\nSumm.Sim. &\\nRecencyPolicy\\nLearn.\\nLEO [302] EmbodyMulti-\\nmodalWorkingLong-\\nHorizon\\nRep.ObservationSpatial-\\nTemp.\\nLearn.Goal-Cond.\\nPolicyHierarch.\\nPlanLong-\\nHorizon\\nExec.\\nIER [303] EmbodyMulti-\\nmodalContext EpisodicEnv.\\nInteract.Multi-\\nmodal\\nEmbedIter. Refine. Sim. MatchAction\\nPlan.\\nV oyager [47] Embody Text Working ProceduralAuto.\\nCurriculumSkill\\nLibraryIter.\\nPrompt.- Skill Exec.\\nA3T [49]Embody,\\nRoboticsText Context -Task\\nDecomp.Token. &\\nEmbed.Action\\nPlanning-Action\\nselect.\\nSTARLING [304] RoboticsMulti-\\nmodalContext Procedural Demo.Traj.\\nEncodeSkill\\nRefine.Sim. &\\nContextSkill Exec.\\nTable 3.1: Summary of the memory module in various agents. Refer to Figure 3.6 for abbreviations.\\nExpel [ 69] constructs an experience pool to collect and extract insights from training tasks, facilitating generalization\\nto unseen tasks. More recently, MindOS [ 233] proposed a working memory-centric central processing module for\\nbuilding autonomous AI agents, where working memory consolidates task-relevant experiences into structured thoughts\\nfor guiding future decisions and actions.\\nThese two mechanisms work in concert with preliminary LLM input. To address the initial challenges, several\\nmechanisms have to be deployed. Agents must be equipped with mechanisms to assess the potential relevance of\\nincoming information rapidly. This preliminary filtering prevents cognitive overload. The acquisition phase also benefits\\nfrom LLM.\\n3.4.2 Memory Encoding\\nMemory encoding builds upon acquisition by transforming the filtered perceptual information into internal representa-\\ntions suitable for storage and later use. A key aspect of encoding is selective filtering. This selective attention mimics\\nhuman cognitive processes [ 309]. The inherent challenges of encoding stem from the complexity, high dimensionality,\\nand often noisy nature of raw perceptual data. Effective encoding requires advanced mechanisms to identify key\\n48\\nMemory\\nAcquisition\\nMemory\\nEncoding\\nMemory\\nDerivation\\nMemory\\nMatching\\nNeural Memory\\nNetwork\\nMemory\\nUtilization\\nWrite Read\\nStoreRaw Data Knowledge\\nRefinement FilteringStructured  Data\\nAttentionAgentRetention ProcessRetrieval ProcessFigure 3.7: Illustration of the memory lifecycle. The memory retention process involves three sequential steps—memory\\nacquisition, encoding, and derivation, while the memory retrieval process encompasses several independent applications,\\nincluding matching (vector search), neural memory networks, and memory utilization (for long-context modeling and\\nhallucination mitigation).\\nfeatures, compress them compactly, and integrate information from multiple modalities. Modern approaches address\\nthese challenges by leveraging selective attention andmulti-modal fusion .\\nSelective Attention mechanisms, inspired by human cognition, allow the agent to dynamically focus computational\\nresources on the most relevant parts of the input. This might involve attending to specific regions of an image, keywords\\nin a text, or particular frequencies in an audio signal. Different attention mechanisms can be used depending on the\\nmodality and task. For example, as the candidate memory dynamically expands, MS [ 237] employs an LLM-based\\nscorer to selectively retain the top-scoring half, creating a more compact shared memory across multiple agent systems.\\nIn other modalities, GraphVideoAgent [ 238] utilizes graph-based memory to enable selective and multi-turn video scene\\nunderstanding, enhancing question-answering performance. In robot control, [ 240] implements selective attention as a\\nfiltering mechanism to extract task-relevant objects from the set of all perceived objects on the table.\\nMulti-modal Fusion [ 310] is essential for integrating information from different sensory inputs (e.g., combining visual\\nand auditory data to understand a scene). This involves creating a unified representation space where features from\\ndifferent modalities are aligned. Cross-modal encoders and contrastive learning techniques are often used to achieve this\\nfusion. For example, JARVIS-1 [ 228] uses the general-domain video-language model CLIP [ 51] to compute alignment\\nwithin a multimodal key-value memory, where the key comprises elements such as task, plan, and visual observations,\\nand the value is a text-based representation of successfully executed plans. Furthermore, Optimus-1 [ 241] refines\\nmemory representation and optimizes the multimodal encoder by leveraging MineCLIP [ 311], a domain-specific video-\\nlanguage model pre-trained on Minecraft gameplay, to align and fuse filtered video streams with textual instructions\\nand plans, encoding the agent’s multimodal experiences into an abstracted memory pool. This integrated representation\\nenhances information retrieval and reasoning across modalities and acts as another filter, reinforcing consistent data.\\nLLMs’ semantic understanding is utilized to extract relevant features efficiently.\\n3.4.3 Memory Derivation\\nMemory derivation focuses on extracting meaningful knowledge and insights from the acquired and encoded memories.\\nThis process goes beyond simple storage. This stage is essential for enhancing the agent’s learning capabilities. The\\ngoal is to continuously optimize the structure and content of the agent’s memory. A significant challenge in derivation\\nis the dynamic evaluation of information value. Strategies to address these challenges include reflection ,summarization ,\\nknowledge distillation , and selective forgetting .\\nReflection involves an agent actively analyzing its memories to identify patterns, relationships, and potential incon-\\nsistencies. It can be triggered by specific events (e.g., an unexpected outcome) or occur periodically as a background\\nprocess. This process may include comparing memories, reasoning about causal relationships, and generating hypothe-\\nses [300]. ExpeL [ 69] leverages reflection to collect past experiences for generalization to unseen tasks and to support\\ntrial-and-error reattempts following failures. R2D2 [ 243] models memory as a replay buffer and applies reflection to\\nrefine it by correcting failed execution trajectories in web agents. These corrected trajectories are then combined with\\nsuccessful ones to construct reflective memory, which serves as a reference for future decision-making.\\nSummarization aims to produce concise representations of larger bodies of information while preserving their most\\nessential content. This can include extracting key sentences from a document, generating abstractive summaries of\\nconversations, or condensing sequences of events. Summarization techniques range from simple extractive methods\\nto advanced abstractive approaches powered by large language models (LLMs) [ 245,312,246]. For example, [ 248]\\nintroduces a recursive summarization strategy over dialogue history and prior memory to support long-term dialogue\\nmemory derivation. Building on this, Healthcare Copilot [ 247] maintains concise memory by transforming conversation\\n49\\nmemory, representing the full ongoing medical consultation, into history memory that retains only key information\\nrelevant to the patient’s medical history.\\nKnowledge distillation [ 313] enables agents to transfer knowledge from larger, more complex models (or ensembles)\\nto smaller, more efficient ones. This is particularly important for resource-constrained agents and for enhancing\\ngeneralization. Distillation can also involve consolidating knowledge from multiple specialized models into a single,\\ngeneral-purpose model. For example, AoTD [ 250] distills textual chains of thought from execution traces of subtasks\\ninto a Video-LLM to enhance multi-step reasoning performance in video question answering tasks. LDPD [ 251]\\ntransfers decision-making outcomes from teacher agents (i.e., expert buffers) to student agents, optimizing the student’s\\npolicy to align with the teacher’s. In multi-agent systems, MAGDi [ 253] distills the reasoning interactions among\\nmultiple LLMs into smaller models by structurally representing multi-round interactions as graphs, thereby improving\\nthe reasoning capabilities of smaller LLMs.\\nSelective forgetting [ 314] is the crucial process of removing or down-weighting memories that are deemed irrelevant,\\nredundant, or outdated. This is essential for maintaining memory efficiency and preventing cognitive overload.\\nForgetting mechanisms can be based on time (older memories are more likely to be forgotten) [ 247], usage frequency\\n(infrequently accessed memories are more likely forgotten) [ 203], and relevance to the current task or context [ 255]. In\\nmore fine-grained forgetting mechanisms, MemoryBank [ 207] applies the Ebbinghaus Forgetting Curve to quantify the\\nforgetting rate, accounting for both time decay and the spacing effect, i.e., the principle that relearning information\\nis easier than learning it for the first time. In contrast, Lyfe Agent [ 254] adopts a hierarchical summarize-and-forget\\nstrategy: it first clusters related memories, refines them into concise summaries, and then removes older memories\\nthat are highly similar to newer ones. This approach enables efficient, low-cost memory updates for real-time social\\ninteractions.\\n3.4.4 Memory Retrieval and Matching\\nMemory retrieval is a process that emulates the human ability to recall relevant knowledge and experiences to solve\\nproblems. The goal is to efficiently and accurately extract the most pertinent memory fragments from a large and diverse\\nmemory pool, encompassing sensory, short-term, and long-term memory, to inform the agent’s decisions, planning, and\\nactions. Just as humans rely on past experiences to navigate complex situations, agents require a sophisticated memory\\nretrieval mechanism to handle a wide range of tasks effectively.\\nHowever, achieving this goal presents several significant challenges. First, the agent’s memory repository is often\\nheterogeneous, comprising various forms of memory such as natural language descriptions, structured knowledge\\ngraphs, and state-action-reward sequences. These memories differ fundamentally in their data structures, representations,\\nand levels of semantic granularity, posing a challenge for unified retrieval. Second, the retrieved memory fragments\\nmust be highly relevant to the current context, including the agent’s state, task goals, and environmental observations.\\nSimple keyword matching falls short of capturing the deeper semantic relationships required for meaningful retrieval.\\nDeveloping a context-aware semantic matching mechanism that can dynamically adjust the retrieval strategy based\\non the current situation is therefore paramount. Third, the real-time nature of agent interaction with the environment\\nnecessitates efficient memory retrieval to support rapid decision-making and action [ 315]. This demand for efficiency\\nis further compounded by the limitations of the agent’s computational resources. Finally, the agent’s memory is not\\nstatic but constantly evolving as new experiences, knowledge, and skills are acquired. Ensuring memories’ timeliness,\\nreliability, and relevance while avoiding the interference of outdated or erroneous information is a continuous challenge.\\nA comprehensive approach can address these challenges, encompassing four key components. Firstly, a foundational step\\ninvolves constructing a unified memory representation and indexing scheme. This aims to bridge the representational\\ngap between different memory types by embedding them into a common vector space. Pre-trained language models like\\nBERT or Sentence-BERT [ 316] can be leveraged to transform text-based memories into semantic vectors, while graph\\nneural networks (GNNs) can learn vector representations for structured memories like knowledge graphs, capturing\\nboth node and edge relationships [ 317]. To facilitate efficient retrieval, a multi-layered hybrid indexing structure is\\nessential. This integrates techniques like inverted indexes for keyword matching, vector indexes like Faiss [ 318] or\\nAnnoy [ 319] for similarity search, and graph indexes for structural queries [ 320], thus supporting diverse query needs.\\nSecondly, perhaps most critically, the system must develop context-aware semantic similarity computation. This allows\\nthe retrieval process to understand and utilize the current context, such as the agent’s state, goals, and observations,\\nenabling a deeper semantic match beyond keyword overlap. This involves encoding the contextual information into\\nvector representations and effectively fusing them with memory vectors. The attention mechanism plays a crucial role\\nhere, dynamically calculating the relevance between context and memory vectors and assigning different weights to\\nmemory fragments based on their contextual relevance [ 261]. This emphasizes memories that are more pertinent to the\\ncurrent situation.\\n50\\nThirdly, integrating memory retrieval with the agent’s task execution necessitates a task-oriented sequence decision and\\ndynamic routing mechanism. This leverages the structural information of tasks to guide memory retrieval and utilization,\\nenabling complex task decomposition, planning, and dynamic adjustments. By constructing a task dependency\\ngraph, the agent can topologically sort subtasks to determine execution order. During execution, each subtask’s goal\\nserves as context for memory retrieval, extracting relevant knowledge and experience. Moreover, the agent must\\nadapt to environmental feedback and task progress, dynamically adjusting the execution plan. Each decision point\\ninvolves re-retrieving memories based on the current state and goal to select the optimal action and handle unexpected\\nsituations. This aspect also emphasizes how agents can leverage their skill memory to solve problems, including\\nskill distillation, combination, and innovation. Pattern recognition allows for summarising general problem-solving\\nsteps, while structured knowledge organization arranges skills into a retrievable format. Agents can further distill\\ngeneralized skills from specific ones, combine multiple skills to address complex challenges, and even innovate new\\nskill combinations. These processes depend fundamentally on an efficient memory retrieval system that can identify\\nappropriate skills or skill combinations based on task requirements.\\nFinally, a robust memory management mechanism is crucial for maintaining the memory pool’s timeliness, relevance,\\nand efficiency. This mechanism should incorporate a forgetting and updating strategy, mirroring human forgetting\\nmechanisms [ 321]. This might involve regularly purging outdated, redundant, or infrequently used memories based\\non time-based decay (weakening memory strength over time) and frequency-based decay (purging low-frequency\\nmemories). Simultaneously, when a memory fragment relevant to the current task is retrieved, its timestamp and access\\nfrequency are updated, increasing its importance and ensuring dynamic memory updates. Through these concerted\\nefforts, LLM Agents can be equipped with a powerful, flexible, and context-aware memory retrieval and matching\\nsystem, enabling them to effectively utilize their accumulated knowledge, support complex decision-making, and\\nexhibit more intelligent behavior.\\n3.4.5 Neural Memory Networks\\nNeural Memory Networks represent a fascinating frontier in AI research. They aim to integrate memory seamlessly\\ninto the fabric of neural networks. This approach departs from traditional memory architectures by encoding memories\\ndirectly within the network’s weights or activations, transforming the network into a dynamic, read-write memory\\nstorage medium. This tight integration promises significant advancements in efficiency and the utilization of stored\\ninformation. However, realizing this vision presents several formidable challenges.\\nA primary concern is balancing memory capacity with stability. Encoding a vast amount of information within the\\nfinite parameters of a neural network while maintaining long-term stability poses a major hurdle. The network must\\nbe able to store a multitude of memories without succumbing to catastrophic forgetting or confusion between similar\\nmemories. Equally crucial is the development of effective mechanisms for memory read-write operations. The network\\nneeds to reliably write new information, update existing memories, and accurately retrieve stored information on\\ndemand, all while maintaining computational efficiency. Beyond simply storing memories, the ultimate goal is to endow\\nneural networks with the ability to generalize from and reason with the information they store. This would empower\\nthem to perform higher-order cognitive functions beyond rote memorization, allowing for insightful connections and\\ninferences based on past experiences. Several approaches are being explored to address these challenges, notably\\nthrough associative memory andparameter integration .\\nOn the one hand, associative memory, inspired by the interconnectedness of neurons in the brain, offers a promising\\navenue. Models like Hopfield networks [ 262,263], leveraging energy functions, and Bidirectional Associative Memories\\n(BAMs) [ 322], supporting hetero-associative recall, provide mechanisms for encoding and retrieving patterns based\\non the weights between neurons. Besides, Neural Turing Machines (NTMs) [ 264] and Memory-Augmented Neural\\nNetwork (MANNs) [ 323,324,275,265] augment neural networks with external memory modules, employing attention\\nand summary mechanisms to interact with these memories.\\nOn the other hand, parameter integration represents another key research direction, aiming to encode memory directly\\nwithin a network’s weights. This facilitates the seamless integration of world knowledge and accumulated experience\\ninto the operational behavior of intelligent AI agents. For example, some prior works modify model parameters to\\nenable continual learning by updating [ 325,326,327] or forgetting specific knowledge [ 328]. Other studies treat\\nLLMs as standalone memory modules, incorporating world knowledge into their parameters during pre-training [ 329],\\npost-training [ 330], and online deployment [ 331]. For instance, MemoryLLM [ 265] introduces memory tokens, while\\nSELF-PARAM [ 266] leverages knowledge distillation to embed world knowledge and past AI agent experiences into\\nmodel parameters. This approach is further augmented in the M+ model [ 332] with a long-term memory mechanism\\nand a co-trained retriever, enhancing its ability to generalize to longer history memorization. Additionally, [ 333]\\nemploys encoded memory to facilitate further reasoning, thereby improving the generalization of stored knowledge.\\nMore recently, MemoRAG [ 267] and R3Mem [ 270] have been proposed to not only encode memory but also enable\\n51\\nreliable retrieval from neural memory networks, unifying the dual processes of memory storage and retrieval within a\\nsingle model. This advancement contributes to the development of next-generation generative-based retrieval systems,\\nwhich support lifelong AI applications. Furthermore, Titans [ 269] have been introduced to memorize test-time data\\npoints through meta-learning, enabling more efficient test-time cross-task generalization.\\nFuture research will continue to focus on creating larger capacity and more stable neural memory models. Concurrently,\\ndeveloping more efficient and flexible memory read-write mechanisms will be crucial. A critical area of investigation\\nwill involve applying these memory-augmented networks to complex cognitive tasks, pushing the boundaries of what\\nAI can achieve. Progress in this domain will unlock new possibilities for building intelligent agents that can learn,\\nremember, and reason in a manner that is increasingly reminiscent of human cognition.\\n3.4.6 Memory Utilization\\nA critical aspect of agent design lies in memory utilization, which focuses on maximizing the value of stored memory\\nsegments for the current task. The core objective is to apply these memories effectively and appropriately to enhance\\nreasoning, decision-making, planning, and action generation, ultimately boosting the agent’s performance and efficiency\\nwhile avoiding the pitfalls of irrelevant or incorrect memory interference. Achieving this, however, presents several\\nchallenges.\\nOne primary challenge is balancing the vastness of the memory store with its effective utilization. Agents must navigate\\na potential information overload, ensuring that relevant memories are fully leveraged without overwhelming the system.\\nAnother hurdle is the need for abstraction and generalization. Agents need to distill specific memory segments into more\\ngeneral knowledge and apply this knowledge to new and varied situations. Furthermore, the issue of hallucinations and\\nincorrect memories within the LLM requires careful consideration. Preventing the generation of content that contradicts\\nor misrepresents stored information is crucial, as is the ability to identify and rectify erroneous information that may\\nreside within the memory store itself.\\nTo address these challenges, several strategies are employed. Retrieval-augmented generation (RAG) [334] combines\\nretrieval and generation models to enhance the LLM’s capabilities by drawing upon external knowledge sources.\\nUnlike the methods mentioned in memory retrieval and matching, RAG focuses on integrating retrieved information\\ninto the generation process itself. When prompted, the agent retrieves relevant memory segments and incorporates\\nthem into the context provided by the generation model. This contextual enrichment guides the model towards more\\nfactual and informative outputs. For instance, when responding to a user’s query, the agent can first retrieve related\\nentries from its knowledge base and then generate an answer based on this retrieved information, thus grounding\\nthe response in established knowledge. More recently, some studies have integrated memory modules with RAG,\\nincorporating self-reflection [ 274] and adaptive retrieval mechanisms [ 272] to enhance both generation reliability and\\nefficiency. For example, Atlas [ 273] leverages causal mediation analysis, while [ 284] employs consistency-based\\nhallucination detection to determine whether the model already possesses the necessary knowledge—allowing for\\ndirect generation—or whether retrieval is required, in which case the model first retrieves relevant information before\\ngenerating a response. In a unified framework, RAGLAB [ 271] offers a comprehensive ecosystem for evaluating and\\nanalyzing mainstream RAG algorithms. HippoRAG [ 222] employs a strategy inspired by the hippocampal indexing\\ntheory of human memory to create a KG-based index for memory and use Personalized PageRank for memory retrieval.\\nFurthermore, long-context modeling plays a vital role in managing extensive memory stores. This approach enhances the\\nLLM’s ability to process long sequences and large-scale memories, allowing for a deeper understanding and utilization of\\nlong-range dependencies. By employing Transformer model variants like Transformer-XL [ 324] and Longformer [ 335],\\nor through hierarchical and recursive processing techniques, such as recurrent memory transformer (RMT) [ 275,276],\\nagents can expand their context window. This enables them to handle significantly more extensive memory stores and\\nreason and make decisions within a much broader context. For example, agents can maintain a longer memory span when\\nprocessing extensive documents or engaging in prolonged conversations. Additionally, some studies leverage memory\\nto compress long contexts, enabling more effective long-context modeling. For example, AutoCompressor [ 277]\\nintroduces summary vectors as memory to transfer information from previous context windows into the current window,\\nfacilitating long-context understanding. Similarly, the in-context autoencoder (ICAE) [ 278] generates memory slots\\nthat accurately and comprehensively represent the original context, while LLMLingua [ 336,337], Gist [ 279], and\\nCompAct [280] further optimize long-prompt compression to reduce input context length.\\nFinally, hallucination mitigation strategies are essential for ensuring the reliability of generated outputs. These\\nstrategies aim to minimize the LLM’s tendency to produce factually incorrect or nonsensical content. One approach is\\nimplementing fact-checking mechanisms [ 338], verifying generated content against established knowledge or memory\\nstores. Another involves uncertainty estimation [ 339,340], where the model evaluates the confidence level of its\\ngenerated content and flags or filters out low-confidence outputs. Additionally, knowledge-based decoding strategies can\\n52\\nbe employed during the generation phase, introducing constraints that guide the model towards more factually accurate\\ncontent. These techniques collectively contribute to generating more trustworthy outputs and aligned with the agent’s\\nestablished knowledge base. Recent research has introduced expert memory subnetworks, such as PEER [ 283] and\\nLamini Memory Tuning [ 281], which specialize in memorizing specific types of information, including world knowledge\\nand AI agents’ past experiences. These subnetworks offload memorization to dedicated parameters, reducing the main\\nmodel’s propensity to hallucinate. By implementing these memory utilization strategies, agents can become more\\ncapable, accurate, and reliable. They can successfully leverage their memory stores to achieve superior performance\\nacross complex tasks.\\n3.5 Summary and Discussion\\nThe development of truly intelligent agents depends not just on robust memory systems, but also on their seamless\\nintegration with other cognitive functions like perception, planning, reasoning, and action selection. Memory is not an\\nisolated module; it is deeply intertwined with these other processes. For example, sensory input is encoded and filtered\\nbefore storage (as discussed in the sections on memory representation and lifecycle), highlighting the interplay between\\nperception and memory. Long-term memory, especially procedural memory, directly informs action selection through\\nlearned skills and routines. Retrieval mechanisms, like context-aware semantic similarity computation, are crucial for\\nplanning, allowing agents to access relevant past experiences. This interplay extends to the concept of a “world model.”\\nCentral to intelligent agents is their ability to build and utilize internal world models. These models, representing an\\nagent’s understanding of its environment, enable simulation, reasoning about consequences, and prediction. Robust\\nworld models are crucial for higher-level cognition, planning, and human-like intelligence. A world model is, in essence,\\na highly structured, often predictive, form of long-term memory. Memory provides the raw material—knowledge\\nand experiences—for constructing the world model, while the world model, in turn, acts as an organizing framework,\\ninfluencing how new memories are encoded, consolidated, and retrieved. For instance, a well-developed world model\\nmight prioritize storing surprising events, as these indicate gaps in the agent’s understanding.\\nHowever, developing effective world models and memory systems presents significant challenges. These include\\nmanaging the complexity of real-world environments, determining the appropriate level of abstraction (balancing\\naccuracy, complexity, and computational efficiency), and integrating multi-modal information. Learning and updating\\nthese models efficiently, avoiding bias, ensuring generalization, and enabling continuous adaptation are also critical.\\nFurthermore, model-based planning requires efficient search algorithms to handle the inherent uncertainty in the model’s\\npredictions.\\nFuture research should focus on enhancing agent memory systems by drawing inspiration from the strengths of human\\nmemory, particularly its flexibility, adaptability, and efficiency. While agent memory has advanced considerably, it still\\nlags behind human memory in these key areas. Human memory is remarkably associative, retrieving information from\\nincomplete or noisy cues, and it exhibits a sophisticated form of “forgetting” that involves consolidation and abstraction,\\nprioritizing relevant information and generalizing from experiences. Agent memory, conversely, often relies on precise\\nmatching and struggles with ambiguity.\\nSeveral promising research directions emerge. Exploring biologically-inspired mechanisms, such as neural memory\\nnetworks (as discussed earlier), could lead to significant breakthroughs. Another crucial area is developing memory\\nsystems that actively “curate” their contents—reflecting on information, identifying inconsistencies, and synthesizing\\nnew knowledge. This requires integrating metacognitive capabilities (monitoring and controlling one’s own cognitive\\nprocesses) into agent architectures. Furthermore, creating more robust and nuanced forms of episodic memory, capturing\\nnot just the “what” and “when” but also the “why” and the emotional context of events, is essential for agents that can\\ntruly learn from experience and interact with humans naturally.\\nOvercoming these challenges requires innovative solutions at the intersection of deep learning, reinforcement learning,\\nand cognitive science. Developing more sophisticated and adaptable world models and memory systems—ones that\\nmirror the strengths of human cognition—will pave the way for agents with a deeper understanding of their environment,\\nleading to more intelligent and meaningful interactions.\\n53\\nChapter 4\\nWorld Model\\nA world model enables an agent to predict and reason about future states without direct trial-and-error in reality. This\\nsection explores how human cognitive studies on “mental models” relate to AI world models in artificial intelligence,\\ncategorizing them under four paradigms: implicit paradigm ,explicit paradigm ,simulator-based paradigm , and a class\\nof other emergent methods (e.g., instruction-driven paradigm ). We then discuss how world models inherently intersect\\nwith other agentic components and conclude with open questions and future directions that unite these perspectives\\nunder a unified theoretical and practical framework.\\nUsing the brain\\'s world model to predict the trajectory of the ball\\nFigure 4.1: Humans can use their brain’s model of the world to predict the consequences of their actions. For example,\\nwhen playing table tennis, a player can imagine or predict the trajectory of the ball after an action.\\n54\\n4.1 The Human World Model\\nHumans naturally construct internal representations of the world, often referred to as mental models in psychology\\n[341,342,343]. These models serve as compact and manipulable depictions of external reality, enabling individuals to\\npredict outcomes, plan actions, and interpret novel scenarios with minimal reliance on direct trial-and-error. Early work\\non spatial navigation, for instance, showed that humans and animals form “cognitive maps” of their surroundings [ 341],\\nsuggesting an underlying ability to imagine potential paths before actually traversing them.\\nCraik’s seminal argument was that the human mind runs internal “small-scale models of reality” [ 342] to simulate\\nhow events might unfold and evaluate possible courses of action. Later studies proposed that such simulations stretch\\nacross modalities—vision, language, and motor control—and are dynamically updated by comparing predictions to new\\nobservations. This process merges memory recall with forward projection , implying a close interplay between stored\\nknowledge and the active generation of hypothetical future states [ 343]. More recent predictive processing theories\\nsuch as “Surfing Uncertainty” [ 344] propose that the brain operates as a hierarchical prediction machine, continuously\\ngenerating top-down predictions about sensory inputs and updating its models based on prediction errors.\\nCritically, these human mental models are:\\n•Predictive: They forecast changes in the environment, informing decisions about where to move or how to\\nrespond.\\n•Integrative: They combine sensory input, past experience, and abstract reasoning into a unified perspective\\non “what might happen next”.\\n•Adaptive: They are revised when reality diverges from expectation, reducing the gap between imagined and\\nactual outcomes over time.\\n•Multi-scale: They operate seamlessly across different temporal and spatial scales, simultaneously processing\\nimmediate physical dynamics (milliseconds), medium-term action sequences (seconds to minutes), and long-\\nterm plans (hours to years). This flexibility allows humans to zoom in on fine-grained details or zoom out to\\nconsider broader contexts as needed.\\nConsider hunger and eating as an illustration of integrated world modeling. When hungry, a person’s internal\\nmodel activates predictions about food—simulating not just visual appearance but tastes, smells, and anticipated\\nsatisfaction—triggering physiological responses like salivation before food is even present. This demonstrates seamless\\nintegration across perception, memory, and action planning.\\nThe example also highlights adaptivity: once satiated, the same model dynamically updates, reducing predicted reward\\nvalues for further eating. Despite recognizing the same food items, their anticipated utility changes based on internal\\nstate. Furthermore, humans maintain counterfactual simulations—declining dessert now while accurately predicting\\nthey would enjoy it later—enabling complex planning across hypothetical scenarios and time horizons, a capability\\ncomprehensive AI world models strive to replicate.\\nIn sum, the human world model is not a static library of facts, but a flexible and ever-evolving mental construct, deeply\\nrooted in perception and memory, that continuously shapes (and is shaped by) the individual’s interactions with the\\noutside world.\\n4.2 Translating Human World Models to AI\\nResearch in artificial intelligence has long sought to replicate the predictive, integrative, and adaptive qualities exhibited\\nby human mental models [ 341,342]. Early reinforcement learning frameworks, for instance, proposed learning an\\nenvironment model for planning—exemplified by Dyna [ 345]—while contemporaneous work investigated using neural\\nnetworks to anticipate future observations in streaming data [ 346,347]. Both directions were motivated by the idea\\nthat an internal simulator of the world could enable more efficient decision-making than purely reactive, trial-and-error\\nlearning.\\nSubsequent advancements in deep learning brought the notion of “AI world models” into sharper focus. One influential\\napproach introduced an end-to-end latent generative model of an environment (e.g., “World Models” [348]), whereby\\na recurrent neural network (RNN) and variational auto-encoder (V AE) together learn to “dream” future trajectories.\\nThese latent rollouts allow an agent to train or refine policies offline, effectively mirroring how humans mentally\\nrehearse actions before executing them. Alongside such implicit designs, explicit forward-modeling methods emerged\\nin model-based RL, letting agents predict P(s′|s, a)and plan with approximate lookahead [349, 350].\\n55\\nAnother branch of work leveraged large-scale simulators or real-world robotics to ground learning in richly diverse expe-\\nriences [ 351,352]. Such setups are reminiscent of how human children learn by actively exploring their environments,\\ngradually honing their internal representations. Yet a key question lingers: can agentic systems unify these approaches\\n(implicit generative modeling, explicit factorization, and simulator-driven exploration) into a cohesive “mental model”\\nakin to that observed in humans? The recent proliferation of language-model-based reasoning [ 107,74] hints at the\\npotential to cross modalities and tasks, echoing how humans integrate linguistic, visual, and motor knowledge under\\none predictive framework.\\nOverall, as AI systems strive for flexible, sample-efficient learning, the AI world model stands as a conceptual bridge\\nfrom cognitive theories of mental models to implementations that equip artificial agents with imagination ,predictive\\nreasoning , and robust adaptation in complex domains.\\n4.3 Paradigms of AI World Models\\nDesigning an AI world model involves determining how an AI agent acquires, represents, and updates its understanding\\nof the environment’s dynamics. While implementations vary, most approaches fall into four broad paradigms: implicit ,\\nexplicit ,simulator-based , and hybrid or instruction-driven models. These paradigms can be further analyzed along two\\nkey dimensions: reliance on internal (neural-based) vs. external (rule-based or structured) mechanisms, and overall\\nsystem complexity . Figure 4.2 illustrates this two-dimensional space, showing how different approaches distribute\\nthemselves across these axes. Generally, implicit models tend to rely more on internal mechanisms, while explicit and\\nsimulator-based models incorporate more external structures. Simulator-based and explicit models also tend to be more\\ncomplex than implicit and hybrid approaches, reflecting their structured reasoning and engineered constraints.\\nComplexityForm\\nSimple ComplexInternal External\\nActRe [49]World Models [348]Dreamer [350]\\nDiffusion WM [353]GQN [354]Daydreamer [352]PILCO [355]AutoManual [108]COAT [356]SAPIEN [351]\\nMuZero [349]\\nGR-2 [357]DINO-WM [358]\\nFigure 4.2: A two-dimensional layout of AI world-model methods. The horizontal axis indicates Complexity (left to\\nright). The vertical axis spans Internal approaches (bottom) to External solutions (top). Approximate positions reflect\\neach method’s reliance on large learned networks vs. explicit rules or code, and its overall system complexity.\\n4.3.1 Overview of World Model Paradigms\\nAnAI world model is broadly any mechanism by which an agent captures or accesses approximate environment\\ndynamics. Let Sdenote the set of possible environment states ,Athe set of actions , andOthe set of observations . In\\nan idealized Markovian framework, the environment is characterized by transition and observation distributions:\\nT(s′|s, a) : S × A → ∆(S), (4.1)\\nO(o|s′) : S → ∆(O), (4.2)\\n56\\nwhere T(·)dictates how states evolve under actions, and O(·)defines how states produce observations. A world model\\ntypically learns orutilizes approximations of these functions (or a variant), allowing the agent to predict future states or\\nobservations without executing real actions in the environment.\\nNumerous approaches exist to implement these approximations, which we group into four main paradigms :\\n•Implicit paradigm: A single neural network or latent structure encodes both transition and observation\\nmappings without explicit factorization. World Models [ 348] or large language models used for environment\\nreasoning are typical examples. Agents generally unroll this black-box function to simulate hypothetical\\ntrajectories.\\n•Explicit paradigm: The agent directly models or has access to learnable transition model Tθand observation\\nmodel Oθ, often enabling interpretability or modular design. Model-based RL methods–like MuZero [ 349] or\\nDreamer [ 350]–learn or refine Tθ, planning in an approximated state space. Generative visual models such as\\n[353, 358] fall under this category if they explicitly predict the next states or frames.\\n•Simulator-Based paradigm: Rather than approximating (4.1) –(4.2) , the agent relies on an external simulator\\nor even the physical world as the ground-truth. Systems like SAPIEN [ 351] or real-robot pipelines [ 352] can\\nbe seen as “native” environment models that the agent queries. Although no learned T(·)is required, the agent\\npays a cost in terms of runtime or real-world risks.\\n•Other paradigms (Hybrid or Instruction-Driven): Methods that defy simple classification. They may\\nstore emergent rules in textual form [ 108], refine implicit LLM knowledge into partial causal graphs [ 356],\\nor combine external components with learned sub-modules. Such approaches highlight the evolving nature\\nof world-model research, where instructions, symbolic rules, or on-the-fly structures can complement more\\ntraditional approximations.\\nThroughout the remainder of this subsection, we examine how each paradigm addresses (or circumvents) Equations (4.1)\\nand(4.2) , the trade-offs in interpretability and scalability, and their relative merits for different tasks ranging from\\ntext-based to high-dimensional embodied control.\\n4.3.2 Implicit Paradigm\\nIn the implicit paradigm, an agent encodes all environment dynamics—including how states evolve and how observations\\nare generated—within a single (or tightly coupled) neural model. Formally, one maintains a latent state htthat is\\nupdated according to\\nht+1=fθ(ht, at),ˆot+1=gθ\\x00\\nht+1\\x01\\n, (4.3)\\nwhere fθsubsumes the transition function T(·)(and part of O(·)) from Eqs. (4.1) –(4.2) , but without making these\\ncomponents explicit. A classic example is the World Models framework [ 348], in which a Variational Autoencoder\\n(V AE) first compresses visual inputs into latent codes, and a recurrent network predicts the next latent code, effectively\\n“dreaming” trajectories in latent space. Recent work also explores repurposing large language models (LLMs) for\\nenvironment simulation in purely textual or symbolic domains [ 107,74], although these models are not always grounded\\nin strict time-series or physics-based data.\\nBecause implicit models fuse the transition and observation mechanisms into one monolithic function, they can be\\nelegantly trained end to end and unrolled internally for planning. However, they tend to be opaque: it is difficult\\nto interpret how precisely the network captures domain constraints or to inject knowledge directly into any part of\\nthe transition. This can be advantageous for highly complex environments where a single large-capacity model can\\ndiscover latent structure on its own, but it also risks brittleness under distribution shifts. Overall, the implicit paradigm\\nis appealing for its simplicity and flexibility, but it can pose challenges when interpretability, explicit constraints, or\\nfine-grained control of the dynamics are required.\\n4.3.3 Explicit Paradigm\\nTheexplicit paradigm instead factorizes the world model, often by learning or encoding a transition function ˆTθ(st+1|\\nst, at)and an observation function ˆOθ(ot+1|st+1). This explicit separation makes it possible to query each function\\nindependently. For instance, one might draw samples from\\nˆst+1∼ˆTθ(st, at),ˆot+1∼ˆOθ\\x00\\nˆst+1\\x01\\n. (4.4)\\nModel-based reinforcement-learning algorithms like MuZero [ 349] or Dreamer [ 350] exemplify this paradigm by\\nrefining a forward model for planning. Other explicit approaches prioritize fidelity in generating future frames, such as\\n57\\nDiffusion WM [ 353], which applies diffusion processes at the pixel level, or DINO-WM [ 358], which rolls out future\\nstates within a pretrained feature space.\\nBy factorizing transitions and observations, explicit methods can be more interpretable and more amenable to debugging\\nand domain-specific constraints. That said, they are still sensitive to model errors: if ˆTθdeviates significantly from\\nreality, the agent’s planning and decision-making can become ineffective. Many explicit systems still rely predominantly\\non internal (neural) representations, but they may integrate external planners (e.g., tree-search algorithms) to leverage\\nthe explicit transition structure. This blend of learned and symbolic components offers a natural way to incorporate\\nhuman knowledge, while preserving the strengths of deep learning.\\n4.3.4 Simulator-Based Paradigm\\nIn the simulator-based paradigm, the agent outsources environment updates to a simulator, effectively bypassing the\\nneed to learn ˆTθfrom data. Formally,\\n(st+1, ot+1)← SIM (st, at), (4.5)\\nwhere SIM is often an external physics engine or the real world itself. Platforms like SAPIEN [ 351] and AI Habitat\\nprovide deterministic 3D physics simulations, allowing agents to practice or iterate strategies in a controlled environment.\\nAlternatively, methods such as Daydreamer [ 352] treat real-world interaction loops like a “simulator,” continually\\nupdating on-policy data from physical robots.\\nThis approach yields accurate transitions (assuming the simulator accurately reflects reality), which alleviates the risk\\nof learned-model errors. However, it can be computationally or financially expensive, especially if the simulator is high\\nfidelity or if real-world trials are time-consuming and risky. As a result, some agents combine partial learned dynamics\\nwith occasional simulator queries, aiming to balance accurate rollouts with efficient coverage of state-action space.\\n4.3.5 Hybrid and Instruction-Driven Paradigms\\nBeyond these three primary paradigms, there is a growing number of hybrid orinstruction-driven approaches, which\\nblend implicit and explicit modeling or incorporate external symbolic knowledge and large language models. Often,\\nthese systems dynamically extract rules from data, maintain evolving textual knowledge bases, or prompt LLMs to\\nhypothesize causal relationships that can then be tested or refined.\\nAutoManual [ 108], for example, iteratively compiles interactive environment rules into human-readable manuals,\\ninforming future actions in a more transparent way. Meanwhile, COAT [ 356] prompts an LLM to propose possible\\ncausal factors behind observed events, then validates or refines those factors via direct interaction, bridging text-based\\nreasoning with partial learned models. Although these solutions offer remarkable flexibility—particularly in adapting\\nto unfamiliar domains or integrating real-time human insights—they can be inconsistent in how they structure or\\nupdate internal representations. As language-model prompting and real-time rule discovery continue to evolve, these\\nhybrid methods are poised to become increasingly common, reflecting the need to balance end-to-end learning with the\\ntransparency and adaptability offered by external instruction.\\nUntil now, we have introduced the four typical paradigms of existing world model techniques, as illustrated in\\nFigure 4.3.5. As we can see, each type of technique has trade-offs in different aspects.\\n4.3.6 Comparative Summary of Paradigms\\nThe table summarizes the key methods in AI world modeling, categorizing them based on their reliance on external\\norinternal mechanisms, their complexity, and their respective paradigms. The form column uses ◦for external\\napproaches and •for internal ones, with mixed methods having both symbols. This classification aligns with the\\nprevious subsections, including the detailed discussion of each paradigm, and complements the visual representation in\\nFigure 4.2.\\n4.4 Relationships to Other Modules\\nA comprehensive AI world model does not exist in isolation but interacts with several key components of the agent’s\\narchitecture. These include (but not limited to) the memory, perception, and action modules. In this subsection, we\\nexplore how world models integrate with these critical components to enable coherent and adaptive behavior in dynamic\\nenvironments.\\n58\\n(a) Implicit\\nht\\nImplicit\\nModelht+1atˆot+1(b) Explicit\\nstˆTθ\\nˆst+1ˆOθˆot+1\\nat\\n(c) Simulator-Based\\nstat\\nSimulator\\nst+1ot+1(d) Hybrid / Instruction-Driven\\nImplicit or\\nPartial\\nModelLLM /\\nRules KBprompts / updates\\nRefined Prediction\\n˜st,˜ottrain / refine rules / constraints\\nFigure 4.3: Four paradigms of world modeling: (a) implicit, (b) explicit, (c) simulator-based, and (d) hybrid/instruction-\\ndriven.\\nTable 4.1: Summary of AI world-model methods across paradigms, showing their form (External or Internal), complexity,\\nand paradigm.\\nMethod Form Complexity Paradigm\\nActRe [49] • Simple Implicit\\nWorld Models [348] • Simple Implicit\\nDreamer [350] • Moderate Implicit\\nDiffusion WM [353] • High Explicit\\nGQN [354] • High Explicit\\nDaydreamer [352] ◦ High Simulator-based\\nSAPIEN [351] ◦ High Simulator-based\\nPILCO [355] ◦ Moderate Explicit\\nAutoManual [108] ◦ Simple Other\\nMuZero [349] ◦ High Explicit\\nGR-2 [357] • High Explicit\\nDINO-WM [358] • High Explicit\\nCOAT [356] ◦ Moderate Other\\n4.4.1 Memory and the World Model\\nMemory systems play a crucial role in the operation of world models. While a world model generates predictive\\nrepresentations of future states or actions, memory serves as the foundation upon which these representations are built\\nand updated. The relationship between the world model and memory can be viewed as a loop where the world model\\npredicts potential futures, while the memory stores past experiences, observations, and learned patterns, allowing for\\ncontext-dependent reasoning and future predictions.\\nMemory mechanisms can be structured in various ways, including:\\n•Short-term memory : This enables the agent to hold and update its internal state temporarily, storing the most\\nrecent interactions or observations. This short-term context helps the agent make decisions in the immediate\\nenvironment.\\n•Long-term memory : This serves as a more persistent repository of experiences and general knowledge about\\nthe environment. A world model can interact with long-term memory to refine its predictions, and it may use\\nhistorical data to make more informed decisions or simulate more realistic futures.\\n59\\nFor example, in model-based RL frameworks like Dreamer [ 350], recurrent neural networks act as both the world model\\nand a form of memory, maintaining a latent state that is updated with each time step to predict future states. This form\\nof integrated memory allows the agent to both recall past interactions and anticipate future ones.\\n4.4.2 Perception and the World Model\\nPerception refers to the agent’s ability to sense and interpret its environment through various modalities (e.g., vision,\\ntouch, sound, etc.). The world model relies heavily on accurate sensory input to form coherent predictions about the\\nenvironment. In many AI systems, the perception module converts raw sensor data into a higher-level representation,\\nsuch as an image, sound wave, or other structured data.\\nA key aspect of the interaction between the world model and perception is how the agent processes and integrates\\nsensory input into the model. The world model often depends on processed data (such as features from convolutional\\nneural networks or embeddings from transformers) to simulate potential futures. Additionally, the world model can\\nguide perceptual processes by focusing attention on the most relevant sensory input needed to refine predictions.\\nFor example, in autonomous robotics, perception systems typically detect objects or environmental features, which\\nare then fed into a world model that predicts how the scene will evolve. RoboCraft [ 359] achieves this perception-to-\\nmodeling transformation by converting visual observations into particles and capturing the underlying system structure\\nthrough graph neural networks. PointNet [ 360] further enriches perception systems’ understanding of physical space\\nby encoding unstructured 3D point clouds to capture spatial characteristics of the environment. In navigation tasks,\\nOVER-NA V [ 361] further combine large language models and open-vocabulary detection to construct the relationship\\nbetween multi-modal signals and key information, proposing an omni-graph to capture the structure of local space as\\nthe world model for navigation tasks. This feedback loop between perception and the world model enables agents to\\nupdate their perception dynamically based on ongoing predictions, allowing for real-time adaptation.\\n4.4.3 Action and the World Model\\nAction refers to the decision-making process through which an agent interacts with its environment. In agentic systems,\\nactions are driven by the world model’s predictions of future states. The world model aids in planning by simulating the\\noutcomes of different actions before they are executed, allowing the agent to choose the most optimal course of action\\nbased on the predicted consequences.\\nThe integration between world models and action modules can take various forms:\\n•Model-based planning : World models explicitly model the environment’s transition dynamics [ 349,362,107],\\nallowing the agent to simulate multiple action sequences (rollouts) before selecting the most optimal one.\\n•Exploration : World models also support exploration strategies by simulating unseen states or unexpected\\nactions [ 363,350,364]. These simulations enable the agent to evaluate the potential benefits of exploring new\\nparts of the state space.\\nIn model-based planning, MuZero [ 349] performs implicit planning through self-play and Monte Carlo Tree Search\\n(MCTS), transforming current state representations into future state and reward predictions to guide the decision-making\\nprocess without prior knowledge of environment rules. In contrast, MPC [ 362] utilizes explicit dynamics models to\\npredict multiple possible trajectories within a finite time horizon, determines the optimal control sequence by solving\\nan optimization problem, and continuously updates planning using a receding horizon approach. Alpha-SQL [ 365],\\non the other hand, integrates an LLM-as-Action-Model within an MCTS framework to explore potential SQL queries\\nwithin the database’s “world model”. This approach dynamically generates promising SQL construction actions based\\non partial query states, enabling zero-shot Text-to-SQL interactions without task-specific fine-tuning. Unlike MuZero,\\nwhich focuses on planning for decision-making in uncertain environments, Alpha-SQL applies MCTS in a specific\\ntask—guiding SQL query construction through self-generated actions within a complex database context.\\nFor exploration strategies, Nagabandi et al. [363] incentivizes agents to explore unknown regions by providing reward\\nmechanisms (exploration bonuses) for discovering new states. Dreamer [ 350] propose that world models can generate\\nimaginary action sequences (imaginary rollouts), allowing agents to safely evaluate the benefits of new actions in\\nsimulated environments without risking real-world experimentation. Similarly, in the discrete world model Hafner\\net al. [364] , agents efficiently explore complex environments by simulating multiple possible future states, effectively\\nbalancing the trade-off between exploration and exploitation.\\nFor example, in reinforcement learning, agents can employ a learned world model to simulate future trajectories in\\naction-selection tasks. The world model evaluates the potential rewards of different actions, enabling the agent to plan\\neffectively and take actions that maximize long-term goals.\\n60\\n4.4.4 Cross-Module Integration\\nWhile memory, perception, and action are discussed as separate modules, the true strength of world models lies in their\\nability to seamlessly integrate across these domains. A world model continuously receives sensory input, updates its\\ninternal memory, simulates future states, and uses this information to drive action selection. The iterative feedback loop\\nbetween these modules allows agents to engage in intelligent, goal-directed behavior that is highly adaptive to changes\\nin the environment.\\nThis cross-module interaction is particularly relevant in complex, dynamic systems such as robotics, where an agent\\nmust continuously adapt its internal representation of the world, process sensory input, store relevant experiences, and\\ntake actions in real time. In the context of embodied agents, the integration of these modules ensures that predictions\\nmade by the world model are grounded in current observations and the agent’s ongoing experiences.\\nWorld models provide a fundamental unifying principle across modalities. Whether predicting physical outcomes\\nin embodied robotics, anticipating visual changes on screens, or inferring semantic relationships in text, the core\\nmechanism remains consistent: generating predictions about how states evolve under different actions. This cross-\\nmodal capacity explains why humans transition effortlessly between manipulating objects, navigating interfaces, and\\nprocessing language—all activities driven by the same underlying predictive architecture. Future AI systems may\\nachieve similar integration by developing world models that bridge these traditionally separate domains through a\\ncommon predictive framework.\\nIn summary, the relationship between the world model and the other modules—memory, perception, and action—forms\\nthe backbone of intelligent behavior in AI systems. Each module contributes to a cycle of prediction, update, and action,\\nallowing agents to function effectively in dynamic and uncertain environments. These interactions highlight the need\\nfor a holistic approach when designing agent architectures, where world models are closely intertwined with sensory\\ninput, memory systems, and decision-making processes.\\n4.5 Summary and Discussion\\nThe evolution of AI world models, from early cognitive insights to advanced AI architectures, underscores the growing\\nrealization that true intelligence relies on the ability to predict, simulate, and imagine. Unlike classical reinforcement\\nlearning, where agents operate solely through trial-and-error interactions, world models enable foresight—agents\\ncan plan, anticipate, and adapt to changes before they happen. This leap in cognitive modeling—whether implicit,\\nexplicit, or simulator-based—marks a significant shift in how machines can be endowed with flexibility, robustness, and\\ngeneralization across tasks.\\nAn essential yet often overlooked aspect of world models is their operation across multiple temporal and spatial scales.\\nHuman mental models seamlessly integrate predictions spanning milliseconds (reflexive responses), seconds (immediate\\naction planning), minutes to hours (task completion), and even years (life planning) [ 366]. This multi-scale capability\\nallows us to simultaneously predict immediate physical dynamics while maintaining coherent long-term narratives\\nand goals. Similarly, humans process spatial information across scales—from fine-grained object manipulation to\\nnavigation across environments to abstract geographical reasoning. Current AI world models typically excel within\\nnarrow temporal and spatial bands, whereas human cognition demonstrates remarkable flexibility in scaling predictions\\nup and down as context demands. This suggests that truly general-purpose AI world models may require explicit\\nmechanisms for integrating predictions across multiple time horizons and spatial resolutions, dynamically adjusting the\\ngranularity of simulation based on task requirements.\\nOne central challenge in designing world models is the interplay between complexity andpredictive accuracy . As\\ndiscussed, implicit models, such as those based on recurrent neural networks or transformers, offer simplicity and\\nelegance, but they often come with the trade-off of limited interpretability. The model’s internal state is an opaque\\nlatent space, making it difficult to enforce domain constraints or provide guarantees about the accuracy of predictions.\\nWhile such systems excel at capturing highly complex relationships and data-driven patterns, they also risk overfitting\\nor failing to generalize to unseen scenarios.\\nExplicit models, by contrast, offer greater transparency and control. By factorizing state transitions and observations\\ninto separate functions, we gain a clearer understanding of how predictions are formed, and we can more easily integrate\\nstructured knowledge, such as physical laws or domain-specific rules. However, this approach comes with its own\\nset of challenges. First, it often requires large amounts of labeled training data or simulated experiences to accurately\\ncapture environment dynamics. Second, even the most well-structured explicit models may struggle with complex\\nenvironments that require fine-grained, high-dimensional state representations, such as in video prediction or robotics.\\n61\\nThesimulator-based approach offers a promising alternative, wherein agents rely on external environments—either\\nphysically grounded or simulated—for dynamic updates. This method avoids many of the challenges inherent in\\nlearning accurate world models from scratch, as the simulator itself serves as the “oracle” of state transitions and\\nobservations. However, the reliance on simulators also introduces limitations: simulators often fail to capture the full\\nrichness of real-world dynamics and can be computationally expensive to maintain or scale. Furthermore, real-world\\nenvironments introduce noise and variability that a purely learned or pre-configured model might miss. As AI agents\\nstrive to perform tasks in open-ended, unpredictable settings, the robustness of their world models will be tested by the\\ngap between simulated and actual environments.\\nA key theme that emerges from this discussion is the trade-off between generalization and specialization . The more\\nspecific a world model is to a particular domain or task, the less likely it is to generalize across different contexts. Models\\nlike MuZero [ 349] and Dreamer [ 350] exemplify this: they excel at specific environments (e.g., Atari games or robotics)\\nbut require careful adaptation when transferred to new, uncharted domains. Conversely, implicit models—particularly\\nthose leveraging large-scale neural networks—have the potential to generalize across tasks but often do so at the cost of\\nsacrificing domain-specific expertise.\\nMoreover, integrating memory with world models is crucial for agents that need to handle long-term dependencies\\nand past experiences. While world models excel at predicting the next state based on immediate inputs, true intelligent\\nbehavior often requires reasoning about distant outcomes. Long-term memory allows agents to store critical environ-\\nmental knowledge, ensuring that short-term predictions are grounded in a broader understanding of the world. This\\nfusion of memory, perception, and action, mediated by the world model, creates a feedback loop where predictions\\nshape actions, which in turn inform future predictions.\\nThehuman analogy remains compelling: just as humans integrate sensory inputs, memories, and internal models\\nto navigate the world, so too must intelligent agents combine perception, memory, and action through their world\\nmodels. As the field advances, it is clear that a holistic approach—one that unifies implicit, explicit, and simulator-based\\nmethods—may be the key to achieving more robust, generalizable, and adaptive agents. Hybrid methods, like those\\nused in AutoManual [ 108] or discovery-based models [ 356], offer exciting possibilities for blending learned knowledge\\nwith structured rules and real-time interactions, potentially pushing the boundaries of what we consider a world model.\\nLooking forward, open questions remain . How can we ensure that world models exhibit long-term stability and\\nreliability in real-world settings? How do we handle the inherent uncertainty in dynamic environments while\\nmaintaining the flexibility to adapt? Furthermore, as agents grow more sophisticated, how can we design systems that\\nare both efficient andscalable across increasingly complex tasks without incurring massive computational costs?\\nIn conclusion, the future of world models lies in their ability to balance the need for generalization with the requirement\\nfordomain expertise . By continuing to explore and refine the interplay between model simplicity and complexity,\\nbetween external and internal approaches, we move closer to developing AI systems that not only understand the world\\nbut can actively shape their understanding to navigate and adapt in a rapidly changing reality.\\n62\\nChapter 5\\nReward\\nRewardExtrinsic RewardDense RewardInstructGPT [ 43] DRO [ 367] sDPO [ 368]\\nΨPO [ 369]β-DPO [ 370] ORPO [ 371]\\nDNO [ 372]f-DPO [ 373] [374] [375]\\nSparse RewardPAFT [ 376] SimPO [ 377]\\nLiPO [ 378] RRHF [ 379] PRO [ 380]\\nD2O [381] NPO [ 382] [383]\\nDelayed Reward CPO [ 384] NLHF [ 385] [386]\\nAdaptive RewardInstructGPT [ 43] DRO [ 367]β-DPO [ 370]\\nORPO [ 371] PAFT [ 376] SimPO [ 377]\\nNLHF [ 385] [386]f-DPO [ 373]\\nIntrinsic RewardCuriosity-\\nDriven Reward[387] [388] Plan2Explore [ 389]\\nDiversity Reward LIIR [ 390]\\nCompetence-\\nBased RewardCURIOUS [ 391] Skew-Fit [ 392]\\nDISCERN [ 393] [394] KTO [ 395]\\nExploration Reward [394] [396]\\nInformation\\nGain Reward[397] VIME [ 398] EMI [ 399]\\nMAX [ 400] KTO [ 395]\\nHybrid RewardCombination\\nof Intrinsic and\\nExtrinsic Rewardd-RLAIF [ 401] [402] [403] [404]\\nHierarchical Reward Hierarchical Reward TDPO [ 405]\\nFigure 5.1: Illustrative Taxonomy of Reward system\\nRewards help the agent distinguish between beneficial and detrimental actions, shaping its learning process and\\ninfluencing its decision-making. This chapter first introduces common reward substances in the human body and the\\ncorresponding reward pathways. Then, the reward paradigm under the agent and the different methods involved are\\ndefined. In the discussion section, the influence relationship between other modules is described, and the existing\\nmethods are summarized, then the problems that need to be solved in the future and the optimization directions are\\ndiscussed.\\n63\\nTable 5.1: The comparison of human common reward pathways.\\nReward Pathway Neurotransmitter Mechanism\\nMesolimbic path-\\nway [406]Dopamine Dopaminergic neurons in the ventral tegmental area (VTA) extend pro-\\njections to the nucleus accumbens, where they release dopamine to\\nregulate reward-related signaling. Dopamine diffuses across the synaptic\\ncleft and binds to dopamine receptors—primarily D1-like (excitatory\\nvia Gs proteins, increasing cAMP) and D2-like (inhibitory via Gi pro-\\nteins, reducing cAMP)—thereby modulating reward, motivation, and\\nreinforcement.\\nMesocortical path-\\nway [407]Dopamine Dopaminergic projections from the VTA reach the prefrontal cortex\\n(PFC). Here, dopamine binds to its receptors to influence cognitive\\nfunctions such as decision-making, working memory, and emotional\\nregulation, all of which contribute to evaluating and anticipating rewards.\\nNigrostriatal path-\\nway [407]Dopamine Dopamine’s action on D1 and D2 receptors in the striatum helps shape\\nboth motor routines and reward-related behaviors.\\nLocus\\ncoeruleus [408]Norepinephrine Neurons in the locus coeruleus release norepinephrine to widely dis-\\ntributed targets across the brain. At synapses, norepinephrine binds to\\nadrenergic receptors( αandβsubtypes), modulating neuronal excitabil-\\nity, arousal, attention, and stress responses. These modulatory effects\\ncan indirectly influence reward processing and decision-making circuits.\\nGlutamatergic pro-\\njection [409]Glutamate Upon releasing into the synaptic cleft, glutamate binds to both ionotropic\\nreceptors (such as AMPA and NMDA receptors) and metabotropic re-\\nceptors located on the postsynaptic neuron, thereby initiating excitatory\\nsignaling. This binding produces excitatory postsynaptic potentials and\\nis crucial for synaptic plasticity and learning within reward circuits.\\nGABAergic modu-\\nlation [410]Gamma-\\nAminobutyric\\nAcid (GABA)GABA serves as the principal inhibitory neurotransmitter. At the synapse,\\nGABA binds to GABAA receptors and GABAB receptors. This binding\\nresults in hyperpolarization of the postsynaptic cell, thereby providing\\ninhibitory regulation that balances excitatory signals in the reward net-\\nwork.\\n5.1 The Human Reward Pathway\\nThe brain’s reward system is broadly organized into two major anatomical pathways. The first is the medial forebrain\\nbundle, which originates in the basal forebrain and projects through the midbrain, ultimately terminating in brainstem\\nregions. The second is the dorsal diencephalic conduction system, which arises from the rostral portion of the medial\\nforebrain bundle, traverses the habenula, and projects toward midbrain structures [ 407]. The feedback mechanisms and\\nsubstances in the human brain are complex, involving a variety of neurotransmitters, hormones, and other molecules,\\nwhich regulate brain function, emotions, cognition, and behavior through feedback mechanisms such as neurotransmitter\\nsystems and reward circuits. Feedback mechanisms can be positive (such as feedback in the reward system) or negative\\n(such as inhibiting excessive neural activity). Well-known feedback substances [ 411] include dopamine, neuropeptides,\\nendorphins, glutamate, etc.\\nDopamine is a signaling molecule that plays an important role in the brain, affecting our emotions, motivation,\\nmovement, and other aspects [ 412]. This neurotransmitter is critical for reward-based learning, but this function can be\\ndisrupted in many psychiatric conditions, such as mood disorders and addiction. The mesolimbic pathway [ 406], a key\\ndopaminergic system, originates from dopamine-producing neurons in the ventral tegmental area (VTA) and projects\\nto multiple limbic and cortical regions, including the striatum, prefrontal cortex, amygdala, and hippocampus. This\\npathway plays a central role in reward processing, motivation, and reinforcement learning, and is widely recognized as\\na core component of the brain’s reward system. Neuropeptides are another important class of signaling molecules in\\nthe nervous system, involved in a variety of functions from mood regulation to metabolic control, and are slow-acting\\nsignaling molecules. Unlike neurotransmitters, which are limited to synapses, neuropeptide signals can affect a wider\\nrange of neural networks and provide broader physiological regulation. There is a significant cortical-subcortical\\ngradient in the distribution of different neuropeptide receptors in the brain. In addition, neuropeptide signaling has been\\nshown to significantly enhance the structure-function coupling of brain regions and exhibit a specialized gradient from\\n64\\nsensory-cognitive to reward-physical function [ 413]. Table 5 lists the common reward pathways in the human brain, the\\nneurotransmitters they transmit, and the corresponding mechanisms of action, describing the basic framework of the\\nhuman brain reward system.\\n5.2 From Human Rewards to Agent Rewards\\nHaving examined the foundations of human reward pathways, we now turn to how artificial agents learn and optimize\\nbehavior through reward signals. While biological systems rely on complex neurochemical and psychological feedback\\nloops, artificial agents operate using formalized reward functions designed to guide learning and decision-making.\\nThough inspired by human cognition, agent reward mechanisms are structurally and functionally distinct. Understanding\\nthe analogies and disanalogies between these systems is crucial for aligning artificial behavior with human preferences.\\nIn humans, rewards are deeply embedded in a rich web of emotional, social, and physiological contexts. They emerge\\nthrough evolutionarily tuned mechanisms involving neurotransmitters like dopamine and are shaped by experiences,\\nculture, and individual psychology. In contrast, artificial agents rely on mathematically defined reward functions that\\nare externally specified and precisely quantified. These functions assign scalar or probabilistic feedback to actions or\\nstates, providing a signal for optimization algorithms such as reinforcement learning [3, 414].\\nOne key distinction lies in the programmability and plasticity of agent rewards. Unlike human reward systems, which\\nare constrained by biological architecture and evolutionary inertia, agent reward functions are fully customizable\\nand can be rapidly redefined or adjusted based on task requirements. This flexibility enables targeted learning but\\nalso introduces design challenges—specifying a reward function that accurately captures nuanced human values is\\nnotoriously difficult.\\nAnother important disanalogy concerns interpretability and generalization. Human rewards are often implicit and\\ncontext-dependent, whereas agent rewards tend to be explicit and task-specific. Agents lack emotional intuition and\\ninstinctual drives; their learning depends entirely on the form and fidelity of the reward signal. While frameworks like\\nreinforcement learning from human feedback (RLHF) attempt to bridge this gap by using preference data to shape\\nagent behavior [ 12], such methods still struggle with capturing the full complexity of human goals, especially when\\npreferences are intransitive, cyclical, or context-sensitive [321].\\nMoreover, attempts to borrow from human reward mechanisms—such as modeling intrinsic motivation or social\\napproval—face limitations due to the absence of consciousness, embodiment, and subjective experience in artificial\\nagents. Consequently, while human reward systems offer valuable inspiration, the design of agent reward functions\\nmust address fundamentally different constraints, including robustness to misspecification, adversarial manipulation,\\nand misalignment with long-term human interests.\\nThe following section will delve deeper into agent reward models, focusing on their design principles, evolution, and\\nhow these models selectively incorporate human-inspired insights to optimize artificial behavior within formal systems.\\n5.3 AI Reward Paradigms\\nRewards also exist in intelligent agents, especially in reinforcement learning scenarios. Rewards are the core signal\\nused to guide how intelligent agents act in the environment. They express feedback on the behavior of intelligent agents\\nand are used to evaluate an action’s quality in a certain state, thereby affecting the decision-making of subsequent\\nactions. Through continuous trial and error and adjustment, intelligent agents learn to choose behavioral strategies that\\ncan obtain high rewards in different states.\\n5.3.1 Definitions and Overview\\nIn reinforcement learning, the reward model dictates how an agent is provided with feedback according to the actions it\\nperforms within its environment. This model plays a crucial role in guiding the agent’s behavior by quantifying the\\ndesirability of actions in a given state, thus influencing its decision-making.\\nFormal Definition. The agent’s interaction with its environment can be framed within the formalism of a Markov\\nDecision Process (MDP) [415], which is represented as:\\nM= (S,A, P, r, γ ), (5.1)\\nwhere:\\n65\\n•Sdenotes the state space, encompassing all possible states in the environment.\\n•Adenotes the action space, which encompasses all actions available to the agent at any given state.\\n•P(s′|s, a)defines the state transition probability. It represents the likelihood of transitioning to state s′after\\nthe agent takes action ain state s.\\n•r(s, a)specifies the reward function, which assigns an immediate scalar reward received by the agent for\\nexecuting action ain state s.\\n•γ∈[0,1]is the discount factor, which controlls the agent’s preference for immediate versus future rewards by\\nweighting the contribution of future rewards to the overall return.\\nThe reward function r(s, a)serves as a fundamental component in the formulation of the Agent Reward Model. It is\\nmathematically represented as:\\nr(s, a) :S × A → R (5.2)\\nThis function returns a scalar reward based on the agent’s current state sand the action ait selects. The scalar value\\nr(s, a)is a feedback signal that indicates the immediate benefit (or cost) of the chosen action in the given state. This\\nreward signal guides the agent’s learning process, as it helps evaluate the quality of actions taken within specific\\ncontexts.\\nObjective of the Agent Reward Model. The agent’s primary objective is to maximize its overall cumulative reward\\nover time. This is typically achieved by selecting actions that yield higher long-term rewards, which are captured in the\\nform of the return Gtat time step t, defined as the sum of future discounted rewards:\\nGt=∞X\\nk=0γkrt+k, (5.3)\\nwhere rt+kdenotes the reward received at time step t+k, and γkis the discount factor applied to rewards received at\\ntime step t+k. The agent aims to optimize its policy by maximizing the expected return over time.\\nAt a higher level, the reward model can be classified into three categories based on the origin of the feedback signal: i)\\nextrinsic reward, ii) intrinsic reward, iii) hybrid reward and iv) hierarchical model. Each of these categories can be\\nfurther subdivided into smaller subclasses. Figure 5.2 illustrates different types of rewards. Next, we will explore these\\ndifferent types of reward in more detail, outlining the distinct features and applications of each type.\\nAgentWorldActionFeedbackRewardRewardEvaluator          Agent           WorldActionFeedbackRewardSelf-Evaluation\\nWorldActionFeedbackRewardRewardEvaluator          Agent           RewardSelf-EvaluationAgentWorldActionFeedbackRewardRewardEvaluator and/or Self-Evaluation(a) Extrinsic Reward(b) Intrinsic Reward\\n(c) Hybrid Reward(d) Hierarchical Reward\\nFigure 5.2: Illustration of different types of reward.\\n66\\n5.3.2 Extrinsic Rewards\\nExtrinsic rewards are externally defined signals that guide an agent’s behavior toward specific goals. In artificial\\nlearning systems, especially reinforcement learning, these signals serve as a proxy for success that shape the policy\\nthrough measurable outcomes. However, the structure and delivery of these rewards significantly influence the learning\\ndynamics, which present different trade-offs depending on how feedback is distributed.\\nDense Reward. Dense reward signals provide high-frequency feedback, typically at every step or after each action. This\\nfrequent guidance accelerates learning by allowing agents to immediately associate actions with outcomes. However,\\ndense feedback can sometimes incentivize short-sighted behavior or overfit to easily measurable proxies rather than\\ndeeper alignment.\\nFor example, InstructGPT [ 43] uses human rankings of model outputs to provide continuous preference signals\\nthroughout fine-tuning, enabling efficient behavior shaping. Similarly, Cringe Loss [ 416] and its extensions [ 374]\\ntransform pairwise human preferences into dense training objectives, offering immediate signal at each comparison.\\nDirect Reward Optimization (DRO) [ 367] further simplifies this paradigm by avoiding pairwise comparisons entirely,\\nassociating each response with a scalar score—making the reward signal more scalable and cost-effective. These\\nmethods exemplify how dense feedback facilitates fine-grained optimization but must be carefully designed to avoid\\nsuperficial alignment.\\nSparse Reward. Sparse rewards are infrequent and typically only triggered by major milestones or task completions.\\nWhile they often reflect more meaningful or holistic success criteria, their delayed nature can make credit assignment\\nmore difficult, especially in complex environments.\\nPAFT [ 376] exemplifies this challenge by decoupling supervised learning and preference alignment, with feedback\\napplied only at select decision points. This sparsity reflects a more global notion of success but increases the burden on\\noptimization. Similarly, SimPO [ 377] uses log-probability-based implicit rewards without dense comparisons. The\\nsparsity simplifies the training pipeline but can limit responsiveness to subtle preference shifts. Sparse reward systems\\nthus tend to be more robust but demand stronger modeling assumptions or more strategic exploration.\\nDelayed Reward. Delayed rewards defer feedback until after a sequence of actions, requiring agents to reason about\\nlong-term consequences. This setup is essential for tasks where intermediate steps may be misleading or only make\\nsense in retrospect. The challenge lies in attributing outcomes to earlier decisions, which complicates learning but\\nencourages planning and abstraction.\\nContrastive Preference Optimization (CPO) [ 384] trains models by comparing sets of translations rather than evaluating\\neach one in isolation. The reward signal arises only after generating multiple candidates, reinforcing patterns across\\niterations. Nash Learning from Human Feedback [ 385] similarly delays feedback until the model identifies stable\\nstrategies through competitive comparisons. These methods leverage delayed rewards to push beyond surface-level\\noptimization, aligning more with long-term goals at the cost of slower convergence and more complex training dynamics.\\nAdaptive Reward. Adaptive rewards evolve dynamically in response to the agent’s behavior or learning progress. By\\nmodulating the reward function such as increasing task difficulty or shifting reward targets, this approach supports\\ncontinual improvement, especially in non-stationary or ambiguous environments. However, it introduces additional\\ncomplexity in reward design and evaluation.\\nSelf-Play Preference Optimization (SPO) [ 386] adapts rewards based on self-play outcomes, using social choice theory\\nto aggregate preferences and guide learning. This approach allows the system to refine itself by evolving internal\\nstandards. f-DPO [ 373] builds on this idea by introducing divergence constraints that adapt the reward landscape during\\ntraining. By tuning alignment-diversity trade-offs dynamically, these methods enable robust preference modeling under\\nuncertainty, though they require careful calibration to avoid instability or unintended bias.\\n5.3.3 Intrinsic Rewards\\nIntrinsic rewards serve as internally generated signals that motivate agents to explore, learn, and improve, independent\\nof external task-specific outcomes. These rewards are often structured to promote generalization, adaptability, and\\nself-directed skill acquisition—qualities critical for long-term performance in complex or sparse-reward environments.\\nDifferent intrinsic reward paradigms focus on fostering distinct behavioral tendencies within agents.\\nCuriosity-Driven Reward. This reward encourages agents to reduce uncertainty by seeking novel or surprising\\nexperiences. The key concept is to incentivize the agent to explore novel states where prediction errors are significant.\\nThis paradigm excels in sparse-reward settings by promoting information acquisition when external guidance is limited.\\nFor example, Pathak et al. [ 387] leverage an inverse dynamics model to predict the outcome of actions, creating a\\nfeedback loop that rewards novelty. Plan2Explore [ 389] extends this further by incorporating forward planning to\\n67\\nactively target areas of high epistemic uncertainty, thereby enabling faster adaptation to unseen environments. While\\neffective at discovery, curiosity-driven methods can be sensitive to noise or deceptive novelty without safeguards.\\nDiversity Reward. Diversity reward shifts focus from novelty to behavioral heterogeneity, encouraging agents to\\nexplore a wide range of strategies rather than converging prematurely on suboptimal solutions. This approach is\\nparticularly useful in multi-agent or multimodal settings, where strategic variety enhances robustness and collective\\nperformance. LIIR [ 390] exemplifies this by assigning personalized intrinsic signals to different agents, driving them\\ntoward distinct roles while maintaining shared objectives. Diversity-driven exploration fosters broader policy coverage\\nbut may require careful balancing to avoid destabilizing coordination or goal pursuit.\\nCompetence-Based Reward. Competence-based reward aims to foster learning progress by rewarding improvements\\nin the agent’s task proficiency. This reward adapts dynamically as the agent grows more capable, which creates a\\nself-curriculum that supports continual skill acquisition. Skew-Fit [ 392] facilitates this through entropy-based goal\\nsampling, encouraging agents to reach diverse states while maintaining challenge. CURIOUS [ 391] further automates\\ncurriculum generation by selecting goals that maximize learning progress over time. Competence-based methods are\\nwell-suited for open-ended environments, though they often require sophisticated estimation of progress and goal\\ndifficulty.\\nExploration Reward. Exploration reward directly incentivizes the agent to engage with under-explored states or actions,\\nwhich emphasize breadth over depth in environment interaction. Unlike curiosity, which focuses on unpredictability,\\nexploration reward often targets coverage or novelty relative to the agent’s visitation history. RND [ 394] exemplifies\\nthis by rewarding the prediction error of a randomly initialized network, pushing the agent toward unfamiliar states.\\nThis approach helps prevent premature convergence and encourages robustness, though it may lack focus if not paired\\nwith meaningful learning objectives.\\nInformation Gain Reward. Information gain reward formalizes exploration as a process of uncertainty reduction,\\nwhich guides agents to take actions that yield the highest expected learning. This reward is grounded in information\\ntheory and is especially powerful in model-based or reasoning-intensive tasks. CoT-Info [ 397] applies this to language\\nmodels by quantifying knowledge gain at each reasoning step, optimizing sub-task decomposition. VIME [ 398]\\nsimilarly employs Bayesian inference to reward belief updates about environmental dynamics. By explicitly targeting\\ninformational value, these methods offer principled exploration strategies, though they often incur high computational\\ncost and require accurate uncertainty modeling.\\n5.3.4 Hybrid Rewards\\nHybrid reward frameworks integrate multiple sources of feedback, most commonly intrinsic and extrinsic rewards,\\nto enable more balanced and adaptive learning. By combining the exploratory drive of intrinsic rewards with the\\ngoal-directed structure of extrinsic rewards, these systems aim to improve both sample efficiency and generalization.\\nThis paradigm is especially beneficial in complex environments or open-ended tasks, where pure reliance on either\\nfeedback type may be insufficient.\\nA core advantage of hybrid rewards is their capacity to resolve the exploration-exploitation trade-off dynamically. For\\ninstance, Xiong et al. [ 403] combine intrinsic exploration with extrinsic human feedback within the context of RLHF.\\nUsing a reverse-KL regularized contextual bandit framework, they facilitate strategic exploration while aligning the\\nagent’s actions with human preferences. The method integrates intrinsic and extrinsic rewards through an iterative DPO\\nalgorithm and multi-step rejection sampling, optimizing exploration and alignment without compromising efficiency.\\n5.3.5 Hierarchical Rewards\\nHierarchical reward architectures decompose complex objectives into layered subgoals, each associated with distinct\\nreward signals. This structure mirrors the hierarchical organization of many real-world tasks, allowing agents to\\ncoordinate short-term decisions with long-term planning. By assigning lower-level rewards to immediate actions and\\nhigher-level rewards to abstract goals, agents can learn compositional behaviors that scale more effectively to complex\\nenvironments.\\nIn language modeling, Token-level Direct Preference Optimization (TDPO) [ 405] illustrates this principle by aligning\\nLLMs through fine-grained token-level rewards derived from preference modeling. Using forward KL divergence and\\nthe Bradley-Terry model, TDPO simultaneously refines local choices and global coherence, improving alignment with\\nnuanced human preferences. The hierarchical reward process here is not merely a structural design but a functional one:\\nreinforcing both micro-decisions and macro-outcomes in a coordinated fashion.\\n68\\nMore generally, hierarchical rewards can serve as scaffolding for curriculum learning, where agents progressively learn\\nfrom simpler subtasks before tackling the overarching objective. In LLM agents, this might mean structuring rewards\\nfor subcomponents like tool-use, reasoning chains, or interaction flows, each of which contributes to broader task\\nsuccess.\\n5.4 Summary and Discussion\\n5.4.1 Interaction with Other Modules\\nIn intelligent systems, reward signals function not only as outcome-driven feedback but as central regulators that\\ninterface with core cognitive modules such as perception, emotion, and memory. In the context of LLM-based agents,\\nthese interactions become particularly salient, as modules like attention, generation style, and retrieval memory can be\\ndirectly influenced through reward shaping, preference modeling, or fine-tuning objectives.\\nPerception . In LLM agents, perception is often realized through attention mechanisms that prioritize certain tokens,\\ninputs, or modalities. Reward signals can modulate these attention weights implicitly during training, reinforcing\\npatterns that correlate with positive outcomes. For example, during reinforcement fine-tuning, reward models may\\nupweight specific linguistic features—such as informativeness, factuality, or politeness—causing the model to attend\\nmore to tokens that align with these traits. This parallels how biological perception prioritizes salient stimuli via\\nreward-linked attentional modulation [ 417]. Over time, the agent internalizes a perception policy: not merely “what is\\nsaid,” but “what is worth paying attention to” in task-specific contexts.\\nEmotion . Though LLMs do not possess emotions in the biological sense, reward signals can guide the emergence\\nof emotion-like expressions and regulate dialogue style. In human alignment settings, models are often rewarded for\\ngenerating responses that are empathetic, polite, or cooperative—leading to stylistic patterns that simulate emotional\\nsensitivity. Positive feedback may reinforce a friendly or supportive tone, while negative feedback suppresses dismissive\\nor incoherent behavior. This process mirrors affect-driven behavior regulation in humans [ 418], and allows agents to\\nadapt their interaction style based on user expectations, affective context, or application domain. In multi-turn settings,\\nreward-modulated style persistence can give rise to coherent personas or conversational moods.\\nMemory . Memory in LLM agents spans short-term context (e.g., chat history) and long-term memory modules such as\\nretrieval-augmented generation (RAG) or episodic memory buffers. Reward signals shape how knowledge is encoded,\\nreused, or discarded. For instance, fine-tuning on preference-labeled data can reinforce certain reasoning paths or factual\\npatterns, effectively consolidating them into the model’s internal knowledge representation. Moreover, mechanisms\\nlike experience replay or self-reflection—where agents evaluate past outputs with learned reward estimators—enable\\nselective memory reinforcement, akin to dopamine-driven memory consolidation in biological systems [ 419]. This\\nallows LLM agents to generalize from prior successful strategies and avoid repeating costly errors.\\nIn general, reward in LLM-based agents is not a passive scalar signal but an active agent of behavioral shaping. It\\nmodulates attention to promote salient features, guides stylistic and affective expression to align with human preferences,\\nand structures memory to prioritize useful knowledge. As agents evolve toward greater autonomy and interactivity,\\nunderstanding these cross-module reward interactions will be essential for building systems that are not only intelligent,\\nbut also interpretable, controllable, and aligned with human values.\\n5.4.2 Challenges and Directions\\nAlthough extensive research has been conducted on various reward mechanisms, several persistent challenges remain.\\nOne fundamental issue is reward sparsity and delay. In many real-world scenarios, reward signals are often infrequent\\nand delayed, making it difficult for an agent to accurately attribute credit to specific actions. This, in turn, increases the\\ncomplexity of exploration and slows down the learning process.\\nAnother significant challenge is the potential for reward hacking. Agents, in their pursuit of maximizing rewards,\\nsometimes exploit unintended loopholes in the reward function. This can lead to behaviors that diverge from the\\nintended design goals, particularly in complex environments where optimization objectives may not always align with\\nthe true task requirements.\\nMoreover, the process of reward shaping presents a delicate balance. While shaping rewards can accelerate learning by\\nguiding an agent toward desired behaviors, excessive or poorly designed shaping may lead to local optima, trapping the\\nagent in suboptimal behaviors. In some cases, it may even alter the fundamental structure of the original task, making it\\ndifficult for the agent to generalize to other scenarios.\\n69\\nMany real-world problems are inherently multi-objective in nature, requiring agents to balance competing goals. Under\\na single reward function framework, finding the right trade-offs between these objectives remains an open problem.\\nIdeally, a hierarchical reward mechanism could be designed to guide learning in a structured, step-by-step manner.\\nHowever, constructing such mechanisms effectively is still a challenge.\\nFinally, reward misspecification introduces further uncertainty and limits generalization. Often, a reward function does\\nnot fully capture the true task goal, leading to misalignment between the agent’s learning objective and real-world\\nsuccess. Additionally, many reward functions are tailored to specific environments and fail to generalize when conditions\\nchange or tasks shift, highlighting the need for more robust reward models.\\nAddressing these challenges requires novel approaches. One promising direction is to derive implicit rewards from\\nstandard examples or outcome-based evaluations, which can help mitigate reward sparsity issues. Additionally,\\ndecomposing complex tasks into hierarchical structures and designing rewards from the bottom up can offer a more\\nsystematic approach, even in multi-objective settings. Furthermore, leveraging techniques such as meta-learning and\\nmeta-reinforcement learning can enhance the adaptability of reward models, allowing agents to transfer knowledge\\nacross tasks and perform effectively in diverse environments. By exploring these avenues, we can move toward more\\nreliable and scalable reward mechanisms that better align with real-world objectives.\\n70\\nChapter 6\\nEmotion Modeling\\nEmotions are a key part of how humans think, make decisions, and interact with others. They guide us to understand\\nsituations, make choices, and build relationships. Antonio Damasio, in his book Descartes’ Error [25], explained that\\nemotions are not separate from logic. Instead, they are deeply connected to how we reason and act. When developing\\nLLM agents, adding emotional capabilities can potentially make these systems smarter, more adaptable, and better\\nunderstand the world around them.\\nFor LLM agents, emotions can act as a decision-making tool, much like they do for humans. Emotions help us prioritize\\ntasks, understand risks, and adapt to new challenges. Marvin Minsky, in The Emotion Machine [420], described\\nemotions as a way to adjust our thinking processes, helping us solve problems in a more flexible and creative manner.\\nSimilarly, LLM agents with emotion-like features could improve their ability of solving complex problems and making\\ndecisions in a more human-style.\\nHowever, the integration of emotions into LLM agents is still in its early stages. Researchers are just starting to\\nexplore how emotional capabilities can improve these systems. Furthermore, there is great potential for LLM agents\\nto support human emotional well-being, whether through empathetic conversations, mental health support, or simply\\nbuilding better connections with users. This promising but challenging area requires collaboration between fields such\\nas psychology, cognitive science, and AI ethics. As research advances, emotion-understanding LLM agents could\\nredefine how we interact with technology, creating deeper trust and more meaningful relationships between humans and\\nmachines.\\nIn the following subsections, we will delve deeper into the role of emotions in shaping LLM agents. We will explore\\nhow emotions can be used to enhance learning and adaptability, how LLMs understand human emotions, and how\\nthese systems express and model their own emotional states. We will also examine how emotions can be manipulated\\nto influence LLM agents’ behavior and personalities, as well as the ethical and safety concerns that arise from these\\ncapabilities. Each of these discussions builds on the foundational importance of emotion to create LLM agents that are\\nmore intelligent, empathetic, and aligned with human values.\\n6.1 Psychological Foundations of Emotion\\nPsychological and neuroscientific theories of emotion provide essential frameworks for developing emotionally\\nintelligent LLM agents. These theories can be categorized into several major approaches, each offering unique\\nperspectives on how emotions function and how they might be implemented in AI systems.\\nCategorical Theories. These models posit that emotions exist as discrete, universal categories with distinct physiolog-\\nical and behavioral signatures. Ekman’s theory of basic emotions [ 421] identifies six fundamental emotions (anger,\\ndisgust, fear, happiness, sadness, and surprise) that are recognized across cultures and expressed through specific\\nfacial configurations. This discrete approach has significantly influenced affective computing, with many emotion\\nclassification systems in AI adopting these labels for training [ 422,423]. For LLM agents, categorical frameworks\\nprovide clear taxonomies for classifying user emotions and generating appropriate responses. However, they face\\ncriticism for oversimplifying the complex, blended nature of human emotional experience [ 424] and may not capture\\ncultural variations in emotional expression [425].\\n71\\nDimensional Models. Rather than discrete categories, dimensional approaches represent emotions as points in a\\ncontinuous space defined by fundamental dimensions. Russell’s Circumplex Model [ 426] maps emotions onto two\\nprimary dimensions: valence (pleasure-displeasure) and arousal (activation-deactivation). This framework enables more\\nnuanced tracking of emotional states. It distinguishes between high-arousal panic and low-arousal anxiety despite both\\nhaving negative valence. The PAD (Pleasure-Arousal-Dominance) model [ 427] extends this by adding a dominance\\ndimension, capturing the sense of control or power associated with emotional states. These continuous representations\\nhave proven valuable for LLM systems that need to generate emotionally graded responses or track subtle shifts in user\\naffect over time [ 428,429,430]. Dimensional models allow for fine-grained control over generated content, enabling\\nhumans or agents to modulate tone along continuous scales rather than switching between discrete emotional states.\\nHybrid and Componential Frameworks. Recognizing limitations in pure categorical or dimensional approaches,\\nseveral theories integrate aspects of both. Plutchik’s Wheel of Emotions [ 431] arranges eight primary emotions in\\na wheel structure with intensity gradients and dimensional properties, allowing for the representation of complex\\nemotional blends (e.g., love as a mixture of joy and trust). Meanwhile, componential models like Scherer’s Component\\nProcess Model (CPM) [ 432] conceptualize emotions as emerging from synchronized components including cognitive\\nappraisal, physiological arousal, action tendencies, and subjective feelings. Particularly influential in AI research is the\\nOCC (Ortony-Clore-Collins) model [ 433], which defines 22 emotion types based on how events, agents, or objects\\nare evaluated relative to goals and standards. These appraisal-based frameworks have been implemented in dialogue\\nsystems that generate emotional responses through rule-based evaluation of situations [ 434,435]. For LLM agents,\\nsuch models provide computational structures for evaluating text input and selecting contextually appropriate emotional\\nresponses, improving both coherence and perceived empathy [436, 437].\\nNeurocognitive Perspectives. The neuroscience of emotion offers additional insights for LLM architectures. Damasio’s\\nsomatic marker hypothesis [ 25] emphasizes how emotions, implemented through body-brain interactions, guide decision-\\nmaking by associating physiological states with anticipated outcomes. This interaction between the limbic system and\\nthe cortex shows a two-process architecture: fast “alarm” signals in the limbic system, like those processed by the\\namygdala, work alongside slower, more deliberate reasoning in the cortex. Contemporary LLM systems have begun\\nimplementing analogous architectures, where fast sentiment detection modules work in parallel with more thorough\\nchain-of-thought reasoning [ 436,437]. Recent evidence further suggests that opponent circuitry in the striatum enables\\ndistributional reinforcement learning by encoding not just mean rewards but entire probability distributions, offering a\\nneural basis for emotion-influenced decision-making under uncertainty [ 438]. Similarly, LeDoux’s distinction between\\n“low road” (quick, automatic) and “high road” (slower, cognitive) fear processing [ 24] suggests design patterns for\\nsystems that need both immediate safety responses and nuanced emotional understanding. Minsky’s framing of emotions\\nas “ways to think” [ 420] that reorganize cognitive processes has influenced frameworks like EmotionPrompt [ 428] and\\nEmotion-LLaMA [423], where emotional context dynamically reshapes LLM reasoning.\\nThese theoretical frameworks increasingly inform the development of emotionally intelligent LLM agents. Categor-\\nical models provide clear labels for emotion classification tasks [ 423,429], while dimensional embeddings enable\\ncontinuous control over generated text [ 428]. Hybrid approaches help systems handle mixed emotions and emotional\\nintensity. Appraisal-based methods, particularly those derived from the OCC model, allow LLMs to evaluate narrative\\nevents or user statements contextually, selecting appropriate emotional responses that foster rapport and trust [ 439].\\nNeuroscientifically-inspired dual-process architectures combine “fast” sentiment detection with “slow” deliberative\\nreasoning, enabling both quick safety responses and deeper emotional understanding [ 436,437]. While explicit\\nneurocognitive mechanisms (like dedicated “amygdala-like” pathways) remain rare in current LLM pipelines, emerging\\nresearch explores biologically-inspired modules to handle urgent emotional signals and maintain consistent emotional\\nstates across extended interactions [440, 441].\\nEmotion is a key part of human intelligence, and it will likely become one of the key components or design considerations\\nof LLM agents. One key future direction is systematically translating these psychological and neuroscience theories\\ninto an LLM agent’s internal processes. Techniques for translating might include using dimensional models (e.g.,\\nvalence/arousal/dominance) as latent states that influence generation or adopting explicit rule-based appraisals (OCC) to\\nlabel user messages and shape the agent’s subsequent moves. Hybrid approaches offer a compelling balance: an LLM\\ncould first recognize a discrete category (e.g., “fear”) but also gauge its intensity and control dimension for finer-grained\\nconversation. Such emotion-infused architectures might yield more coherent “moods” over time, analogous to how\\nhumans sustain affective states rather than resetting at every turn. Explicit alignment with psychological theories also\\nenhances interpretability: designers can debug or refine the agent’s responses by comparing them to well-established\\nemotion constructs, rather than dealing with opaque emergent behaviors.\\nA second direction is harnessing these theories to improve affectionate or supportive interactions , often referred to\\nas emotional alignment. For example, circumplex or PAD-based tracking can help an LLM detect negative valence\\nand high arousal in a user’s text and respond soothingly (e.g., lowering arousal, offering empathetic reappraisals). In\\n72\\nValence (+) Valence (–)Arousal (+)\\nArousal (–)NeutralHappyExcitedAlert Tense\\nAngry\\nDistressed\\nSad\\nDepressed\\nBored CalmRelaxedContent\\nterror fear apprehensionadmirationtrustacceptance\\necstasyjoyserenity\\nvigilanceanticipationinterest\\nrage anger annoyance\\nloathing\\ndisgust\\nboredomgrief\\nsadness\\npensivenessamazement\\nsurprise\\ndistractionsubmissionlove optimism\\naggressiveness\\ncontempt\\nremorse disapprovalawe\\nsadness anger\\ncontempt disgust\\nsurprise fear1. Dropping upper\\n    eyelids\\n2. Losing focus in eyes\\n3. Slight pulling down\\n    of lip corners\\n1. Lip corner tightened\\n    and raised on only\\n    one side of face\\n1. Eyebrows raised\\n2. Eyes widened\\n3. Mouth open1. Eyebrows down\\n    and together\\n2. Eyes glare\\n3. Narrowing of the lips\\n1. Nose wrinkling\\n2. Upper lip raised\\n1. Eyebrows raised and\\n    Pulled together\\n2. Raised upper eyelids\\n3. Tensed lower eyelids\\n4. Lips slightly stretched\\n    horizontally back to ears\\nSensory\\nCortexPrefrontal\\nCortexHippocampus\\nBasal \\ngangliaSensory \\nThalamus\\nHypothalamus Brain stemAMYGDALAperception and \\nshort-term storageworking memory and executive functions\\nlong-term \\nexplicit memory\\nmotor control\\nemotional responsesStimulus with\\nEmotional \\nsigniﬁcance(a) Categorical Model\\n(Ekman’s Six Universal Facial Expressions)(b) Dimensional Model\\n(Russell’s Circumﬂex Model)\\n(c) Hybrid Model\\n(Plutchik’s Wheel of Emotions)(d) Neurocognitive Model\\n(LeDoux’s Amygdala-Centered Model)Figure 6.1: Visualization and examples of major emotion theory categories. (a) Categorical Theories: Ekman’s\\nsix basic emotions [ 421] showing discrete emotional states. (b) Dimensional Models: Russell’s Circumplex [ 426]\\nrepresenting emotions as coordinates in continuous space. (c) Hybrid/Componential Frameworks: Plutchik’s Wheel\\n[431] combining intensity gradients with categorical emotions. (d) Neurocognitive Perspectives: LeDoux’s Amygdala-\\nCentered Model [ 24] showing dual-pathway processing of emotional stimuli. These psychological foundations inform\\ndifferent approaches to emotion modeling in AI systems, from discrete classification to dimensional representations,\\nappraisal-based reasoning, and multi-pathway information processing.\\nmental health or counseling scenarios, an appraisal-informed method could let the agent validate the user’s feelings\\nand understand their situation in terms of goal incongruence or perceived blame, which helps craft responses that\\nconvey genuine empathy. Grounding emotional outputs in cognitive theories (like “relief” if a negative outcome is\\navoided, or “gratitude” when a user helps the system) likewise makes interactions feel more natural and ethically aligned.\\nThese enhancements are particularly salient as LLMs migrate into real-world applications like customer service, elder\\ncare, and tutoring, where emotional sensitivity can improve outcomes and user well-being. By incorporating robust\\npsychological and limbic-system insights, developers can design LLM agents that not only reason more effectively but\\nalso provide sincere emotional support, bridging the gap between computational precision and human-centric care.\\n73\\n6.2 Incorporating Emotions in AI Agents\\nThe integration of emotional intelligence into large language models (LLMs) has emerged as a transformative approach\\nto enhancing their performance and adaptability. Recent studies, such as those of EmotionPrompt [ 422], highlight\\nhow emotional stimuli embedded in prompts can significantly improve outcomes across various tasks, including a\\nnotable 10.9%improvement in generative task metrics such as truthfulness and responsibility. By influencing the\\nattention mechanisms of LLMs, emotionally enriched prompts enrich representation layers and result in more nuanced\\noutputs [ 422]. These advancements bridge AI with emotional intelligence, offering a foundation for training paradigms\\nthat better simulate human cognition and decision-making, particularly in contexts requiring social reasoning and\\nempathy.\\nMultimodal approaches further elevate the impact of emotional integration. Models like Emotion-LLaMA [ 440]\\ndemonstrate how combining audio, visual, and textual data enables better recognition and reasoning of emotions.\\nUsing datasets such as MERR [ 440], these models align multimodal inputs into shared representations, facilitating\\nimproved emotional understanding and generation. This innovation extends beyond linguistic improvements, offering\\napplications in human-computer interaction and adaptive learning. Together, these methods underscore the critical role\\nof emotions in bridging technical robustness with human-centric AI development, paving the way for systems that are\\nboth intelligent and empathetic.\\n6.3 Understanding Human Emotions through AI\\nTextual Approaches. Recent work highlights the ability of LLMs to perform detailed reasoning about latent sentiment\\nand emotion. Using step-by-step prompting strategies, such as chain of thought reasoning, researchers enable LLMs to\\ninfer sentiment even when explicit cues are absent [ 436]. Beyond single-turn inference, negotiation-based frameworks\\nfurther refine emotional judgments by leveraging multiple LLMs that cross-evaluate each other’s outputs, effectively\\nmimicking a more deliberative human reasoning process [ 437]. These techniques underscore the importance of iterative,\\ncontext-aware strategies to capture subtle emotional signals from purely textual input.\\nMultimodal Approaches. LLMs have also been extended to integrate signals from audio, video, and images. Recent\\nefforts show how additional contextual or world knowledge can be fused with visual and textual information to capture\\ndeeper affective states [ 442]. Moreover, frameworks that convert speech signals into textual prompts demonstrate that\\nvocal nuances can be embedded in LLM reasoning without changing the underlying model architecture [ 443]. This\\nmultimodal integration, combined with explainable approaches, allows for richer and more transparent representations\\nof emotional content [444].\\nSpecialized Frameworks. Beyond generic techniques, specialized systems address tasks in which emotion recognition\\nrequires higher levels of awareness of ambiguity [ 439], context sensitivity, and generative adaptability [ 445]. These\\napproaches emphasize the inherent complexity of human emotion, treating it as dynamic and probabilistic rather than\\nstrictly categorical. Using flexible LLM instruction paradigms, they offer pathways to better interpret ambiguous\\nemotional expressions and integrate contextual cues (e.g., dialogue history), moving LLM closer to human-like\\nemotional comprehension.\\nEvaluation and Benchmarks. To holistically assess the emotional intelligence of LLM, researchers have proposed\\nvarious benchmark suites. Some focus on generalized emotion recognition across different modalities and social con-\\ntexts [ 446,447], while others compare the performance and efficiency of models of varying sizes [ 448]. There are also\\nspecialized benchmarks that evaluate multilingual capabilities [ 449], annotation quality [ 450], or empathetic dialogue\\nsystems [ 451]. Furthermore, frameworks such as EMOBENCH [ 441] and MEMO-Bench [ 452] test nuanced emotional\\nunderstanding and expression in both text and images, while MERBench [ 453] and wide-scale evaluations [ 454] address\\nstandardization concerns in multimodal emotion recognition. Together, these benchmarks reveal the growing, yet still\\nimperfect grasp of human emotion by LLMs, highlighting ongoing challenges such as implicit sentiment detection,\\ncultural adaptation, and context-dependent empathy [455].\\n6.4 Analyzing AI Emotions and Personality\\nReliability of Personality Scales for LLMs. Large language models (LLMs) show conflicting evidence when evaluated\\nthrough human-centered personality tests. On one hand, some studies challenge the validity of common metrics,\\nreporting biases such as “agree bias” and inconsistent factor structures, raising doubts about whether these instruments\\ncapture genuine traits [ 456,457]. On the other hand, systematic experiments reveal that LLMs can exhibit stable,\\nhuman-like trait patterns and even adapt to different personas under specific prompts [ 458,459]. Yet, concerns persist\\n74\\nabout action consistency, alignment of self-knowledge, and whether role-playing agents truly maintain fidelity to their\\nassigned characters [460, 461].\\nPsychometric Methods & Cognitive Modeling Approaches. Recent work applies rigorous psychometric testing,\\ncognitive tasks, and population-based analyses to uncover how LLM processes and represents mental constructs [462,\\n463,464]. Fine-tuning on human behavioral data can align models with decision patterns that mirror individual-level\\ncognition, while population-based sampling techniques expose variability in neural responses [ 465,466]. By merging\\npsychological theories with advanced prompting and embedding methods, researchers illuminate latent representations\\nof constructs like anxiety or risk-taking, showing how LLMs can approximate human reasoning across tasks.\\nEmotion Modeling. Studies on LLM-based emotional intelligence reveal notable abilities to interpret nuanced affect\\nand predict emotion-laden outcomes, often surpassing average human baselines in standard tests [ 423,429]. However,\\nthese models do not necessarily emulate human-like emotional processes; they rely on high-dimensional pattern\\nmatching that sometimes fails under changing contexts, negative input, or conflicting cues [ 467,468]. However,\\nhierarchical emotion structures, coping strategies, and empathy-like behaviors can emerge in larger-scale models,\\nunderscoring both the promise of emotional alignment and the ethical challenges in creating AI systems that appear and\\noccasionally function as affective agents.\\n6.5 Manipulating AI Emotional Responses\\nPrompt-based Methods. Recent research shows that adopting specific personas or roles through well-engineered\\nprompts can bias LLM cognition, allowing targeted emotional or personality outcomes [ 469,470,471,472]. By\\ninserting instructions such as “If you were a [persona]”, LLMs adapt not only their thematic style, but also their\\nunderlying emotional stance. This approach is powerful for real-time manipulation, though it can be inconsistent across\\ntasks and model variants, highlighting the need for more systematic methods.\\nTraining-based Methods. Fine-tuning and parameter-efficient strategies offer deeper, more stable ways to induce or\\nalter LLM emotions [ 473,428,474]. Quantized Low-Rank Adaptation (QLoRA) and specialized datasets can embed\\nnuanced traits such as the Big Five or MBTI profiles directly into the model’s learned weights. These methods enable\\nLLMs to spontaneously exhibit trait-specific behaviors (including emoji use) and sustain their emotional states over\\nlonger dialogues, while also offering interpretability through neuron-level activation patterns.\\nNeuron-based Methods. A recent advance isolates personality-specific neurons and manipulates them directly to\\nevoke or suppress emotional traits [ 475]. By toggling neuron activations pinpointed through psychologically grounded\\nbenchmarks (e.g., PersonalityBench), LLMs can embody targeted emotional dimensions without retraining the entire\\nnetwork. This neuron-centric approach provides fine-grained, dynamic control over model behaviors, representing a\\nleap in precision and efficiency for emotional manipulation in LLMs.\\n6.6 Summary and Discussion\\nManipulation and Privacy Concerns. The rapid adoption of Emotional AI in advertising and politics raises significant\\nmanipulation and privacy risks [ 476,477]. Emotional AI often collects sensitive biometric data, such as facial\\nexpressions and voice tones, to infer emotional states, enabling targeted advertising or political influence. However,\\nthese systems can exploit human emotions for profit or political gain, infringing on fundamental rights and fostering\\nover-surveillance in public spaces [ 478,477]. Regulatory frameworks like GDPR and the EU AI Act are critical to\\nmitigating these risks responsibly.\\nAlignment Issues. Emotional AI’s capacity to detect and interpret emotions is often misaligned with intended outcomes,\\nleading to inaccuracies and biases. Anxiety-inducing prompts, for instance, have been shown to exacerbate biases in\\nlarge language models (LLMs), affecting outputs in high-stakes domains such as healthcare and education [ 479,480].\\nMisinterpretation of emotional cues by AI systems, as seen in workplace applications, can exacerbate discrimination and\\npower imbalances [ 481]. Techniques like reinforcement learning from human feedback (RLHF) have proven effective\\nin mitigating these issues but require further development to ensure robust alignment in diverse contexts [479, 423].\\nEthical Implications. Trust and acceptance of AI systems are significantly influenced by their ability to exhibit empathy\\nand maintain socially appropriate behavior [ 482,483]. However, the commodification of emotions in workplace\\nmanagement and customer service has raised concerns about ethical labor practices and AI-human relationships [ 481].\\nMoreover, Emotional AI’s reliance on anthropomorphic characteristics without sufficient empathy can undermine user\\ntrust [ 482]. Frameworks like SafeguardGPT, which incorporate psychotherapy techniques, demonstrate promising\\napproaches to fostering trust and aligning AI behavior with societal norms [ 484]. Nonetheless, challenges remain in\\nensuring privacy, fairness, and cultural sensitivity [484, 483].\\n75\\nDistinguishing AI Emotional Mimicry from Human Experience. Despite advances in emotion modeling for LLM\\nagents, a fundamental distinction remains: these systems do not actually “feel” emotions as humans do but only show\\nhuman-emotion-like patterns via probabilistic modeling. While LLMs can convincingly simulate emotional responses,\\nrecognize emotional patterns, and generate affectional outputs, they lack the embodied, phenomenological experience\\nthat defines human emotions. This simulation-reality gap creates both technical and ethical challenges. Users frequently\\nanthropomorphize AI systems that display emotion-like behaviors [ 482], potentially leading to misplaced trust or\\nexpectations. This distinction needs to be carefully thought in both research and deployment contexts, as the perceived\\nemotional capabilities of LLMs influence human-AI relationships, ethical frameworks, and regulatory approaches.\\nFuture work should balance enhancing LLMs’ emotional intelligence while maintaining transparency about their\\nfundamental limitations as non-sentient systems.\\n76\\nChapter 7\\nPerception\\nPerception is the foundational gateway through which both humans and intelligent agents acquire information, interpret\\ntheir surroundings, and ultimately make informed decisions. For humans, perception is seamless and intuitive,\\neffortlessly transforming sensory inputs into meaningful interpretations. In artificial intelligence, however, perception\\nsystems are meticulously engineered to emulate—and in some respects surpass—human sensory processing, profoundly\\ninfluencing an agent’s capacity for interaction, learning, and adaptation in complex environments.\\nIn this chapter, we begin by exploring key differences in the nature and efficiency of perception between humans and AI\\nagents. Next, we categorize agent perception based on different forms and representations of perceptual input. We then\\ndiscuss ongoing challenges in the agent perception system and highlight promising directions for improvement, both at\\nthe modeling and system architecture levels. Finally, we illustrate how perception modules can be effectively tailored to\\ndifferent intelligent agent scenarios, offering practical guidance for optimizing their use and suggesting pivotal areas for\\nfuture research.\\n7.1 Human versus AI Perception\\nPerception is fundamental to intelligence, serving as the interface through which both humans and artificial agents\\ninteract with the world. Although humans commonly think of perception in terms of the five classical senses—vision,\\nhearing, taste, smell, and touch—modern neuroscience identifies a richer sensory landscape. Conservatively, humans are\\ndescribed as having around 10 senses; more comprehensive views list approximately 21, while some researchers propose\\nup to 33 distinct sensory modalities [ 546,547]. Beyond the familiar senses, humans possess sophisticated internal\\nperceptions, such as vestibular (balance), proprioception (awareness of body position), thermoception (temperature),\\nand nociception (pain), enabling nuanced interaction with their environment.\\nHuman senses are finely tuned to specific physical signals: for example, human vision detects electromagnetic waves\\nwith wavelengths between approximately 380–780 nm, whereas hearing perceives sound frequencies from about 20\\nHz to 20 kHz [ 548]. These sensory modalities allow humans to effortlessly engage in complex tasks like language\\ncommunication, object recognition, social interaction, and spatial navigation. Additionally, humans naturally perceive\\ncontinuous changes over time, seamlessly integrating motion perception and temporal awareness, abilities essential for\\ncoordinated movement and decision-making [ 549]. Animals in the natural world exhibit even more diverse perceptual\\ncapabilities. Birds and certain marine organisms, for instance, utilize magnetoreception to navigate using Earth’s\\nmagnetic fields, while sharks and electric eels exploit electroreception to sense electrical signals emitted by other\\norganisms—abilities humans do not possess [550].\\nIn contrast to biological perception, artificial agents rely upon engineered sensors designed to transform environmental\\nstimuli into digital signals that algorithms can interpret. Common sensor modalities for AI agents include visual sensors\\n(cameras), auditory sensors (microphones), tactile sensors, and inertial measurement units. AI agents typically excel at\\nprocessing visual, auditory, and textual data, leveraging advances in deep learning and signal processing. However,\\ncertain human sensory abilities—particularly taste and smell—remain challenging for machines to emulate accurately.\\nFor example, the advanced bio-inspired olfactory chip developed by researchers [ 551] currently distinguishes around 24\\ndifferent odors, a capability significantly less sensitive than the human olfactory system, which discriminates among\\nmore than 4,000 distinct smells [552].\\n77\\nPerceptionUnimodal ModelsText BERT [ 485] RoBERTa [ 486] ALBERT [ 487]\\nImageResNet [ 488] DETR [ 489]\\nGrounding DINO 1.5 [ 490]\\nVideo ViViT [ 491] VideoMAE [ 492]\\nAudioFastSpeech 2 [ 493] Seam-\\nless [ 494] wav2vec 2.0 [ 495]\\nOthersVisual ChatGPT [ 496] HuggingGPT [ 152]\\nMM-REACT [ 497] ViperGPT [ 498]\\nAudioGPT [ 499] LLaV A-Plus [ 500]\\nCross-modal ModelsText-ImageCLIP [ 51] ALIGN [ 501] DALL ·E\\n3 [502] VisualBERT [ 503]\\nText-VideoVideoCLIP [ 504] Phenaki [ 505]\\nMake-A-Video [ 506]\\nText-Audio Wav2CLIP [ 507] V ATT [ 508] AudioCLIP [ 509]\\nOthers CLIP-Forge [ 510] Point-E [ 511]\\nMultiModal ModelsVLMMiniGPT-v2 [ 512] LLaV A-NeXT [ 513]\\nCogVLM2 [ 514] Qwen2-VL [ 515] Emu2 [ 516]\\nEdge-SideTinyGPT-V [ 517] Mo-\\nbileVLM [ 518] MiniCPM-\\nV [519] OmniParser [ 520]\\nVLACLIPort [ 521] RT-1 [ 522] MOO [ 523]\\nPerAct [ 524] Diffusion Policy [ 525]\\nPaLM-E [ 526] MultiPLY [ 527]\\nALMAudio Flamingo [ 528] SpeechVerse [ 529]\\nUniAudio 1.5 [ 530] Qwen2-Audio [ 54] Audio-\\nLLM [ 531] Mini-Omni [ 532] SpeechGPT [ 533]\\nA VLMONE-PEACE [ 534] PandaGPT [ 535]\\nMacaw-LLM [ 536] Language-\\nBind [ 537] UnIV AL [ 538] X-LLM [ 539]\\nOthersPointLLM [ 540] MiniGPT-3D [ 541]\\nNExT-GPT [ 542] Unified-IO 2 [ 543]\\nCoDi-2 [ 544] ModaVerse [ 545]\\nFigure 7.1: Illustrative Taxonomy of Perception System.\\nAnother crucial distinction lies in perceptual processing efficiency. Human perception is limited by biological constraints\\nsuch as nerve conduction speeds, typically in the range of milliseconds. Conversely, AI systems can process sensory\\ninputs at speeds of microseconds or even nanoseconds, constrained primarily by computational hardware performance\\nrather than biological limitations. Nevertheless, human perception naturally integrates information from multiple\\nsensory modalities—known as multimodal perception—into coherent experiences effortlessly. For AI agents, achieving\\nthis multimodal integration requires carefully designed fusion algorithms that explicitly combine inputs from diverse\\nsensors to build unified environmental representations [553].\\nFurther differences arise in the way humans and artificial agents handle temporal and spatial information. Human\\nperception is inherently continuous and fluid, smoothly experiencing the passage of time and spatial motion without ex-\\nplicit temporal discretization. In contrast, AI agents typically rely on discrete sampling of sensor data, using timestamps\\nor sequential processing to simulate continuity. Spatial awareness in humans effortlessly merges visual, auditory, and\\nvestibular information to achieve intuitive spatial positioning. For artificial agents, spatial perception usually involves\\n78\\nalgorithmic processes such as simultaneous localization and mapping (SLAM) or 3D scene reconstruction from visual\\ndata sequences [554].\\nPhysical or chemical stimuli transmitted from the external environment to human sensory organs will be received by the\\nsensory system (such as eyes, ears, skin, etc.) and converted into neural signals, which are finally processed by the brain\\nto produce perception of the environment. Similarly, to allow the intelligent agent to connect with the environment,\\nit is also crucial to obtain these perception contents. Currently, various sensors are mainly used to convert electrical\\nsignals into processable digital signals. In this section, We distinguish between Unimodal models, Cross-modal models,\\nand Multimodal models based on the number of modalities involved in the input and whether unified fusion modeling\\noperations are performed. Unimodal Models specifically process and analyze data from a single modality or type\\nof input (such as text, image, or audio), while Cross-modal Models establish relationships and enable translations\\nbetween different modalities through dedicated mapping mechanisms, and Multimodal Models holistically integrate and\\nprocess multiple modalities simultaneously to leverage complementary information for comprehensive understanding\\nand decision-making.\\nVisionSoundSmellTasteVestibularPainTouchThalposisChronoceptionProprioceptionVisceral SenseKinesthetic SenseMagnetoreceptionElectroreceptionHumanAgent\\nFigure 7.2: Comparison of common perceptual types between human and agent.\\n7.2 Types of Perception Representation\\n7.2.1 Unimodal Models\\nWhen humans are in an environment, they can listen to beautiful music, look at sunrise and sunset, or experience a\\nwonderful audiovisual feast on stage. These perception contents can be either a single image or audio, or a fusion\\nof multiple perception contents. Regarding the types of perception input of intelligent agents, we will start with\\nsingle-modal and multimodal inputs, and introduce their implementation and differences.\\nText As an important means of communication, text carries a wealth of information, thoughts, emotions and culture.\\nHumans indirectly obtain the content of text through vision, hearing and touch, which is one of the most important ways\\nfor humans to interact with the environment. But for intelligent agents, text can directly serve as a bridge to connect\\nwith the environment, taking text as direct input and outputting response content. In addition to the literal meaning,\\ntext also contains rich semantic information and emotional color. In the early days, the bag-of-words model [ 555]\\nwas used to count text content and was widely used in text classification scenarios, but semantic expression could not\\nbe obtained. BERT [ 485] uses a bidirectional Transformer architecture for language modeling and captures the deep\\nsemantic information of text through large-scale unsupervised pre-training. [ 486,487] further optimized the training\\nefficiency of BERT. The autoregressive model represented by GPT3.5 [ 556] opened the prelude to LLM and further\\nunified the tasks of text understanding and text generation, while technologies such as LoRA [ 109] greatly reduced the\\napplication cost of LLM and improved the agent’s perception ability of complex real-world scenario tasks.\\nImage Image is another important way for humans to interact with the environment which inherently encode spatial\\ninformation, encompassing crucial attributes such as morphological characteristics, spatial positioning, dimensional\\nrelationships, and kinematic properties of objects. The evolution of computer vision architectures has demonstrated\\nsignificant advancement in processing these spatial attributes. The seminal ResNet architecture [ 488] established\\nfoundational principles for deep visual feature extraction, while subsequent YOLO series [ 557,558] demonstrated the\\ncapability to simultaneously determine object localization and classification with remarkable efficiency. A paradigm\\nshift occurred with the introduction of DETR [ 489], which revolutionized object detection by implementing parallel\\nprediction through global context reasoning, effectively eliminating traditional computational overhead associated with\\nnon-maximum suppression and anchor point generation. More recently, DINO 1.5 [ 490] has extended these capabilities\\nto open-set scenarios through architectural innovations, enhanced backbone networks, and expanded training paradigms,\\n79\\nsubstantially improving open-set detection performance and advancing the perceptual generalization capabilities of\\nartificial agents in unconstrained environments.\\nVideo Video is an expression of continuous image frames, which includes the time dimension and displays dynamic\\ninformation that changes over time through continuous image frames. The intelligent agent uses video as input and\\nobtains richer perceptual content through continuous frames. ViViT [ 491] extracts spatiotemporal markers from videos,\\neffectively decomposing the spatial and temporal dimensions of the input. VideoMAE [ 492] learns general video\\nfeature representations through self-supervised pre-training and has strong generalization capabilities on out-of-domain\\ndata. It lays a solid foundation for intelligent agents to acquire perceptual capabilities in new scenarios.\\nAudio In addition to text and vision, another important way for humans to interact with the environment is through audio.\\nAudio not only contains direct text content, but also contains the speaker’s tone and emotion [ 559]. Wav2Vec2 [ 495]\\ndefines the contrast task by quantizing the potential representation of joint learning, achieving speech recognition\\neffectiveness with 1/100 labeled data volume. FastSpeech 2 [ 493] directly introduces voice change information (pitch,\\nenergy, duration, etc.) and uses real targets to train the model to achieve more realistic text-to-speech conversion.\\nSeamless [ 494] generates low-latency target translations through streaming and using an efficient monotonic multi-head\\nattention mechanism, while maintaining the human voice style, to achieve synchronous speech-to-speech/text translation\\nfrom multiple source languages to target languages. Based on these means, the intelligent agent can achieve the ability\\nto listen and speak.\\nOthers At present, most of the research on intelligent agents focuses on the above-mentioned common sensory input\\ntypes. However, just as humans have more than 20 types of perception, intelligent agents have also made progress\\nin achieving corresponding perception capabilities through other sensors. The bionic olfactory chip developed by\\nHong Kong University of Science and Technology [ 551] integrates a nanotube sensor array on a nanoporous substrate,\\nwith up to 10,000 independently addressable gas sensors on each chip, which is similar to the configuration of the\\nolfactory system of humans and other animals, and can accurately distinguish between mixed gases and 24 different\\nodors. In terms of taste, Tongji University [ 560] combines fluorescence and phosphorescence signals to develop an\\nintelligent taste sensor with multi-mode light response, which can effectively identify umami, sourness and bitterness.\\nIn order to achieve human-like perception and grasping capabilities, New York University [ 561] launched a low-cost\\nmagnetic tactile sensor AnySkin, which can be quickly assembled and replaced. Even in the perception of pain,\\nthe Chinese Academy of Sciences uses the unique electrical properties of liquid metal particle films when they are\\n“injured” (mechanically scratched) to imitate the perception and positioning of “wound.” Some other works, including\\nHuggingGPT [ 152], LLaV A-Plus [ 500], and ViperGPT [ 498], integrate these single-modal perception capabilities\\nwithin the framework, select and apply them according to task requirements, and achieve the goal of achieving more\\ncomplex tasks.\\n7.2.2 Cross-modal Models\\nText-Image Cross-modal models integrating text and images have witnessed significant advancements in recent\\nyears, leading to improved alignment, retrieval, and generation between the two modalities. These models can be\\ncategorized based on their primary objectives, including cross-modal alignment and retrieval, text-to-image generation,\\nand image-to-text generation.\\nOne of the primary focuses in cross-modal research is the alignment and retrieval of text and images. CLIP [ 51],\\nintroduced by OpenAI in 2021, employs contrastive learning to align textual and visual representations, enabling\\nzero-shot cross-modal retrieval and classification. Similarly, ALIGN [ 501], developed by Google in the same year,\\nleverages large-scale noisy web data to optimize text-image embedding alignment. In 2022, CyCLIP [ 562] introduced a\\ncyclic consistency loss to further enhance the robustness of cross-modal alignment, improving the reliability of retrieval\\ntasks.\\nAnother major area of progress involves text-to-image generation, where models aim to synthesize high-quality images\\nbased on textual descriptions. OpenAI’s DALL ·E series [ 563,564,502], spanning from 2021 to 2023, has made\\nsubstantial contributions in this domain, with DALL ·E 3 offering fine-grained semantic control over generated images.\\nStable Diffusion [ 565], introduced by Stability AI in 2022, employs a diffusion-based generative approach that supports\\nopen-domain text-to-image synthesis and cross-modal editing.\\nA third significant research direction is image-to-text generation, where models aim to generate high-quality textual\\ndescriptions based on image inputs. Typical representative work is the BLIP [ 566] and BLIP-2 [ 567] models, introduced\\nby Salesforce between 2022 and 2023, which utilize lightweight bridging modules to enhance vision-language model\\nintegration, enabling tasks such as image captioning and question answering.\\n80\\nText-Video The key research here involves video text alignment, generation and retrieval. VideoCLIP [ 504] employs\\na video encoder—typically based on temporal convolution or a transformer structure—to extract sequential features\\nfrom video frames. These features are subsequently aligned with textual representations generated by a language\\nencoder, facilitating robust video-text association. In the domain of text-to-video generation, Meta’s Make-A-Video\\nmodel [ 506] extends spatial-temporal dimensions using diffusion-based techniques, allowing for high-quality video\\nsynthesis from textual descriptions. Additionally, Google’s Phenaki [ 505] addresses the challenge of generating long,\\ntemporally coherent video sequences, demonstrating significant advancements in video synthesis through cross-modal\\nlearning.DeepMind’s Frozen in Time [ 568] adopts contrastive learning for video-text matching, thereby enabling\\nefficient cross-modal retrieval. This approach enhances the capacity to search and retrieve relevant video segments\\nbased on textual queries, further improving the integration of vision and language understanding.\\nText-Audio Cross-modal models connecting text and audio have made significant improvements in related tasks such\\nas modal representation, generation, and conversion, and enhanced the perception ability under a single modality.\\nAudioCLIP [ 509], introduced in 2021, extends the CLIP framework to the audio domain, enabling tri-modal retrieval\\nacross audio, text, and images. By incorporating audio as an additional modality, AudioCLIP utilizes multi-task\\nlearning to unify image, text, and audio representations into a shared embedding space. This advancement enhances the\\ncapability of cross-modal retrieval and interaction. In a similar vein, V ATT [ 508] adopts a unified Transformer-based\\narchitecture to process video, audio, and text through independent encoding branches. These branches are subsequently\\nfused into a shared multimodal space, facilitating tasks such as cross-modal retrieval and multi-task learning. This\\ndesign allows for greater adaptability across diverse multimodal scenarios.\\nFor text-to-audio generation, Meta introduced AudioGen [ 569] in 2023, which enables the synthesis of audio, such\\nas environmental sounds and music fragments, directly from textual descriptions. This model exemplifies the grow-\\ning capabilities of AI in generating high-fidelity audio based on linguistic input, expanding applications in media,\\nentertainment, and accessibility.\\nAdditionally, in the domain of speech-to-text and text-to-speech conversion, Microsoft developed SpeechT5 [ 570]. This\\nmodel unifies speech and text generation, supporting both speech synthesis and recognition within a single framework.\\nBy leveraging a shared architecture for these dual functionalities, SpeechT5 contributes to the seamless integration of\\nspeech and text processing, thereby enhancing applications in automated transcription, voice assistants, and accessibility\\ntools.\\nOthers In some other scenarios and domains, cross-modal modeling also plays an important role.\\nCLIP-Forge [ 510] presents a novel method for generating 3D shapes from textual descriptions. By leveraging the\\ncapabilities of Contrastive Language-Image Pre-training (CLIP), this approach enables the synthesis of high-quality 3D\\nobjects conditioned on natural language inputs, bridging the gap between text and 3D geometry. Point-E [ 511] extends\\nthis concept by generating 3D point clouds from text descriptions. Unlike traditional 3D reconstruction techniques,\\nPoint-E focuses on point cloud representations, facilitating efficient and scalable 3D content creation while maintaining\\nhigh fidelity to textual prompts.\\nIn the field of medical imaging, MoCoCLIP [ 571] introduces an approach that enhances zero-shot learning capabilities.\\nBy integrating CLIP with Momentum Contrast (MoCo), this method improves the generalization of deep learning\\nmodels in medical imaging applications, addressing the challenges associated with limited annotated data and domain\\nadaptation.\\n7.2.3 Multimodal Models\\nThe cross-modal model described above mainly aligns and maps between modalities through contrastive learning and\\nother methods to achieve information complementarity and conversion between modalities. Furthermore, the work\\nof multimodal models focuses on how to integrate the features of multiple data (such as vision, text, audio, etc.) to\\nimprove the performance of the overall model.\\nVision Language Model Vision Language Model(VLM) is broadly defined as multimodal model that can learn from\\nimages(or videos) and text. Humans live in a world full of multimodal information. Visual information (such as images\\nand videos) and language information (such as text) often need to be combined to fully express meaning. The same is\\ntrue for intelligent agents. LLaV A [ 513] first tried to use gpt-4 to generate a multimodal language image instruction\\ndataset. Through end-to-end training, a large multimodal model was obtained and excellent multimodal chat capabilities\\nwere demonstrated. LLaV A-NeXT [ 513] uses dynamic high-resolution and mixed data to show amazing zero-shot\\ncapabilities even in pure English modal data, and the computational/training data cost is 100-1000 times smaller than\\nother methods. Emu2 [ 516] changes the traditional way of using image tokenizer to convert images into discrete tokens,\\nand directly uses image encoders to convert images into continuous embeddings and provide them to Transformer,\\n81\\nenhancing multimodal context learning capabilities. MiniGPT-v2 [ 512] employs unique identifiers for various tasks\\nduring training. These identifiers help the model differentiate task instructions more effectively, enhancing its learning\\nefficiency for each task. Qwen2-VL [ 515], DeepSeek-VL2 [ 572] use dynamic encoding strategies on visual components,\\naiming to process images with different resolutions and generate more efficient and accurate visual representations.\\nAt the same time, DeepSeek-VL2 [ 572] also uses the MoE model with a multi-head potential attention mechanism to\\ncompress the key-value cache into a latent vector to achieve efficient reasoning.\\nPrevious work mainly uses image fusion text for training. Video-ChatGPT [ 573] extends the input to video and\\ndirectly uses a video adaptive visual encoder combined with LLM for training to capture the temporal dynamics and\\ninter-frame consistency relationships in video data, thereby enabling open conversations about video content in a\\ncoherent manner. To solve the lack of unified tokenization for images and videos, Video-LLaV A [ 574] unifies the visual\\nrepresentations of image and video encoding into the language feature space, making the two mutually reinforcing.\\nSimilarly, Chat-UniVi [ 575] employs a set of dynamic visual tokens to integrate images and videos, while utilizing\\nmulti-scale representations to allow the model to grasp both high-level semantic concepts and low-level visual details.\\nYouku-mPLUG [ 576] has made in-depth research in specific scenarios. Based on the high-quality Chinese video-text\\npairs in the Youku video sharing platform, it enhances the ability to understand overall and detailed visual semantics\\nand recognize scene text. Unlike the previous method that requires training, SlowFast-LLaV A [ 577] can effectively\\ncapture the detailed spatial semantics and long-term temporal context in the video through a two-stream SlowFast\\ndesign without any additional fine-tuning of the video data, achieving the same or even better results than the fine-tuning\\nmethod.\\nAs the parameters of large models gradually decrease and the computing power of the end-side increases, high-\\nperformance end-side models are gaining momentum. Smart terminal devices such as mobile phones and PCs have\\nstrong demands for image visual processing, which puts forward higher multimodal recognition effects and reasoning\\nperformance requirements for the deployment of AI models on the end-side. TinyGPT-V [ 517] is built based on the\\nPhi-2 [ 578] small backbone combined with BLIP-2 [ 567], only 8G video memory or CPU is needed for reasoning,\\nand solving the computational efficiency problems of LLaV A [ 513] and MiniGPT-4 [ 579]. MiniCPM-V [ 519] mainly\\nprovides powerful OCR capabilities for long and difficult images, and has a low hallucination rate, providing reliable\\nperception output. Megrez-3B-Omni [ 580] ensures that all structural parameters are highly compatible with mainstream\\nhardware through coordinated optimization of software and hardware. Its inference speed is up to 300% faster than that\\nof models with the same precision, improving its adaptability to different end-side hardware.\\nSimilarly, there are more GUI-related works focusing on automatic task execution on mobile phones and PCs. Omni-\\nParser [ 520] uses popular web page and icon description datasets for fine-tuning, significantly enhancing the detection\\nand functional semantic expression capabilities of icons in screenshots. GUICourse [ 581] and OS-ATLAS [ 582] also\\nbuilt a cross-platform GUI grounding corpus, which brought significant performance improvements in the understanding\\nof GUI screenshots and enriching the interactive knowledge of GUI components.\\nVision Language Action Model Vision-Language-Action (VLA) model, which takes vision and language as inputs\\nand generates robotic actions as outputs, represents an important research direction in the field of embodied intelligence.\\nThe selection of vision and language encoders in VLA models has undergone diverse development, evolving from\\nearly CNNs to Transformer architectures, and further integrating 3D vision and large language models. Early models\\nsuch as CLIPort [ 521] used ResNet [ 488] to process visual inputs and combined language embeddings to generate\\nactions, laying the foundation for multimodal fusion. RT-1 [ 522] introduced the Transformer architecture, employing\\nEfficientNet as the visual encoder and USE as the language encoder, and fused visual and language information via\\nFiLM mechanisms, significantly enhancing the model’s generalization ability. VIMA [ 523] further adopted multimodal\\nprompts, combining the ViT visual encoder and the T5 language model to support more complex tasks. PerAct [ 524]\\ninnovatively used 3D point clouds as visual inputs and processed multi-view information through Perceiver IO,\\nproviding richer spatial perception for robotic manipulation. Diffusion Policy [ 525] combined ResNet visual encoders\\nand Transformer language models, generating actions through diffusion models to improve the diversity and accuracy\\nof action generation. SayCan [ 583] integrated the PaLM language model with visual inputs, using the CLIP visual\\nencoder for task decomposition. PaLM-E [ 526] combined the ViT visual encoder and the PaLM language model,\\nguiding low-level action execution through text planning. MultiPLY [ 527] further integrated 3D information into\\nLLMs, combining the EV A visual encoder and the LLaMA language model to provide more comprehensive planning\\ncapabilities for complex tasks.\\nAudio Language Model Audio Language Model(ALM) uses the audio and text to build multimodal model.\\nSpeechgpt [ 533] built a large-scale cross-modal speech instruction dataset SpeechInstruct and trained discrete speech\\nrepresentations, achieving cross-modal speech dialogue capabilities beyond expectations. LauraGPT [ 584], unlike the\\nprevious sampling of discrete audio tokens to represent input and output audio, proposed a novel data representation\\nthat combines the continuous and discrete features of audio, and demonstrated excellent performance on a wide range of\\n82\\naudio tasks through supervised multi-task learning. [ 529,585,531] converts audio data into embedded representations\\nand then fine-tunes instructions, so that excellent performance can be achieved on various speech processing tasks\\nthrough natural language instructions. In order to reduce the cost of fine-tuning training, Audio Flamingo [ 528]\\nquickly enhances the ability to adapt to unseen tasks through contextual learning and retrieval based on the audio\\nlanguage model. UniAudio 1.5 [ 530] uses words or subwords in the text vocabulary as audio tokens, learns these audio\\nrepresentations through a small number of samples, and achieves cross-modal output without fine-tuning. In order to\\nmake the output more realistic and in line with human expectations, Qwen2-Audio [ 54] introduced the DPO training\\nmethod to achieve human preference alignment.\\nAudio Vision Language Model Audio Vision Language Model (A VLM) ultilizes audio, vision, and text to unify\\nmultimodal models. Previously, we introduced some work on building multimodal models using information from two\\nmodalities. In the pursuit of AGI, the obstacle to achieving this goal lies in the diversity and heterogeneity of tasks and\\nmodalities. A suitable approach is to allow more modal capabilities to be supported within a unified framework. Some\\nclosed-source work [ 586,587] has achieved excellent capabilities across modalities such as text, vision, and audio.\\nImageBind [ 588] implements joint embedding across six different modes (image, text, audio, depth, thermal, and IMU\\ndata). Panda-GPT [ 535] combines ImageBind’s multi-modal encoder and Vicuna [ 589], showing zero-shot cross-modal\\nperformance in addition to images and text. Similar work includes [ 539,539,536], which achieves alignment and\\ntraining through the encoding information of vision, audio and text. Multimodal models often require more resources\\nto train, and UniV AL [ 538] trained a model with only ∼0.25Bparameters based on task balance and multimodal\\ncurriculum learning, and used weight interpolation to merge multimodal models, maintaining generalization under\\nout-of-distribution. NExT-GPT [ 542] connects LLM with multimodal adapters and different diffusion decoders, and\\nonly trains a small number of parameters (1%) of certain projection layers.\\nOther works [ 543,590,544,545] have achieved input-output conversion between arbitrary modalities. Unified-IO\\n2 [543] is the first autoregressive multimodal model that can understand and generate images, text, audio, and actions.\\nIt tokenizes different modal inputs into a shared semantic space and processes them using an encoder-decoder model.\\nAnyGPT [ 590] builds the first large-scale any-to-any multimodal instruction dataset, using discrete representations to\\nuniformly process various modal inputs. Modaverse [ 545] directly aligns the output of the LLM with the input of the\\ngenerative model to solve the problem that previous work relies heavily on the alignment of the latent space of text and\\nnon-text features, avoiding the complexity associated with the alignment of latent features. CoDi-2 [ 544] outperforms\\nearlier domain-specific models in tasks like topic-based image generation, visual transformation, and audio editing.\\nOthers Humans have explored the 2D world more than the 3D world, but 3D can more accurately describe the shape\\nand texture information of objects and provide richer perceptual information. PointLLM [ 540] uses a point cloud\\nencoder to express geometric and appearance features, and integrates language features for two-stage training of\\ncomplex point-text instructions, achieving excellent 3D object description and classification capabilities. Since 3D\\ncontains richer information than 2D, it also brings greater training costs. [ 541,591] reduces the training cost here,\\nand MiniGPT-3D [ 541] uses 2D priors from 2D-LLM to align 3D point clouds with LLMs. Modal alignment is\\nperformed in a cascade manner, and query expert modules are mixed to efficiently and adaptively aggregate features,\\nachieving efficient training with small parameter updates. LLaV A-3D [ 591] connects 2D CLIP patch features with their\\ncorresponding positions in 3D space, integrates 3D Patches into 2D LMM and uses joint 2D and 3D visual language\\ncommand adjustment to achieve a 3.5-fold acceleration in convergence speed.\\nIn order to enable intelligent agents to accurately perceive and manipulate unknown objects, Meta [ 592] developed\\nNeuralFeels technology, which combines vision and touch to continuously model unknown objects in 3D, more\\naccurately estimate the posture and shape of objects in handheld operations, and improve the accuracy of ignorant\\nobject operations by 94%.\\n7.3 Optimizing Perception Systems\\nPerception errors, including inaccuracies, misinterpretations, and “hallucinations” (generation of false information),\\npose substantial challenges to the reliability and effectiveness of LLM-based agents. Optimizing perception thus\\nrequires minimizing these errors using various strategies across model, system, and external levels.\\n7.3.1 Model-Level Enhancements\\nFine-tuning. Fine-tuning pre-trained LLMs on domain-specific data significantly improves their ability to accurately\\nperceive and interpret relevant information. For example, fine-tuning models such as LLaV A on specific landmarks\\nhas been shown to enhance their recognition accuracy, particularly in urban navigation tasks [ 513,593]. Moreover,\\ntechniques such as Low-Rank Adaptation (LoRA) enable more efficient fine-tuning, avoiding a substantial increase in\\n83\\nmodel complexity while still improving performance [ 109,594]. Some LLM work combined with traditional vision is\\nalso widely used. Integrating with YOLOS [ 595] on the basis of the the Llama-Adapter [ 596] architecture significantly\\nimproves the detection and positioning capability.\\nPrompt Engineering. The design of effective prompts is crucial to ensure LLMs generate outputs that are both accurate\\nand aligned with the desired goals. By providing clear instructions, contextual information, and specific formatting\\nrequirements, prompt engineering minimizes misinterpretation and hallucination [ 597]. System prompts define the\\nagent’s role, historical prompts to provide context from past interactions, and customized prompts to ensure output\\nconsistency has been shown to reduce errors significantly [597].\\nRetrieval-Augmented Generation. Supplementing LLMs with external knowledge sources through retrieval mecha-\\nnisms helps ground their responses in factual information, reducing the likelihood of hallucinations and improving the\\naccuracy of perceived information [334].\\n7.3.2 System-Level Optimizations\\nAnticipation-Reevaluation Mechanism. In scenarios where agents face incomplete or ambiguous information, an\\nanticipation-reevaluation mechanism can enhance robustness. For instance, in navigation tasks, agents can anticipate\\ngoal directions based on historical data and reevaluate their inferences when new information becomes available [ 598].\\nMulti-Agent Collaboration. In multi-agent systems, structured communication and collaboration among agents\\ncan facilitate information sharing, error correction, and consensus-building, leading to a more accurate collective\\nperception of the environment [ 599]. Different communication topologies, such as fully connected, centralized, and\\nhierarchical structures, offer varying trade-offs in terms of efficiency and robustness [ 600]. InsightSee [ 601] refines\\nvisual information through a multi-agent framework with description, reasoning, and decision-making, effectively\\nenhancing visual information processing capabilities. Similarly, HEV [ 602] integrates the global perspective information\\nof multiple agents and endows RL agents with global reasoning capabilities through cooperative perception, thereby\\nenhancing their decision-making capabilities.\\nAgent Specialization. Assigning distinct roles and capabilities to individual agents within a multi-agent system allows\\nfor a division of labor in perception, with each agent focusing on specific aspects of the environment or task. This can\\nenhance the overall accuracy and efficiency of perception [603].\\n7.3.3 External Feedback and Control\\nLoss Agents for Optimization. Utilizing LLMs as loss agents, allows for the dynamic adjustment of loss function\\nweights during training [ 604]. This enables the optimization of image processing models based on complex, potentially\\nnon-differentiable objectives, including human feedback and evaluations from specialized models. This approach\\nessentially externalizes the optimization objective, allowing the LLM to “perceive” and adapt to complex criteria [ 605].\\nHuman-in-the-Loop Systems. Incorporating human feedback and oversight can help correct errors, guide the agent’s\\nlearning process, and ensure alignment with human values and expectations [43].\\nContent and Output Mediation. Before presenting LLM outputs to users, content mediation filters and refines these\\noutputs. This helps prevent unexpected or harmful behaviors, ensuring alignment with user expectations and safety\\nguidelines [606].\\n7.4 Perception Applications\\nThe operational efficacy of intelligent agents is predominantly influenced by three critical factors: model architecture\\ndimensionality, hardware infrastructure specifications, and quantization optimization methodologies. The exponential\\nprogression in model parameters—from Bert-Base’s modest 110M to GPT-3’s substantial 175 billion, culminating\\nin Llama 3’s unprecedented 405 billion—has correspondingly escalated processing latency from milliseconds to\\nhundreds of milliseconds. Hardware performance variations are particularly noteworthy; empirical evidence with GPT-3\\ndemonstrates that NVIDIA H100 exhibits a 50% improvement in token processing throughput compared to A100, while\\nRTX 4090 achieves approximately double the processing capability.\\nContemporary intelligent agents have penetrated diverse domains, encompassing personal assistance systems, gaming\\nenvironments, Robotic Process Automation (RPA), and multimedia content generation, predominantly leveraging visual\\nperception as their primary input modality. In the context of procedurally generated environments like Minecraft,\\nSTEVE [ 607] demonstrates remarkable performance improvements, achieving a 1.5x acceleration in technology tree\\nprogression and a 2.5x enhancement in block search efficiency through visual information processing. Steve-Eye [ 608]\\n84\\nadvances this paradigm through end-to-end multimodal training, addressing environmental comprehension latency\\nthrough integrated visual-textual input processing.\\nIn creative content generation, AssistEditor [ 609] exemplifies sophisticated multi-agent collaboration, facilitating\\nprofessional video editing through style-driven content understanding. Similarly, Audio-Agent [ 610] implements\\ncross-modal integration between textual/visual inputs and audio outputs, enabling comprehensive audio manipulation\\ncapabilities [611, 612, 613].\\nMobile and desktop platforms have witnessed significant advancements in agent applications. ExACT [ 614] has\\nestablished new state-of-the-art benchmarks in VisualWebArena [ 615], achieving a 33.7% Success Rate through\\nscreenshot-based exploratory learning with caption and Set of Mask integration. SPA-Bench [ 616] introduces a compre-\\nhensive mobile evaluation framework that authentically replicates real-world complexity. M3A [ 617] demonstrates\\nsuperior performance with a 64.0% success rate in SPA-Bench through multimodal input processing. AgentStore [ 618]\\nhas markedly improved OSWorld PC benchmark performance to 23.85% through enhanced visual and accessibility tree\\nprocessing.\\nV oice interaction capabilities [ 619,586] in personal AI assistants have significantly reduced interaction friction while\\nenhancing operational efficiency. The integration of emotional prosody in voice interactions has demonstrated increased\\nuser engagement and retention.\\nIn embodied intelligence applications, haptic and force feedback mechanisms have emerged as crucial modalities for\\nenvironmental interaction, with enhanced sensory fidelity enabling increasingly precise operational capabilities [620].\\n7.5 Summary and Discussion\\nAlthough more and more research works [ 543,590] focus on building unified multimodal models to support the input\\nand output of multiple perception capabilities. Agent perception, a cornerstone of autonomous systems, faces significant\\nchallenges in effectively interpreting and integrating multi-modal data. Current methodologies encounter persistent\\nissues in representation learning, alignment, and fusion, which hinder the development of robust and generalizable\\nperception systems.\\nOne of the primary issues lies in the representation methods employed, which often fail to capture the intricate\\nnuances of multi-modal data. This shortfall is particularly evident in scenarios where high-dimensional sensory\\ninputs require a sophisticated abstraction that preserves critical semantic information. Furthermore, the alignment of\\nrepresentations presents additional difficulties. Integrating heterogeneous data types into a cohesive feature space is not\\nonly computationally intensive but also prone to inconsistencies, which can lead to misinterpretation of ambiguous\\nsignals. The challenge is compounded when attempting to fuse these diverse representations, as the process of merging\\nfeatures from various sources frequently results in suboptimal integration and potential loss of vital information.\\nFuture research directions should prioritize adaptive representation learning through dynamic neural architectures\\ncapable of automatically adjusting their structure based on environmental context and task demands. This could involve\\nmeta-learned parameterization or graph-based representations that explicitly model relationships between perceptual\\nentities. For cross-modal alignment, self-supervised spacetime synchronization mechanisms leveraging contrastive\\nlearning principles show promise in establishing dense correspondence without requiring exhaustive labeled data. The\\nintegration of causal inference frameworks into alignment processes [ 621] could further enhance robustness against\\nspurious correlations. In representation fusion, hierarchical attention mechanisms with learnable gating functions merit\\ndeeper exploration to enable context-aware integration of complementary modality features. Emerging techniques in\\ndifferentiable memory networks may provide new pathways for maintaining and updating fused representations over\\nextended temporal horizons.\\n85\\nChapter 8\\nAction Systems\\nIn the realm of philosophy, action is defined as the behaviors that agents can perform for a potential or specific purpose\\nin the environment. For example, manipulation, moving, reasoning, and tool utilization can all be considered as\\nfundamental actions that an intelligent agent can execute to fulfill a goal in real-world scenarios. In other words,\\nactions emerge from the goal-oriented engagement of an agent in its environment, reflecting its intent to transform the\\nexternal world in pursuit of its goals. Therefore, the action system also plays a vital role in differentiating AI agents and\\nfoundation models (e.g., LLMs). Generally, existing foundation models have demonstrated impressive performance\\nacross various tasks, but their task scope is still limited as they predominantly relies on the original pre-training objective\\n(e.g., next-token prediction). By serving foundation models as brain intelligence, AI agents equipped with action\\nsystems can directly engage with their environment and execute complex user intent. Moreover, action systems can\\nsupport agents to utilize available tools from external environments, thus significantly extending agents’ task scopes.\\nTherefore, the design of action systems will also determine the capability of AI agents in perception, decision making,\\nexecution, tool utilization, and any other components to align with the human brain. In other words, foundation models\\nlay the groundwork for agents while action systems determine their ultimate potential to achieve complex targets.\\nDesigning an effective and comprehensive action system for AI agents is a critical endeavor that involves significant\\nchallenges and notable benefits. In Figure 8.1, we demonstrate the execution process of the action system in the\\ncognition system. In this section, we will first discuss the human action system in Section 8.1, and then examine the\\ntransition from human action to agentic action in AI agents in Section 8.2. After that, we will systematically summarize\\nthe paradigms of existing action systems in AI agents, including action space, action learning, and tool learning, in\\nSection 8.3. In Section 8.4, we analyze the differences between action and perception, and finally we summarize the\\nconclusion in Section 8.5.\\nCognition SystemActionSystemAction:A directive arising from cognitive reasoning.API: interface for programmatically invoking servicesTool: callable utility for specialized executionproducesexecuted bycallFunctions: atomic unit of implementation.\\nFigure 8.1: Illustration of several concepts related to action and action execution.\\n8.1 The Human Action System\\nAction system in human cognition refers to the processes that allow humans to perceive, plan, and execute goal-directed\\nactions. It is a complex system that enables individuals to interact with a dynamic environment, make decisions,\\nand adapt their behavior based on feedback. Generally, the action system within human cognition could be broadly\\ncategorized as mental action andphysical action :\\n•Mental action can be viewed as a kind of distinct action, which is formulated as a thinking process to drive the\\nfinal intention in the human brain. For example, reasoning, decision making, imagining, and planning can all be\\nconsidered as various types of mental action. In other words, mental actions are equal to a brain signal that drives the\\nphysical actions of humans to fulfill the final objective.\\n86\\n•Physical action refers to any goal-directed bodily movement executed by the human motor system. To some extent,\\nphysical actions are usually expressed as a kind of continuous action. For example, speaking, manipulating, drawing,\\nrunning, and grasping can all be regarded as physical actions. Employing a sequence of physical actions, humans can\\nconduct the interaction and collect feedback from real-world environments.\\nFigure 8.2 illustrates a simple taxonomy of the human action system from the perspective of mental action and physical\\naction. Empowered with both mental and physical actions, the human cognition system can handle diverse complex\\ntasks from real-world scenarios. Drawing inspiration from human cognition, it is also essential for us to revisit how to\\nformulate action systems in AI agents across different tasks, from language to digital and then in physical environments.\\nModel Examples Inputs Objective Definition\\nLarge Language Model (LLM) GPT-4 [7] Language Next-Token PredictionLLM is to generate text based on the provided\\nuser prompts.\\nLarge Multimodal Model (LMM) LLaV A [513] Multi-modal Multi-modal GenerationLMM is to generate multimodal data based on\\nmultimodal inputs.\\nRobotic Foundation Model (RFM) RT-1 [522] Sensory inputs Robotic ControlRFM is to generate robotic control based on\\nthe sensory inputs from dynamic environments.\\nLarge Action Model (LAM) LAM [622]InteractiveExecutable ActionLAM is to generate executable actions based on\\nEnvironment the interactions within the environment.\\nTable 8.1: Definitions between different kinds of foundation models.\\nHuman Actions\\nMental Actions\\nCognitive\\nReasoning\\nPlanning\\nReflection\\nImagination\\nDecision-makingAffective\\nEmotion Regulation\\nMotivation\\nEmpathyMemory & Learning\\nMemory Recall\\nSkill AcquisitionPhysical Actions\\nBody Movements\\nLocomotion\\nGestures\\nPosture AdjustmentObject Use\\nManipulation\\nAssemblyCommunication\\nSpeech & Language\\nWriting & Typing\\nNonverbal (e.g., Sign)\\nFigure 8.2: Illustrative Taxonomy of Human Actions, showing both mental and physical facets.\\n8.2 From Human Action to Agentic Action\\nIn the past long period of time, human action systems [ 623] have significantly motivated us to shape the development\\nof a computer system toward autonomous paradigms. The action mechanism plays a critical role in the human brain\\nin driving goal-directed behavior. In an intelligent human brain [ 624], conscious and unconscious thinking signals\\nare produced, converted into mental signals, which eventually lead to a sequence of action operations. This process\\ncan be mapped as a multi-stage pipeline that involves constructing action spaces, formulating learning mechanisms for\\nimproved decision making, and integrating external states (e.g., tools). Inspired by these principles, we discover that\\nthese designs are essential to formulate the prototype of AI agent.\\nMany existing frameworks incorporate action learning into their design or utilize it as an output. To clarify the\\ndefinition of an action system, we highlight the distinctions among various frameworks, including large language\\nmodels (LLM), large multi-modal models (LMM), robotic foundation models (RFM), and large action models (LAM),\\nas shown in Table 8.1. Specifically, an LLM is to produce language output based on provided prompts, while an\\nLMM is to generate multi-modality artifacts based on the multi-modal inputs. Existing language-based or digital AI\\nagent frameworks are built upon these foundation models (e.g., LLM or LMM) via predefining the scope of action\\nspace and its learning strategies. On the other hand, an RFM is to optimize robotic control based on real-world\\nenvironments (e.g., robotic video). Existing RFMs are pre-trained from web-scale video data and use video prediction\\nto simulate the action of robotic control. The core of RFM is still to use the generative objective to learn knowledge\\nfrom large-scale data, although it has involved some action designs in building physical AI agents. Moreover, some\\nrecent works [ 622] introduce the concept of large action model (LAM), which further highlights the stage to generate\\nthe action strategies, interact with real-world environments and enhance self-learning paradigm. From these definitions,\\nwe notice that, regardless of the foundational models employed, the core of action system is to build the interaction\\nwith the environment and then enable the learning process from the collected action trajectories via pre-defined reward\\n87\\nfunctions. Specifically, the mechanisms underlying these behaviors are also similar to the action system in human\\ncognition, offering valuable insights for designing action systems in AI agent frameworks. For example:\\n•When processing different scenarios, humans usually will pre-define the action space to perform action\\ntrajectories to solve specific tasks. For instance, when playing computer games like Minecraft, we will set our\\naction operations via keyboard or mouse to simulate behaviors like building house, mining gold, and so on.\\nOn the basis of this, we also need to build or create an action space for handling complex tasks in AI Agent\\nframeworks.\\n•Compared to machines, the human cognitive system excels in continuously acquiring new knowledge through\\nreal-world interactions, guided by generating and optimizing the action sequences. Thus, replicating this\\nlearning ability in AI agents is essential to adapt the dynamic environment and build a new skill library.\\n•In addition, with the development of human civilization, learning to use external tools has been recognized as\\none of the most significant milestones in the evolution of human intelligence. By leveraging these external\\ntools, humans can extremely extend the problem-solving capability in different scenarios, from the stone age\\nto the industrial revolution.\\nTo this end, we expect to build the mapping between the action system of human cognition system and the design of AI\\nAgent framework, including how to build action space for AI agent from specific scenarios to general domain, how to\\nbuild action learning within the environment, and how to leverage external states (e.g., tools) to extend the task scope of\\nAI Agent. By developing this a systematic survey, we strive to provide more in-depth insights for the community with a\\nclear understanding of the significance of action systems in AI agent frameworks.\\n8.3 Paradigms of Agentic Action System\\nGenerally, the action system of AI agent frameworks consists of three major components: 1) the action space A, which\\nincludes all types of action that agent can perform in real-world scenarios or downstream tasks, and can vary significantly\\ndepending on different agent settings, ranging from language-based agents to embodied agents; 2) the action learning\\nwithin an dynamic environment that determines the state S, observation Oand the optimization process of agent; 3) the\\ntool space Tthat encompasses the instruments, interfaces, or middle-wares the agent can perform for utilization, which\\nranges from physical devices such as robotic arms to digital interfaces like APIs. Overall, these components collectively\\ndefine the scope and characteristics of the action system for AI agents, shaping their formulation and execution.\\nTo fully explore the possible actions atin practical scenarios, we must formally represent the action space and consider\\nboth individual operations and the underlying hierarchical reasoning processes. This means examining the action space\\nat various levels, from low-level manipulations to high-level operators that orchestrate complex workflows.\\nAccordingly, the AI agent decision making process can be formalized as a trajectory ⟨ot, st, at⟩, where atis selected\\nfrom the action space Ato transform the current state stbased on observation otinto the next state. In some cases,\\nintegrating external tool systems may also be necessary. By executing a sequence of ⟨ot, st, at⟩, the agent is steered\\ntoward achieving its final objectives.\\n8.3.1 Action Space Paradigm\\nAction space Ais an important component, which serves as the basis for building an action system within AI agent\\nframeworks. The composition of the action space determines how AI agents solve complex tasks in different scenarios.\\nIn Figure 8.2, we present an illustrative taxonomy of the action system based on its action space. Generally, we\\nsummarize the action space within existing works as three distinct types, as outlined below.\\nLanguage Language-based AI agents typically operate through language-driven actions in interactive linguistic\\nenvironments, such as reasoning, programming, retrieving information, executing API calls, or interacting with external\\ntools. In our study, we summarize three distinct types of language-based action spaces, including plain text, code\\nprogramming, and communication. Specifically, early language-based AI agents are built with plain text, which\\naim to perform interactive decision-making in verbal environments or text-based games. Here, ReAct [ 70] is a\\nrepresentative language-based AI agent, which synergizes the reasoning and actions of an LLM to solve various\\nproblems. AutoGPT [ 625] analyzes and decomposes user requests into multiple subtasks and uses web search or other\\ntools to tackle each of them. Reflexion [ 48] involves self-refinement and the memory mechanism to enhance action\\nexecution in language tasks. LLM+P [ 163] empowers LLM-based agent with planning capability to aid decision-\\nmaking. However, converting plain text into an executable command usually requires LLMs to first interpret the text\\nand then perform instruction conversion, leading to additional information loss. To this end, some work explores using\\n88\\nAction SystemAction SpaceLanguageTextReAct [ 70], AutoGPT [ 625],\\nReflexion [ 48], LLM+P [ 163]\\nCodeMetaGPT [ 626], ChatDev [ 627],\\nSWE-Agent [ 628], OpenDevin [ 629]\\nChatGenerative Agents [ 50], MetaGPT [ 626],\\nAutoGen [ 630], ChatDev [ 627]\\nDigitalGameMineDojo [ 311], V oyager [ 47],\\nSwarmBrain [ 631], JARVIS-1 [ 228]\\nMultimodalMM-ReAct [ 497], ViperGPT [ 498],\\nVisual-ChatGPT [ 496], HuggingGPT [ 152]\\nWebWebGPT [ 632], WebShop [ 633],\\nWebAgent [ 634], Mind2Web [ 97]\\nGUIMobile-Agent [ 635], AppAgent [ 636],\\nUFO [ 637], OmniParser [ 520]\\nDB & KGUnifiedSKG [ 638], Pangu [ 639], BIRD [ 640],\\nSpider 2.0 [ 641], Middleware [ 642]\\nPhysicalRT-1 [ 522], RT-2 [ 643], RT-X [ 644],\\nGR-2 [ 357],π0[645], Saycan [ 646],\\nV oxPoser [ 647], EmbodiedGPT [ 648],\\nLearningICLPromptCoT [ 46], ReAct [ 70], Auto-CoT [ 137],\\nToT [ 72], GoT [ 75], CoA [ 649]\\nDecomposeLeast-to-Most [ 138], HuggingGPT [ 152],\\nPlan-and-Solve [ 650], ProgPrompt [ 93]\\nRole-playGenerative Agents [ 50], MetaGPT [ 626],\\nChatDev [ 627], SWE-Agent [ 628]\\nRefineReflexion [ 48], Self-refine [ 67],\\nGPTSwarm [ 651]\\nPT & SFT Pre-TrainRT-1 [ 522], RT-2 [ 643], RT-X [ 644],\\nGR-2 [ 357], LAM [ 622]\\nSFTLearnAct [ 652], CogACT [ 653],\\nRT-H [ 654], OpenVLA [ 655],\\nGR-2 [ 357],π0[645], UniAct [ 656]\\nRLRLHF [ 43], DPO [ 111], RLFP [ 657],\\nELLM [ 658], GenSim [ 659], LEA [ 660],\\nMLAQ [ 661], KALM [ 662], When2Ask [ 663],\\nEureka [ 664], ArCHer [ 665], LLaRP [ 666], GPTSwarm [ 651]\\nFigure 8.3: Illustrative Taxonomy of Action system, including action space and learning paradigm.\\ncode as the action space, allowing direct execution of the generated code and self-verification. MetaGPT [ 626] and\\nChatDev [ 627] build the action space via programming language with multi-agent collaboration. SWE-Agent [ 628]\\nconsider different stages of software engineering and thus solve software issues. OpenDevin [ 629] devises an automatic\\nsoftware development platform that integrate code writing, interaction with the command, sandbox for code execution,\\nand collaborations. Moreover, some frameworks are built based on multi-agent communications, and then use chatting\\nto analyze which actions should be employed in the next step. Here, Generative Agents [ 50] directly simulate multiple\\ncharacters in a virtual town, to explore how each agent to conduct next action. MetaGPT [ 626] and ChatDev [ 627]\\n89\\nare both multi-agent frameworks to faciliate the development of software engineering. AutoGen [ 630] is also a\\nrepresentative framework that enable multiple agent collaboration to solve any complex tasks. Generally, language-\\nbased AI agents, empowered by LLMs, perform effectively in linguistic interactions. However, limited to the scope of\\nthe action space, it also poses challenges of how to solve more complex tasks in real-world scenarios. Therefore, we\\nalso need to formulate new research solutions to construct a more sophisticated action space to solve challenging tasks.\\nDigital To expand the capabilities of AI agents beyond language, some works have also developed advanced AI agents\\nthat operate within digital environments, such as web proxies, online shopping platforms, and gaming systems. For\\nexamples, MineDojo [ 311] devises a virtual agent via video-language pre-training and simulates an environment that\\nsupports a multitude of tasks and goals within Minecraft. Moreover, V oyager [ 47] is an embodied AI agent trained\\nto play Minecraft. It simulates multiple executable actions in code form to develop a skill library via interacting\\nwith the Minecraft environment, and thus improve the capability of virtual agents. JARVIS-1 [ 228] is an open-world\\nagent that can handle multi-modal inputs / outputs, generate sophisticated plans, and perform embodied control. It\\nexplores the evolutionary behaviors of the agent when acting in Minecraft. SwarmBrain [ 631] is an embodied agent\\nthat uses LLMs to act strategically and in real time in StarCraft II. Additionally, some research studies investigate\\nhow LLMs can act to process multimodal tasks. MM-ReAct [ 497] and ViperGPT [ 498] apply LLMs to perform the\\nthinking process for multimodal tasks and then select visual experts for task solving. Visual-ChatGPT [ 496] integrates\\nmultiple visual experts and uses LLMs as the controller to solve tasks. HuggingGPT [ 152] directly involves four\\nstages, including task planning, model selection, model execution and response generation, to automatically analyze\\nuser instructions and predict the final answers based on complex multimodal tasks. It is also vital for the agent to\\nkeep up with the latest information available online. Therefore, some AI Agent frameworks (e.g., WebGPT [ 632],\\nWebAgent [ 634]) are designed to interact with search engine to enhance the capability of agent to discover the answers\\nfrom website. WebShop [ 633] is used to explore the potential of AI Agent for online shoping. Mind2Web [ 97] is\\nto build a generalist agent that simulate multiple complex web tasks. As foundation agents advance in processing\\nmultimodal tasks or web tasks, there is a increasing trend to enhance their capability in solving complex computer\\ntasks. Mobile-Agent [ 635] utilizes multimodal models as the cognitive controller to manage and orchestrate mobile\\nfunctionalities. AppAgent [ 636] defines various app usages as action spaces, enabling foundation models to interact\\nwith different apps as a mobile intelligent assistant. UFO [ 637] and OmniParser [ 520] are two advanced GUI agents\\nwhich manipulates UI operations as the action space, enabling AI agent to perform computer-use tasks. Generally,\\nempowered with more advanced skills in digital environment, AI agent can demonstrate better intelligent in solving\\ncomplex tasks, and represent a significant shift from language intelligent to digital intelligent. By expanding the action\\nspace to include web browsing, GUI interaction, mobile applications, and embodied systems, AI agents are evolving\\ninto more autonomous, multimodal, and context-aware systems, bridging the gap between foundation models and\\nhuman cognition systems. In addition, other research explores LLM integration with structured digital environments\\nsuch as relational databases and knowledge graphs (KGs). Pangu [ 639] pioneered the connection between LLMs and\\nlarge-scale KGs, while BIRD [ 640] and Spider 2.0 [ 641] established a foundation for LLMs to operate with enterprise\\ndatabases in real-world settings. NL2SQL-BUGs [ 667] addresses the critical challenge of identifying semantic errors in\\nNL2SQL pipelines [ 365], which enhances the reliability of LLM-driven interactions with relational databases [ 668].\\nSimilarly, frameworks like UnifiedSKG [ 638] and Middleware [ 642] expand LLMs’ action capabilities across both\\ndatabases and KGs.\\nPhysical Building an AI agent to interact with the real physical world can be viewed as the ultimate objective to\\nsimulate a computer program to act as a human cognition system. To achieve this, we require the agent to be capable\\nof processing signals from real-world environments and generating feedback to facilitate continuous improvement.\\nTherefore, it will pose new challenges on how to process the continuous signals collected by sensors and enable\\nfoundation models to make decisions. To fulfill this, RT-family [ 522,643,644] pre-trained vision-language-action\\nmodels to integrate knowledge from web videos into robotic learning, enhancing robotic control and action execution.\\nGR-2 [ 357] is a robotic model that undergoes large-scale pre-training on video clips and language data, followed\\nby fine-tuning on robot trajectories for robotic action prediction. π0[645] pre-trained a robotic model based on\\nrobot platforms, including single-arm robots, dual-arm robots, and mobile manipulators, to build robotic learning\\nin physical systems. SayCan [ 646] bridges the connections between robotic semantics and LLMs, using the robotic\\nmodel to provide perception for LLMs and then using LLMs to make high-level decision-making. V oxPoser [ 647] uses\\nLLMs to understand and decompose 3D Value Maps for Robotic Manipulation. Besides, EmbodiedGPT [ 648] utilizes\\nvision-language models to understand video data and perform decision-driven actions. In physical environments, it is\\nworth noting that we usually need to understand continuous signals and then generate continuous actions for robotic\\ncontrol. Despite the existing foundation models that can effectively process discrete-level actions (e.g., language or\\ncomputer-use), how to process long continuous signals is still challenging. Therefore, eliminating the differences\\nbetween continuous signals and discrete signals in foundation models is still a major problem.\\n90\\nGenerally, action space serves as one of the most critical components in building an effective AI Agent system. An\\neffective action space enhances the capability and efficiency of the AI Agent in processing downstream tasks. Action\\nspace usually ranges from the discrete space (e.g., skill library in Atari games) to the continuous space (e.g., robotic\\nmanipulation). As AI agents become more autonomous and multimodal, designing effective action spaces will be\\ncrucial for advancing general-purpose AI systems capable of real-world interactions.\\n8.3.2 Action Learning Paradigm\\nIn the human cognition system, action learning [ 669] represents the problem-solving process, involving both taking\\nactions and reflecting on feedback. Similarly, action learning for AI agents refers to the iterative process by which\\nan autonomous AI system refines its decision making and behavior through direct interaction with the real world\\nenvironment. Generally, action learning encompasses a cycle of multiple stages, including building action space,\\nchoosing actions, and optimizing action selection based on interaction with the environment (e.g., receiving feedback\\nor rewards and adjusting policy for choosing actions). By iteratively deploying these strategies, AI agents can adapt\\nto the latest information or changing conditions in real time, ultimately enabling more robust, flexible, and efficient\\nproblem-solving capabilities. Therefore, an effective action learning mechanism is crucial for the optimization of\\nagentic action systems. In this part, we mainly focus on three different representative learning paradigms, including\\nin-context learning, supervised training, and reinforcement learning, which are discussed below:\\nIn-context Learning As large language models have demonstrated emergent ability, in-context learning has been\\nconsidered as the most effective method to leverage the existing capabilities of LLM without any modifications.\\nProvided with well-designed prompts to describe actions, AI agents can understand specific actions, perform these\\nactions, reflect on the outcome of the interaction with the environment, and finally achieve goals. Among these\\napproaches, the common method is to use prompting techniques to instruct LLMs to generate agentic action. Here, the\\nmost representative one is Chain-of-Thought (CoT) [ 46] prompting, which applies “ Let us think step by step ” technique\\nto generate a sequence of intermediate reasoning steps, exploring potential solutions systematically. ReAct [ 70] enables\\nLLMs to generate reasoning trails and task-specific actions through interaction within the environment, improving\\nthe reasoning and decision-making capabilities of AI agents. LearnAct [ 652] devises an iterative learning strategy to\\nexpand action space by generating code (i.e., Python) to create and revise new actions. Moreover, some works (e.g.,\\nAuto-CoT [ 137] explores how to automatically generate CoT via LLMs and then enable the autonomous thinking\\nprocess of AI agents. To handle more complex tasks, ToT [ 72] considers the thought process as a tree structure and\\nintroduces the tree search via LLM prompting, while GoT [ 75] applies a graph structure along with the graph search.\\nFor robotic models, CoA [ 649] designed four different prompt settings (e.g., object, grasp, spatial, and movement) to\\nallow robot manipulation with reasoning process. Furthermore, to tackle more complex tasks that require intricate\\nagentic workflows, some frameworks introduce the stage of task decomposition via LLM prompting to break down user\\ninstructions. Least-to-Most [ 138] is a classical prompting technique to convert user instructions into multiple subtasks.\\nHuggingGPT [ 152] is a representative AI agent framework that applies task planning to transform user requirements\\ninto actionable items. Plan-and-Solve [ 650] directly uses LLM to make plans from user instructions and then give\\nanswers based on the generated plans. Progprompt [ 93] applies similar task decomposition to robotic tasks. In addition,\\nusing prompting techniques to formulate the characteristic of AI agent has also been considered as an increasing trend\\nto facilitate the simulation and productivity of AI agent frameworks (e.g., Generative Agents [ 50], MetaGPT [ 626],\\nChatDev [ 627], SWE-Agent [ 628]). Finally, some other frameworks (e.g., Reflexion [ 48] or Self-refine [ 67]) analyze\\nthe external feedbacks of user interaction within the environment and then iteratively refine and polish results via\\nwell-designed reflexion prompts. All of these designs allow us to better understand user instructions, decompose task\\ngoals, and make plans for thinking answers. In-context learning can help us avoid parameter optimization and reduce\\nthe heavy cost of training LLMs. It allows AI agents to perform various actions effectively and adapt to a wide range of\\ndomains. However, challenges still remain if we want to acquire agents of even stronger action learning ability.\\nSupervised Training To further improve the action learning ability of foundation models, increasing research efforts\\nhave focused on training methodologies, including self-supervised pretraining (PT) and supervised fine-tuning (SFT).\\nFor the pre-training paradigm, the most representative works is RT-family [ 522,643,644], which pre-trains robotic\\nTransformer on large-scale web and robotic data, yielding a powerful vision-language-action model. Following this\\npolicy, GR-2 [ 357] is developed through extensive pre-training on a large corpus of web videos to understand the\\ndynamics of the world and post-training on robotic trajectory data to specialize in video generation and action prediction.\\nSimilarly, LAM [ 622] is a large action model pre-trained on trajectories of user interaction with computer usage.\\nHowever, the pre-training paradigm usually incurs massive computation costs. Therefore, many works take the fine-\\ntuning paradigm to enhance the action capability of foundation models. OpenVLA [ 670] is built upon the Llama2 [ 11]\\nlanguage model and incorporates a visual encoder based on DINOv2 [ 671] and SigLIP [ 672]. It is fine-tuned on a\\ndiverse set of real-world robot demonstrations from Open X-Embodiment (OXE) [ 673] and outperforms RT-2-X [ 673]\\n91\\nacross different tasks, all while utilizing 7 ×fewer parameters. Building upon OpenVLA, CogACT [ 653] integrates an\\nadditional diffusion action module and introduces an adaptive action ensemble strategy for inference. It is also fine-tuned\\nusing datasets from OXE and demonstrates a 35% improvement in the SIMPLER [ 674] simulated environment and a\\n55% increment in real robot tasks using the Franka Arm. Besides, some works also explore how to enable robotic model\\nto learn action from plain language in physical world. For examples, RT-H [ 654] introduces a hierarchical architecture\\nto build action space, which first predict language motions and then generate low-level actions. And π0[645] collected\\nmassive diverse datasets from different dexterous robot platforms, and then fine-tune the pre-trained VLMs to learn\\nrobotic actions. UniAct [ 656] learns universal actions that capture generic atomic behaviors across differently shaped\\nrobots by learning their shared structural features. This approach achieves cross-domain data utilization and enables\\ncross-embodiment generalizations by eliminating heterogeneity [ 132]. Overall, using supervised training, including\\npre-training and supervised fine-tuning, can effectively adapt foundation models to perform actions intelligently in\\nreal-world scenarios. Last but not least, it is worth noting that, even with extensive training on a vast corpus, it is still\\nbeneficial to apply in-context learning on top of the trained model for AI agents, in an pursuit for their best performance.\\nReinforcement Learning To facilitate an action learning procedure in addition to in-context learning and supervised\\ntraining, it is also crucial for agents to interact with the environment and eventually optimize their action policy through\\nexperience, feedback, or rewards. Considering this iterative and sequential nature, reinforcement learning (RL) provides\\nus with the systematic methodology we need [ 675,676,677,678]. In RL paradigms, there are several classical and\\nrepresentative algorithms, such as Deep Q-Network (DQN) [ 679] and Proximal Policy Optimization (PPO) [ 680]. The\\nmost representative RL work that applied reinforcement learning to foundation models is InstructGPT [ 43], which\\neffectively aligns LLM outputs with human preferences via RLHF. Since RLHF usually requires additional training to\\nbuild the reward model, some papers (e.g. DPO [ 111]) proposes to directly optimize preference data through contrastive\\nlearning. Existing work [ 89,681] also demonstrate the potential of scaling the RL algorithm for foundation models to\\nproduce long CoT thinking stages with impressive performance. Although RL paradigms have been successfully used\\nto fine-tune LLMs for text generation tasks [ 12,682,43,683], efficiently utilizing the RL algorithm for action learning\\nremains one of the many challenges that require further attempts. Recent advances indicate significant progress in\\napplying RL to action learning with LLMs from various perspectives:\\n•Given the rich world knowledge encapsulated in LLM, we can use LLM to mimic external environments or\\ngenerate imagined trajectories to aid agents in action learning. For instance, RLFP [ 657] utilizes guidance\\nand feedback from the policy, value, and success-reward foundation models to enable agents to explore more\\nefficiently. Similarly, ELLM [ 658] utilizes large-scale background knowledge from LLMs to guide agents\\nin efficient exploration within various environments. GenSim [ 659] automatically generates rich simulation\\nenvironments and expert demonstrations by exploiting the coding abilities of LLM, thereby facilitating the\\ncapability of the agent for free exploration. LEA [ 660] leverages the language understanding capabilities\\nof LLM and adapts LLM as a state transition model and a reward function to improve the performance\\nof offline RL-based recommender systems. MLAQ [ 661] utilizes an LLM-based world model to generate\\nimaginary interactions and then applies Q-learning [ 684] to derive optimal policies from this imaginary\\nmemory. KALM [ 662] fine-tunes LLM to perform bidirectional translations between textual goals and\\nrollouts, allowing agents to extract knowledge from LLM in the form of imaginary rollouts through offline\\nRL. In general, empowered by RL paradigms, we can significantly explore the internal knowledge from\\nLLMs and thus enhance the interactions with external environments. Current works such as Search-R1 [ 685],\\nR1-Searcher [ 686], RAGEN [ 687], and OpenManus-RL [ 688] are exploring utilizing RL methods to fine-tune\\nthe agent models on trajectory data in agentic environments.\\n•Besides, hierarchical RL is also a promising topic that helps foundation model to decompose complex task\\nand then learn optimal policies to solve each task via RL paradigm. For example, When2Ask [ 663] enables\\nagents to request high-level instructions from LLM. The high-level LLM planner provides a plan of options,\\nand the agent learns the low-level policy based on these options. Eureka [ 664] leverages LLM to generate\\nhuman-level reward functions with reflection, allowing agents to efficiently learn complex tasks such as\\nanthropomorphic five-finger manipulation. ArCHer [ 665] adopts a hierarchical RL approach, utilizing an\\noff-policy RL algorithm to learn high-level value functions, which in turn implicitly guide the low-level policy.\\nLLaRP [ 666] leverages LLM to comprehend both textual task goals and visual observations. It employs an\\nadditional action output module to convert the output of the LLM backbone into a distribution over the action\\nspace. Overall, using hierarchical RL can guide AI Agent to explore optimal strategies when analyzing user\\nrequests for reasoning and planning.\\nUsing reinforcement learning, we can integrate foundation models with online learning from interactive environments,\\nincorporating both action policies and world models. This integration enables advanced action systems in AI agents.\\nWithin the reinforcement learning paradigm, agents dynamically adapt and refine their decision-making processes in\\n92\\nTool SystemTypesLanguageToolFormer [ 689], ToolLLM [ 690], Gorilla [ 691],\\nToolkenGPT [ 692], GPT4tools [ 693], AnyTool [ 694]\\nDigitalMM-ReAct [ 497], ViperGPT [ 498], Visual ChatGPT [ 496],\\nHuggingGPT [ 152], Chameleon [ 153], WebGPT [ 632],\\nWebAgent [ 634], Mobile-Agent [ 635], AppAgent [ 636], Middleware [ 642],\\nPhysical RT-2 [ 643], TidyBot [ 695], SayCan [ 646], SayPlan [ 292]\\nScientificHoneyComb [ 696], ChemCrow [ 697],\\nSciToolAgent [ 698], SciAgent [ 699]\\nLearningTool DiscoveryHuggingGPT [ 152], Gorilla [ 691], ToolFormer [ 689],\\nToolLLM [ 690], ToolkenGPT [ 692], ToolChain [ 700]\\nTool CreationPAL [ 701], LATM [ 702], Creator [ 703],\\nMetaGPT [ 626], SWE-Agent [ 628]\\nTool Usage HuggingGPT [ 152], TPTU [ 704], SayCan [ 646]\\nFigure 8.4: Illustrative Taxonomy of Tool Systems in AI Agents, including tool category and learning paradigm.\\nresponse to external feedback, facilitating greater efficiency and effectiveness in action learning and achieving desired\\noutcomes.\\nSummary In general, Empowered by action systems, AI agents have demonstrated significant decision-making\\ncapabilities across various fields. For example, action learning enables AI agents to automate the understanding of\\nGraphical User Interfaces (GUIs) and perform various operations, thereby improving human productivity through\\nautomatic computer usage. Moreover, several studies have shown that AI agents equipped with action systems can\\nachieve remarkable outcomes in robotic manipulation tasks, such as object picking, laundry folding, and table cleaning.\\nThere are also promising research directions in the industry employing action models. For instance, autonomous driving\\n(AD) has attracted considerable attention due to the exceptional performance of VLMs in perception and decision-\\nmaking. By integrating human understanding through foundation models, AD systems can effectively comprehend\\nreal-world surrounding, enabling them to simulate human-level drivers. In summary, action learning endows agents\\nwith the ability to interact with the external world, thereby creating more opportunities for AI applications in real-world\\nscenarios.\\n8.3.3 Tool-Based Action Paradigm\\nTool learning distinguishes human intelligence from that of other animals. Ever since the Stone Age, human use of tools\\nhas boosted efficiency, productivity, and innovation. Similarly, enabling AI agents to operate in digital and physical\\nenvironments by harnessing various tools is a fundamental step toward achieving human-level intelligence.\\nDefinitions In AI, tools are defined as interfaces, instruments, or resources that allow agents to interact with the external\\nworld. Examples include web search [ 632,705,97,634], databases [ 706,707,708,709], coding environments [ 710],\\ndata systems [ 711,712,713], and weather forecasting [ 714]. By translating tool functionality into plain text or API\\nformats, foundation models can expand their problem-solving scope. The evolution of tool systems in AI can be\\nsummarized in stages. Initially, with the advent of large language models [ 2], the focus was on converting tools\\ninto explainable formats (e.g., function calls). Later, advances in multimodal processing shifted interactions from\\nconversational chats to graphical user interfaces (GUIs), and more recent work has explored embodied agents that\\ncontrol hardware (e.g. robotic arms, sensors) to interact with the physical world. To simplify, a tool-based action can be\\nconsidered a form of external action employed for assistance.\\nTool Category Similar to action spaces, tools can also be classified into multiple categories according to their types.\\nIn this part, we mainly summarize three key domains, including language, digital, and physical. In addition, we also\\nexplore the potential of tool learning in emerging areas such as scientific discovery:\\n•Language: To facilitate the use of external tools, we usually denote the tool as a kind of function call\\nfor foundation models, which usually encompasses task descriptions, tool parameters, and corresponding\\n93\\noutputs. This expression allows LLMs to understand when and how to use tools in AI agents. Specifically,\\nToolFormer [ 689] expands the capabilities of language models by integrating external tool spaces, including\\ncalculator, QA systems, search engine, translation, and calendar. ToolLLM [ 690] uses RapidAPI as the action\\nspace and then uses a depth-first search-based decision tree algorithm to determine the most suitable tool\\nfor solving tasks. Gorilla [ 691] is a fine-tuned LLM based on the tool documents and then can be used to\\nwrite API calls. ToolkenGPT [ 692] is to optimize tool embeddings and then enable LLMs to retrieve tools\\nfrom the fine-tuned tool embeddings. GPT4tools [ 693] and AnyTool [ 694] are also building self-instruct\\ndatasets and then fine-tune LLMs on them for tool usage. Generally, due to the impressive capability of LLMs,\\nlanguage-based tool utilization for AI agents has been studied, with its effectiveness validated in abundant\\nworks, ranging from plain text or function calls to code programming.\\n•Digital: With the success of LLMs in processing language information, many researchers are exploring\\nextending the task scope of AI agents from the language to the digital domains (e.g., MultiModal, Web search,\\nGUI, and so on). For example, MM-ReAct [ 497], ViperGPT [ 498], and Visual ChatGPT [ 496] employed LLMs\\nas the controller and then used LLMs to select visual experts for solving different tasks. HuggingGPT [ 152]\\nand Chameleon [ 153] use LLMs to first conduct reasoning and planning actions and then analyze which\\nmultimodal tools should be used for solving user instructions. WebGPT [ 632] and WebAgent [ 634] respectively\\nempowered LLMs with search engines to enhance the capability of LLMs to solve more challenging tasks.\\nMobile-Agent [ 635] and AppAgent [ 636] respectively incorporate GUI manipulations and App usage as\\nthe tool-based actions to extend the task scope of AI agents in solving mobile phone tasks. In contrast to\\nthe physical world, digital environments usually provide simpler pipelines to collect and process data. By\\ninvolving foundation models and their interaction with the digital environment, it is possible for us to develop\\nintelligent assistants in computers, mobile phones, and other digital devices.\\n•Physical: For physical world applications, RT-2 [ 643] demonstrates language-guided robotic manipulation\\nusing visual-language tools, and TidyBot [ 695] shows how LLMs adapt cleaning tools to personalized\\nhousehold preferences. SayCan [ 646] uses LLMs as the cognitive system to guide robots in solving tasks\\nthrough robotic arms and visual perception. SayPlan [ 292] built a 3D scene graph as the action spaces and\\ndesigned multiple actions and tools for 3D simulation, and then used LLMs as planners to invoke these actions\\nor tools for robot task planning. Besides, specialized applications in real-world scenarios now also proliferate\\nacross different domains. For instance, in surgical robotics, [ 715] presents a multi-modal LLM framework for\\nrobot-assisted blood suction that couples high-level task reasoning, enabling autonomous surgical sub-tasks.\\nSome autonomous driving systems [ 716,717] also integrate vision–language models with vehicle control\\ntools for explainable navigation. In total, physical world applications pose the most significant challenge\\nwhen compared to other tasks, but they also offer the biggest industrial value. Therefore, it still requires us to\\ncontinue exploring advanced action learning and tool integration in physical-based agents in the future.\\n•Scientific: Scientific tools have played a transformative role in advancing AI agents across disciplines, enabling\\nthem to learn, adapt, and execute tasks while integrating foundational models with frameworks that drive\\ninnovation and address complex challenges. In materials science, HoneyComb [ 696] exemplifies tool-driven\\nadvancements with its ToolHub. General Tools provide dynamic access to real-time information and the latest\\npublications, effectively bridging gaps in static knowledge bases. Material Science Tools are designed for\\ncomputationally intensive tasks, leveraging a Python REPL environment to dynamically generate and execute\\ncode for precise numerical analysis. Similarly, ChemCrow [ 697] demonstrates the transformative power of\\ntools in chemistry by integrating GPT-4 with 18 expert-designed tools to automate complex tasks such as\\norganic synthesis, drug discovery, and materials design. These tools include OPSIN for IUPAC-to-structure\\nconversion, calculators for precise numerical computations, and other specialized chemistry software that\\nenables accurate reaction predictions and molecular property evaluations. Similarly, SciToolAgent [ 698]\\nshowcases how multi-tool integration can revolutionize scientific research. Designed to address the limitations\\nof existing systems, SciToolAgent integrates over 500 tools (e.g., Web API, ML models, function calls,\\ndatabases, and so on). Finally, SciAgent [ 699] exemplifies a multi-agent framework that integrates ontological\\nknowledge graphs with specialized agents for hypothesis generation and critical analysis, emphasizing the\\npower of modular, tool-driven systems to accelerate discovery in materials science and beyond. These\\nexamples underscore the transformative potential of integrating specialized tools into AI frameworks to address\\ndomain-specific challenges effectively.\\nTool learning Inspired by human evolution [ 718], the integration of tools in AI involves three key aspects: Tool\\nDiscovery (identifying suitable tools), Tool Creation (developing new tools) and Tool Usage (effectively employing\\ntools). We also systematically review existing literature and summarize them in the following:\\n94\\n1.Tool Discovery: In real-world environments, there is a wide range of tools from the digital to the physical\\nworld. Finding the most appropriate tools for user instructions can be challenging. Therefore, the process of\\ntool discovery is to identify and select the appropriate tools that AI agents can operate on to achieve their\\nobjectives. This stage also requires the world models in AI agents to have a profound understanding of any\\ncomplex user instructions and world knowledge of different tools. Moreover, the versatility of AI agents is also\\ncorrelated with its ability to operate diverse tool systems. Generally, tool discovery can be categorized into two\\nmainstream paradigms: retrieval-based and generative-based methods. Retrieval-based methods aim to select\\nthe most relevant tools from the tool library. For example, HuggingGPT [ 152] introduces a framework in\\nwhich LLMs act as controllers, orchestrating task planning and then invoking suitable models from platforms\\nsuch as Hugging Face to fulfill user intention. In generative-based approaches, we often fine-tune LLMs to\\nlearn how to use and select tools based on various user instructions. For instance, ToolFormer [ 689] collects\\na massive corpus with the corresponding API calls (e.g., calculator, QA system, search engines, translation,\\nand calendar) for training. ToolLLM [ 690] collect tool instructions based on solution paths and then fine-tune\\nLlama models to generate better API calls for tool utilization.\\n2.Tool Creation In addition to using existing tools, the ability to create new tools plays a crucial role in human\\ncivilization. For language agents, a widely adopted approach is to use LLMs to generate functions as executable\\nprograms, which consist of both the code and documentation. For example, PAL [ 701] generates programs as\\nintermediate reasoning steps to solve problems, LATM [ 702] or Creator [ 703] use LLMs to create code for\\nuser intentions, and to further design a verifier to validate the created tools. SciAgent [ 699] not only integrates\\nmultiple scientific tools but also crafts new tools for scientific discovery. More details on tool creation from an\\noptimization perspective can be found in Section 9.4.2.\\n3.Tool Usage After collecting or creating tools, the effective use of tools constitutes the cornerstone of the\\ncapabilities of AI agents, allowing applications that bridge virtual and physical worlds. Modern AI agents\\nincreasingly employ tools to tackle complex tasks across diverse domains, with three key dimensions of\\nexpansion: 1) Vertical Specialization : Agents leverage domain-specific tools to achieve professional-grade\\nperformance in complex fields such as robotics, science, and healthcare; 2) Horizontal Integration : Systems\\ncombine multiple toolkits across modalities (vision, language, control) for multimodal problem-solving; 3)\\nEmbodiment : Agents physically interact with environments through robotic tools and sensors.\\nSummary Tool learning and action learning constitute the two most important components of the action system in AI\\nagents. Tool learning can be considered as a kind of action to use external states for problem-solving. Tool learning\\nenables AI agents to substantially broaden their range of tasks, pushing the boundaries beyond the scope of foundation\\nmodels. For example, empowered by API or function calls, language models can directly reuse the capability of existing\\nmodels (e.g., retrieval, coding, web search) to generate answers, rather than next-token prediction [ 719]. Tool learning\\nalso involves multiple challenging stages, including how to determine the tool space, how to discover and select tools,\\nand how to create and use tools. Overall, tool learning plays a pivotal role in building an omnipotent AI agent framework\\nto solve complex tasks in different domains.\\n8.4 Action and Perception: “Outside-In” or “Inside-out”\\nA central debate in cognitive science and neuroscience concerns whether action or perception stands at the root\\nof causal flow in intelligent systems. Figure 8.5 presents different perspectives. The traditional “outside-in” view\\ninsists that causal influence begins with external stimuli. The environment excites peripheral receptors, these signals\\npropagate inward, and eventually produce behavior. This perspective portrays the organism—or agent—as essentially\\nreactive: the external world causes sensory changes, and the agent’s actions represent a downstream effect of those\\nchanges. In contrast, Buzsáki’s “inside-out” framework [ 18] proposes that it is the agent’s own actions that shape\\nthe meaning and consequences of incoming signals. Such a view implies an active agent, one which continuously\\ngenerates predictions and motor commands, while sending “corollary discharg” or “action copies” to sensory areas.\\nThese internally generated signals serve as references that inform the agent which sensory changes are self-initiated\\nrather than imposed by the outside world. In this manner, cause shifts from an external event to an internally launched\\ninitiative, leaving external stimuli to play a confirmatory or corrective role. This reversal has significant implications for\\nhow we interpret perception’s purpose and function: it is not an end in itself, but a means of updating and refining the\\nagent’s own action-driven hypotheses about the environment.\\nFrom an evolutionary perspective, possessing the ability to move without relying on sophisticated sensory analysis\\ncan yield immediate survival benefits. Even simple organisms profit from periodic motion that stirs up food in\\nnutrient-rich water, long before elaborate perceptual capacities evolve. In other words, movement precedes advanced\\nsensing in evolutionary time, suggesting that the capacity to act is not merely the effect of external stimuli but can\\n95\\nBrain from Outside-In\\nBrain from Inside-OutBrainBehaviorEnvironment\\nBrainBehaviorEnvironmentAttention& PredictionsFeedbackActionActionStimuliSensoryMotor\\n<latexit sha1_base64=\"4WuPIgfq1HvT7viSSAz8fnSW0X0=\">AAAB8nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cK9gPSUDbbTbt0kw27E6GU/gwvHhTx6q/x5r9x0+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5jUPJO+H4Lvc7T1wboZJHnKQ8iOkwEZFgFK3k90IxVChibir9as2tu3OQVeIVpAYFmv3qV2+gWBbzBJmkxviem2IwpRoFk3xW6WWGp5SN6ZD7libUbgmm85Nn5MwqAxIpbStBMld/T0xpbMwkDm1nTHFklr1c/M/zM4xugqlI0gx5whaLokwSVCT/nwyE5gzlxBLKtLC3EjaimjK0KeUheMsvr5L2Rd27ql8+XNYat0UcZTiBUzgHD66hAffQhBYwUPAMr/DmoPPivDsfi9aSU8wcwx84nz8JTpEa</latexit>/circlemultiplydisplay\\nAﬀerentCorollaryCorollaryReferent signalExferent signalAﬀerent signalEﬀerent signalMotor command\\n+_(a) Compare the brain from “Outside-In” and “Inside-Out” (b) Schematics of the corollary discharge mechanismFigure 8.5: (a) Compare the brain from “outside-in” and “inside-out”. (b) Illustration of the schematic of the corollary\\ndischarge mechanism. A motor command (efferent signal) travels from motor areas to the eye muscles, while a corollary\\ndischarge (dashed arrow) is routed to a comparator in the sensory system. The comparator uses this internal signal to\\nmodulate or subtract external (exafferent) input. Additionally, tension feedback from the muscles (reafferent signal)\\nexerts a delayed effect on perception. Direct projections from motor to sensory cortices underlie this architecture in all\\nmammals. Part (b) is adapted from the original figure in [18].\\nitself be the driving cause of subsequent perceptual development. It is precisely when action mechanisms become\\nsufficiently established that the agent benefits from additional sensors, which guide those movements more strategically.\\nThis developmental sequence grounds perception in utility, tying sensory discrimination to the practical outcomes of\\nmovement.\\nDisruptions in the normal interplay of action and perception illuminate the intricate cause-effect loop. During sleep\\nparalysis, the brain’s motor commands temporarily fail to reach the muscles; external stimuli still bombard the senses,\\nbut the usual action-to-perception calibration is lost. As a result, the individual experiences a heightened sense\\nof unreality because the brain lacks internally generated reference signals to interpret sensory input. Similarly, if\\none externally manipulates the eye without the brain issuing a motor command, the visual scene appears to move,\\nhighlighting how perception alone—devoid of a preceding, self-initiated action—risks confusion. Neurophysiological\\ndata further support the inside-out model. Many neurons in areas once deemed “purely sensory” track not only changes\\nin external stimuli but also self-generated movements—sometimes more strongly so. This indicates that “cause” in\\nthe brain frequently emerges from within, guiding both the magnitude and meaning of external signals. Without these\\ninternal correlates, raw sensory data can become ambiguous or even useless to the system.\\nImplications for Intelligent Agents The inside-out perspective offers potent insights for modern research on intelligent\\nagents. Most contemporary AI systems—and many LLM agents—still function predominantly in a reactive mode,\\nawaiting user input and generating responses based on statistical correlations learned from vast datasets. Such passivity\\nresembles an “outside-in” framework, where the agent’s role is limited to responding, not initiating. Yet if an agent were\\nto be active, continuously forming and testing hypotheses via self-initiated behaviors (physical or representational),\\nit might ground its own “perceptual” inputs—be they sensory streams or linguistic prompts—and thereby reduce\\nambiguity. For instance, an LLM-based agent that interjects questions or verifies its own statements against a knowledge\\nbase could better discern which inferences are self-caused from those demanded by external data. By tracking these\\nself-initiated contributions (analogous to corollary discharge), the model could improve coherence, lessen errors known\\nas “hallucinations”, and refine its internal state through iterative cause-effect loops.\\nA proactive stance also encourages more data-efficient and context-aware learning. Instead of passively waiting for\\nlabeled examples, an agent can explore, provoke feedback, and incorporate self-generated experiences into its training.\\nOver time, this tight coupling between action and perception may bolster the agent’s ability to handle complex tasks,\\nadapt to unanticipated challenges, and generalize more robustly. The shift from an outside-in to an inside-out model\\nreframes perception as causally downstream of action. Intelligent systems—whether biological or artificial—stand\\n96\\nTable 8.2: Comparing the perception and action of human and AI agents.\\nDimension Human Brain / Cognition LLM Agent Remarks\\nPerception - Integrates multiple sen-\\nsory channels (vision, hear-\\ning, smell, touch, taste).\\n- Perception closely tied to\\nemotions, endocrine system,\\nand physical state.\\n- Highly sensitive, capable of\\ndetecting subtle differences.- Primarily language-based\\nwith some multimodal capa-\\nbilities.\\n- Perception depends on ex-\\nternal sensors and models\\nwith limited integration.\\n- Lacks real-time coupling\\nwith physical states.Perception differences lead\\nto varying ways of under-\\nstanding reality. Embodied\\nAI attempts to bridge this gap\\nbut still faces both hardware\\nand software challenges.\\nUnified Representa-\\ntion- Simultaneously processes\\nmultimodal inputs: vision,\\nhearing, language, motion,\\nand emotions.\\n- Different brain regions col-\\nlaborate to create unified spa-\\ntiotemporal and semantic un-\\nderstanding.- Primarily text-based. Some\\nmultimodal models can pro-\\ncess images or audio but with\\nlow integration.\\n- No fully unified spatiotem-\\nporal modeling like the hu-\\nman brain.Even advanced multimodal\\nmodels lack the human\\nbrain’s holistic, unified\\nrepresentation capacity.\\nHardware and algorithmic\\nchallenges remain.\\nGranularity in Task\\nSwitching- Flexible in shifting between\\nmacro and micro cognitive\\ntasks.\\n- Can plan at a high level\\nand shift focus to finer details\\nwhen needed.\\n- Adjusts task priority and\\nfocus dynamically based on\\ncontext and working mem-\\nory.- Relies heavily on prompt\\nengineering for granularity\\ncontrol.\\n- Cannot autonomously real-\\nlocate attention between task\\nlayers.\\n- May get stuck in a spe-\\ncific level of abstraction in\\nabsence of guided prompts.Humans can dynamically\\nadjust cognitive granular-\\nity based on situational de-\\nmands, while LLMs require\\nexplicit instruction to switch\\ntask focus effectively.\\nAction - Goal-oriented process\\ndrives multiple sensory to\\nmake decisions.\\n- Real-time Learning from\\nthe experience via the\\nenvironmental interaction.\\n- Encompass both physical\\nactivities and mental pro-\\ncesses.- Action space need to be de-\\nfined in advance.\\n- Unable to support actions in\\ncontinuous spaces.\\n- Relies on online training\\nto optimize the decision-\\nmaking process in the envi-\\nronment.Humans are capable of\\nactively learning new actions\\nand performing continuous\\nactions, whereas LLM\\nagents currently lack this\\ncapability.\\nto benefit from recognizing that purposeful movement, or proactive conversational steps in the case of LLMs, can\\nactively create, shape, and interpret the signals that flow back in. By acknowledging the cause-effect power of action\\nand striving to build active rather than merely reactive agents, we may approach a deeper understanding of both natural\\ncognition and the next generation of AI.\\n8.5 Summary and Discussion\\nTraditionally, action represents the behaviors of the human cognition system based on the interactive feedback from the\\nenvironment. It endows humans with the capability to think, reason, speak, run, and perform any complex manipulations.\\nBased on the action system, humans can iteratively evolve the brain intelligence by enhancing their perception and\\nactions from the world, and form a closed loop to further create new civilization and innovation in the world. Similarly to\\na human cognition system, the action system plus the tool system also play an important role for AI agents. Integrating\\naction systems allows AI agents to systematically plan, execute, and adjust their behaviors, facilitating more adaptable\\nand robust performance in dynamic contexts. In this section, we systematically examine and summarize the impact of\\nthe action module on AI agents, focusing on both action systems and tool systems.\\n97\\nAction System In our studies, we briefly describe the action system from three perspectives: action space, action\\nlearning, and tool learning. In an action system, action space usually serves as the most important component, which\\ndetermines the upper bound of AI agents in solving downstream tasks. It formulates which actions can be selected\\nand performed by AI agents during interactions with real-world environments. For action space, there are also various\\ndifficulties depending on data types, ranging from discrete to continuous data. With the growing demand for AI agents,\\nthere is also a rising expectation for AI agents to handle more sophisticated tasks, particularly those involving real-world\\napplications. Therefore, how to build robust and general action space is still an ongoing challenge in action systems.\\nOn the basis of action space, action learning is another crucial component in enabling agents to interact effectively\\nwith the external world and with humans. Action learning represents the process of an AI agent to learn and optimize\\nits policy during interaction with real-world environments. Based on different foundation models, it also derives\\ndifferent action learning paradigms, from zero-shot learning (e.g., prompt engineering) to supervised training and\\nreinforcement learning. In action learning, it is essential to thoroughly understand the task, including how to devise\\nsystem prompts, how to determine the pre-trained or fine-tuned datasets, and the reward signals or optimization polices\\nduring the training. Despite notable progress in action learning to advance AI agent frameworks, numerous questions\\nremain to be addressed. Specifically, the ICL paradigm requires specific prior knowledge for a proper prompt design.\\nAdditionally, combining pre-training and post-training for supervised training necessitates high-quality and diverse\\ndata, which often requires meticulous data processing and significant human effort. Furthermore, the unstable nature of\\nreinforcement learning poses difficulties in its application in large-scale training scenarios. Moreover, the design of\\naction systems plays a crucial role in maximizing the benefits of tool integration. By incorporating an effective action\\nsystem, AI agents can seamlessly engage with various tools, execute complex user intents, and transform external\\ndata into meaningful outcomes. This synergy between action systems and tools not only mitigates the limitations\\nof memorization and reduces the risk of hallucinations [ 714] but also enhances the expertise and robustness of the\\nsystem. For instance, an AI agent equipped with a robust action system can dynamically select and employ the\\nmost appropriate tools for a given task, ensuring both accuracy and efficiency in its responses. Furthermore, action\\nsystems facilitate hierarchical reasoning processes, enabling agents to orchestrate intricate workflows that align closely\\nwith user objectives. This alignment is essential for tasks requiring precise execution and real-time decision-making,\\nthereby bridging the gap between foundational model capabilities and practical application demands. Additionally, the\\ntransparency and interpretability provided by tool execution processes enhance user trust and facilitate effective human-\\nmachine collaboration. Consequently, the combination of specialized tools and robust action systems significantly\\nelevates the performance, reliability, and applicability of AI agents in diverse and dynamic environments.\\nIn summary, action systems can significantly establish the foundation for the problem-solving capabilities of AI agent\\nframeworks, enabling them to tackle a broader range of complex tasks beyond foundation models.\\nFuture Directions Nonetheless, building an effective action system for agents requires solutions to a number of\\nchallenges, as we summarize in the following:\\n1.Efficiency presents a significant hurdle, particularly in real-time applications where swift and precise responses\\nare critical. The complexity involved in action system can lead to unacceptable latency, hindering the practical\\ndeployment of AI systems in scenarios like fraud detection or real-time decision-making. To mitigate these\\nefficiency issues, strategies such as filtering out irrelevant or redundant information, employing zero-shot\\nprompting to streamline reasoning processes, and utilizing high-speed storage solutions for caching pertinent\\nknowledge are imperative. These approaches help in maintaining high performance while reducing response\\ntimes.\\n2.Evaluation is also a important factor in action system, including action learning and tool learning. In the\\nreal-world environment, there exists massive actions from different sources. Therefore, how to determine the\\ncorrect action or tools from disparate sources to avoid conflicting information is still a significant challenge\\nin AI Agent. To alleviate these problems, how to build an effective and robust evaluation system to measure\\naction system is essential to maintain the accuracy and reliability of responses. Developing robust evaluation\\nsystem, verification protocols and creating transparent methods are crucial to reduce incorrectness in action\\nprediction. Besides, exposing the decision-making processes of foundation models also help us understand\\nwhich action is better and how to coordinate with various actions or tools to provide trustworthy outputs.\\n3.Multi-modality Action learning has achieve remarkable progresses in LLM-based autonomous agent, due to\\nthe success of large language models. However, how to understand and invoke action beyond the language\\ninstructions (e.g., GUI operations or embodied tools) still remain challenges. In real-world scenarios, humans\\ncan develop or learn to use new skills through any kinds of instructions (e.g., language, image, videos or\\nhuman guidance). Therefore, enabling AI agents to develop or learn actions through diverse modalities is\\ncrucial to advance the capability of AI Agent in solving practical tasks from the real-world scenarios. In other\\n98\\nwords, it is necessary for us to explore how to reduce the gap between human and AI agents in tool utilization,\\nfacilitating the design of advanced agent frameworks for the future.\\n4.Privacy is a critical concern in the field of generative AI, especially using LLMs. As a consequence,\\nmaintaining the privacy of sensitive user data and preventing the disclosure of user behaviors are essential in\\ntool utilization [ 720]. To address these privacy concerns, some safe techniques like federated learning can\\nbe used to enable models to be trained on decentralized data sources without exposing sensitive information\\ndirectly. Additionally, model distillation is often necessary to ensure models maintain high performance\\nwhile safeguarding data integrity. These methods enable the effective training of models while preserving the\\nconfidentiality of user data.\\n5.Safety Moreover, the ethical implications of human-model collaboration and the safety concerns associated\\nwith models interacting with physical environments necessitate careful consideration. Ensuring that human\\ndignity and agency are preserved when integrating human labor with AI systems is critical. Establishing ethical\\nguidelines, promoting fair working conditions, and fostering interdisciplinary collaboration are necessary to\\naddress these concerns. Additionally, developing robust safety mechanisms to prevent erroneous or malicious\\nactions by AI systems interacting with physical tools or actions is imperative to safeguard against potential\\nrisks.\\nIn addition to the above challenges, there also remain open problems for the action system. For example, how to achieve\\nan optimal balance between the foundation models and external tools, deciding on the appropriate timing to use the\\nformer versus the latter, remains unanswered. Specifically, although tool systems can offer flexibility and extensibility\\nfor foundation models, there is an increasing trend to enhance the intrinsic capability of foundation models. Therefore,\\nbalancing between foundation models and tool systems is essential for developing versatile and efficient AI agents.\\n99\\nPart II\\nSelf-Evolution in Intelligent Agents\\n100\\nAgent Self-EvolutionOptimization\\nSpacesPrompt [721,722,723]\\nWorkflow [724,725,726]\\nTools [689,714,690,727]\\nOptimization\\nAlgorithmsOptimization\\nStrategies[728,729,724,730]\\nMeta Op-\\ntimization[91,731,732]\\nTheoretical\\nPerspectives[733,734,735,736]\\nUtilization\\nScenarioOnline Op-\\ntimization[737,48,738,739]\\nOffline Op-\\ntimization[670,740,741,731]\\nScientific\\nKnowledge\\nDiscoveryHypothesis\\ngeneration\\nand testingSi et al. [ 742] SciAgents [ 743] Gen-\\nesis [ 744] AI Scientist [ 745] Agent\\nLaboratory [ 746] ChemAgent [ 747]\\nChemOS 2.0 [ 748] AI co-scientist [ 749]\\nProtocol\\nplanning\\nand tool\\ninnovationDai et al. [ 750] Strieth-Kalthoff\\net al. [ 751] Virtual Lab [ 752]\\nData\\nanalysis and\\nimplication\\nderivationAlphaGeometry [ 753] TAIS\\n[754] Data Interpreter [ 755]\\nFigure 8.6: Structures of self-evolution in LLM agents.\\nIn the history of machine learning research, manually designed AI systems have gradually been replaced by more\\nefficient, learned solutions [ 756]. For instance, before the advent of deep learning, features were typically handcrafted\\nby experts [ 757,758], but these were eventually superseded by features extracted through neural networks. As neural\\nnetworks have become increasingly complex, various techniques for automated design–such as neural architecture\\nsearch–have emerged, further replacing the need for manually designed network structures [ 759]. Similarly, Agentic\\nsystems initially relied heavily on manual design, with behavior rules and decision-making strategies explicitly crafted\\nby developers. Although full automation of agent self-evolution has not yet been achieved, it is anticipated and deemed\\nnecessary for future progress. A successful precedent for such automation can already be seen in automated machine\\nlearning (AutoML) [ 712,760,761,762,204] which has automated various components of traditional machine learning\\npipelines. In particular, AutoML streamlines the selection and configuration of machine learning algorithm pipelines\\nwhile incorporating advanced techniques for hyperparameter optimization [ 763,764,765,766,767]. Among the most\\nnotable applications of AutoML is NAS [ 768,769], which automates the design of neural network architectures to\\nenhance model performance. Drawing inspiration from this successful transition towards automation in traditional\\nmachine learning, we propose extending similar principles to the domain of agentic AI systems.\\nA key counterintuitive issue in much of current agent research is that, while the ultimate goal of developing or improving\\nagentic AI systems is to automate human efforts, the process of creating these systems remains, for the time being,\\nbeyond the reach of full automation. Therefore, we argue that all manually designed agentic AI systems will eventually\\nbe replaced by learnable and self-evolving systems, which could ultimately place the development and improvement\\nof agentic AI into an autonomous, self-sustaining loop. Enabling self-evolution mechanism in LLM agents has the\\nfollowing benefits:\\n1.Scalability: While LLM-based agents have demonstrated remarkable performance, their improvement still\\nheavily depends on the underlying LLMs. However, upgrading these models is costly, and scaling performance\\nthrough the inclusion of additional real-world data requires extensive retraining on large datasets, which\\n101\\nposes significant resource constraints. Self-evolving agentic systems, in contrast, can optimize agent behavior\\nwithout necessitating modifications to the underlying LLMs, offering a more efficient and scalable solution.\\n2.Reduction in Labor Costs: Manually designing agentic systems is a complex and labor-intensive process\\nthat requires developers to engage deeply with intricate technical details. Traditional methods often involve\\nbuilding these systems from scratch, demanding significant expertise and effort. By contrast, self-evolving\\nagentic systems can automate much of this process, significantly reducing the need for manual intervention\\nand lowering development costs.\\n3.Aligned with Natural Intelligence Development: Just as humans continuously improve themselves through\\nlearning and adaptation, equipping LLM agents with self-improvement capabilities is a necessary step toward\\nthe development of truly autonomous agents. This enables them to refine their performance, adapt to new\\nchallenges, and evolve without direct human intervention.\\nOptimization \\nSpaceOptimizerOptimized\\nAgentic \\nSystems\\nTools\\n Prompts\\n…LLM as Optimizers Traditional Optimizers\\nAgentic \\nSystemsEvaluation\\nFigure 8.7: An illustration of key concepts discussed in this section, including optimization spaces, the optimizer,\\nand the optimizing objective. The optimizer iteratively refines components within the optimization spaces to enhance\\nagentic systems until a satisfactory outcome is achieved, thereby achieving self-improvement in the LLM agent systems.\\nTo achieve the goal of automating human efforts, numerous studies have proposed leveraging LLMs as the driving\\nengine to enable self-evolution in agentic systems. In particular, LLMs provide an efficient alternative to traditional\\noptimization methods, such as gradient-based [ 770] and reinforcement learning-based approaches [ 771]. They extend\\nthe optimization space from numerical values to more diverse domains, with natural language serving as a universal\\nbridge. An LLM is capable of optimizing complex, heterogeneous parameters, such as instructions [ 732] and tool\\nimplementations [ 772], and can operate across a range of LLMs, including both open-source and closed-source models.\\nA notable example of this approach is AFLOW [ 773], which automates the generation and optimization of entire agentic\\nsystem workflows. This system employs Monte Carlo Tree Search to leverage the comprehensive capabilities of LLMs.\\nIn this framework, traditionally handcrafted agentic systems are replaced by algorithmically generated ones, marking a\\nkind of paradigm shift. Additionally, a growing body of research explores similar methodologies, further advancing the\\nfield.\\nThis part is structured as follows: First, we introduce various optimization spaces explored in recent research on agentic\\nsystems, including prompts, tools, and workflows. In the subsequent section, we review optimization algorithms,\\ndiscussing both traditional optimization paradigms and meta-optimization, where the optimization process also affects\\nthe underlying optimization algorithms themselves. We then explore the self-evolution scenarios, categorizing them into\\ntwo types: online optimization and offline optimization. Following this, we discuss the application of large language\\nmodel (LLM) agent self-improvement techniques, particularly in knowledge discovery within the AI-for-science domain.\\nFinally, we discuss the security concerns associated with agent self-evolution technologies.\\n102\\nChapter 9\\nOptimization Spaces and Dimensions for\\nSelf-evolution\\nThe optimization of autonomous agents represents a complex challenge that encompasses multiple levels of abstraction.\\nIn this section, we first establish prompt optimization as the foundational layer, upon which three distinct branches\\nof optimization emerge: agentic workflow optimization, tool optimization, and comprehensively autonomous agent\\noptimization.\\n9.1 Overview of Agent Optimization\\nExisting LLM-based agent optimization can be conceptualized in terms of a two-tiered architecture. At the foundation\\nliesprompt optimization , which focuses on enhancing the basic interaction patterns of Language Model nodes. Building\\nupon this foundation, three parallel branches emerge: i) workflow-level optimization , which focuses on the coordination\\nand interaction patterns between multiple LLM nodes; ii) tool optimization , where agents evolve by developing and\\nimproving tools to adapt to new tasks and leverage past data; and iii) comprehensive autonomous agent optimization ,\\nwhich aims at the holistic enhancement of agent capabilities by considering multiple dimensions.\\nSimilarly to optimization paradigms in AutoML, agent optimization can be categorized as either single-objective or\\nmulti-objective. Contemporary agent optimization primarily centers on three canonical metrics: performance, inference\\ncost, and latency. Performance measures the effectiveness of the agent in completing its assigned tasks, while inference\\ncost quantifies the computational resources required for agent operation. Latency represents the time taken for the\\nagent to respond and complete tasks. These objectives can vary depending on the specific optimization modality. For\\ninstance, in prompt-level optimization, additional constraints such as prompt length may become relevant objectives.\\nThis multi-faceted nature of optimization objectives reflects the complexity of agent systems and the need to balance\\nmultiple competing requirements.\\n9.2 Prompt Optimization\\nPrompt optimization plays the most critical role in LLM-based agent optimization. When optimizing an agent, beyond\\nmodel-level optimizations, task-specific or model-specific prompt optimization directly impacts the agent’s performance,\\nlatency, and cost. Given a task T= (Q, G t), where Qdenotes the input query and Gtrepresents the optional ground\\ntruth, the objective of prompt optimization is to generate a task-specific prompt P∗\\ntthat maximizes performance:\\nP∗= arg max\\nP∈PET∼D[ϕeval(ϕexe(Q, P), T)] (9.1)\\nwherePrepresents the space of possible prompts, ϕexedenotes the execution function, and ϕevalrepresents the evaluation\\nfunction. This optimization is typically implemented through three fundamental functions: ϕopt,ϕexe, and ϕeval. The\\nOptimize function ϕoptrefines existing prompts based on optimization signals, the Execute function ϕexeinvokes the\\ncurrent prompt to obtain output O, and the Evaluation function ϕevalassesses current outputs to generate evaluation\\nsignals Sevaland optimization signals Sopt. The evaluation signals are used to select effective prompts, while the\\noptimization signals assist the Optimize function in performing optimization.\\n103\\n9.2.1 Evaluation Functions\\nAt the core of prompt optimization lies the evaluation function ϕeval, which serves as the cornerstone for deriving\\noptimization signals and guiding the evolutionary trajectory of prompts. This function orchestrates a sophisticated\\ninterplay between evaluation sources, methodologies, and signal generation, establishing a feedback loop that drives\\ncontinuous improvement. The evaluation function ϕevalprocesses evaluation sources as input, and employs various\\nevaluation methods to generate different types of signals, which subsequently guide the optimization process. Here, we\\ndefine the dimensions of sources, methods, and signal types to establish the foundation for prompt optimization.\\nEvaluation Sources Evaluation sources primarily consist of LLM Generated Output Gllmand task-specific Ground\\nTruth Gt. Existing works such as [ 730,774,728,775,732,300] predominantly leverage comparisons between Gllm\\nandGtas evaluation sources. Some approaches [ 776,721,777] utilize only Gllmas the evaluation source. For instance,\\nPROMST [ 721] assesses prompt effectiveness by comparing Gllmagainst human-crafted rules; SPO [ 778] employs\\npairwise comparisons of outputs from different prompts to determine relative effectiveness.\\nEvaluation Methods Evaluation Methods can be broadly categorized into three approaches: benchmark-based\\nevaluation ,LLM-as-a-Judge , and human feedback .Benchmark-based evaluation remains the most prevalent method in\\nprompt optimization [ 730,774,721,732,300]. This approach relies on predefined metrics or rules to provide numerical\\nfeedback as evaluation signals. While it offers an automated evaluation process, its effectiveness ultimately depends on\\nhow well the benchmark design aligns with human preferences.\\nThe introduction of LLM-as-a-Judge represents a significant advancement in automated evaluation and preference\\nalignment. Leveraging LLMs’ inherent alignment with human preferences and carefully designed judging criteria,\\nthis approach [ 589] can assess task completion quality based on task descriptions and prompt outputs Gllm, providing\\nreflective textual gradient feedback. Notable implementations include ProteGi [ 779], TextGrad [ 728], Semantic Search\\n[775] and Revolve [ 780]. Furthermore, LLM-as-a-judge enables comparative evaluation between ground truth Gtand\\noutput Gllmwith specific scoring mechanisms [ 724]. The effectiveness of this method hinges on both the design of\\njudger prompts and the underlying model’s alignment with human preferences. As a specialized extension, Agent-as-a-\\nJudge [781] refines this paradigm by employing dedicated agents for providing process evaluation on complex tasks,\\nwhile maintaining high alignment with human preferences at significantly reduced evaluation costs.\\nHuman feedback represents the highest level of intelligence integration in the evaluation process. As humans remain the\\nultimate arbiters of prompt effectiveness, direct human feedback can rapidly and substantially improve prompt quality.\\nHowever, this approach introduces significant resource overhead. APOHF [ 777] demonstrates that incorporating human\\nfeedback can achieve robust prompt optimization with minimal computational resources, particularly excelling in\\nopen-ended tasks such as user instructions, prompt optimization for text-to-image generative models, and creative\\nwriting. Nevertheless, the requirement for human intervention somewhat contradicts the goal of automated evolution.\\nSignal Types Feedback generated by evaluation methods manifests in three distinct forms, each serving different\\noptimization needs. Numerical feedback [730,774,721,732,300] quantifies performance through scalar metrics,\\ncompatible with rules, ground truth, human assessment, and LLM judgments. While widely applicable, this approach\\nrequires substantial samples for statistical reliability, potentially overlooking instance-specific details that could guide\\noptimization. Textual feedback [728,775,780] provides detailed, instance-specific guidance through analysis and\\nconcrete suggestions. This sophisticated approach requires intelligent participation, either from human experts or\\nadvanced language models, enabling targeted improvements in prompt design through explicit recommendations.\\nHowever, its reliance on sophisticated intelligence sources impacts its scalability. Ranking feedback [778] establishes\\nrelative quality ordering through either comprehensive ranking or pairwise comparisons. This approach uniquely\\ncircumvents the need for absolute quality measures or predefined criteria, requiring only preference judgments. It\\nproves particularly valuable when absolute metrics are difficult to define or when optimization primarily concerns\\nrelative improvements.\\n9.2.2 Optimization Functions\\nThe design of optimization functions is crucial in determining the quality of generated prompts in each iteration of\\nprompt optimization. Through effective signal guidance, prompt self-evolution can achieve faster convergence. Current\\noptimization approaches primarily rely on two types of signals: evaluation signals Sevalthat identify the most effective\\nexisting prompts, and optimization signals Soptthat provide detailed guidance for improvements.\\nOptimize via Evaluation Signals When optimizing with evaluation signals, the process begins by selecting the most\\neffective prompts based on ϕevalassessments. Rather than directly learning from past errors, some methods adopt\\n104\\nheuristic exploration and optimization strategies. SPO [ 778] iteratively refines prompts based on the outputs of current\\nbest-performing ones, leveraging the language model’s inherent ability to align with task requirements. Similarly,\\nEvoprompt [ 723] employs evolutionary algorithms with LLMs serving as evolution operators for heuristic prompt\\ncombination. PromptBreeder [ 732] advances this approach further by comparing score variations between mutated\\nprompts while simultaneously modifying both meta-prompts and prompts through the LLM’s inherent capabilities.\\nOptimize via Optimization Signals While optimization methods based solely on evaluation signals require extensive\\nsearch to find optimal solutions in vast search spaces through trial and error, an alternative approach leverages explicit\\noptimization signals to guide the optimization direction and improve efficiency. Existing methods demonstrate various\\nways to utilize these optimization signals. OPRO [ 730] extracts common patterns from high-performing prompt\\nsolutions to guide subsequent optimization steps. ProTegi [ 779] employs language models to analyze failure cases and\\npredict error causes, using these insights as optimization guidance. TextGrad [ 728] extends this approach further by\\ntransforming prompt reflections into “textual gradients”, applying this guidance across multiple prompts within agentic\\nsystems. Revolve [ 780] further enhances optimization by simulating second-order optimization, extending previous\\nfirst-order feedback mechanisms to model the evolving relationship between consecutive prompts and responses. This\\nallows the system to adjust based on how previous gradients change, avoiding stagnation in suboptimal patterns and\\nenabling more informed, long-term improvements in complex task performance.\\n9.2.3 Evaluation Metrics\\nThe effectiveness of prompt optimization methods can be evaluated across multiple dimensions. Performance met-\\nrics[782,778,730] for Close Tasks serve as the most direct indicators of a prompt’s inherent performance, encompassing\\nmeasures such as pass@1, accuracy, F1 score, and ROUGE-L. These metrics enable researchers to assess the stability,\\neffectiveness, and convergence rate of prompt optimization processes. Another crucial dimension is Efficiency met-\\nrics[778]. While some prompt optimization approaches achieve outstanding results, they often demand substantial\\ncomputational resources, larger sample sizes, and extensive datasets. In contrast, other methods achieve moderate results\\nwith lower resource requirements, highlighting the trade-offs between performance and efficiency in agent evolution.\\nThe third dimension focuses on qualitative metrics that assess specific aspects of agent behavior: consistency [ 776]\\nmeasures output stability across multiple runs, fairness [ 783] evaluates the ability to mitigate the language model’s\\ninherent biases, and confidence [ 784,785] quantifies the agent’s certainty in its predictions. When these behavioral\\naspects are treated as distinct objectives, prompt optimization frameworks provide corresponding metrics for evaluation.\\n9.3 Workflow Optimization\\nWhile prompt-level optimization has shown promising results in enhancing individual LLM capabilities, modern AI\\nsystems often require the coordination of multiple LLM components to tackle complex tasks. This necessitates a\\nmore comprehensive optimization domain—the agentic workflow space. At its core, an agentic workflow consists of\\nLLM-invoking nodes, where each node represents a specialized LLM component designed for specific sub-tasks within\\nthe larger system.\\nAlthough this architecture bears similarities to multi-agent systems, it is important to distinguish agentic workflows\\nfrom fully autonomous multi-agent scenarios. In agentic workflows, nodes operate under predetermined protocols and\\noptimization objectives, rather than exhibiting autonomous decision-making capabilities. Many prominent systems,\\nsuch as MetaGPT [ 626] AlphaCodium [ 786] can be categorized under this framework. Moreover, agentic workflows\\ncan serve as executable components within larger autonomous agent systems, making their optimization crucial for\\nadvancing both specialized task completion and general agent capabilities.\\nFollowing the formalization proposed by GPTSwarm [ 651] and AFLOW [ 773], this section first establishes a formal\\ndefinition of agentic workflows and their optimization objectives. We then examine the core components of agen-\\ntic workflows—nodes and edges—analyzing their respective search spaces and discussing existing representation\\napproaches in the literature.\\n9.3.1 Workflow Formulation\\nAn agentic workflow Kcan be formally represented as:\\nK={(N, E)|N∈ N, E∈ E} (9.2)\\n105\\nwhere N={N(M, τ, P, F )|M∈ M, τ∈[0,1], P∈ P, F∈ F} represents the set of LLM-invoking nodes, with M,\\nτ,P, andFdenoting the available language models, temperature parameter, prompt space, and output format space\\nrespectively. Eindicates the edges between different LLM-invoking nodes. This formulation encapsulates both the\\nstructural components and operational parameters that define an agentic workflow’s behavior.\\nGiven a task Tand evaluation metrics L, the goal of workflow optimization is to discover the optimal workflow K∗\\nthat maximizes performance:\\nK∗= arg max\\nK∈KL(K, T) (9.3)\\nwhere Kis the search space of workflow, and L(K, T)typically measures multiple aspects including task completion\\nquality, computational efficiency, and execution latency. This optimization objective reflects the practical challenges in\\ndeploying agentic workflows, where we must balance effectiveness with resource constraints.\\n9.3.2 Optimizing Workflow Edges\\nThe edge space Edefines the representation formalism for agentic workflows. Current approaches primarily adopt three\\ndistinct representation paradigms: graph-based, neural network-based, and code-based structures. Each paradigm offers\\nunique advantages and introduces specific constraints on the optimization process.\\nGraph-based representations enable the expression of hierarchical, sequential, and parallel relationships between nodes.\\nThis approach naturally accommodates complex branching patterns and facilitates visualization of workflow topology,\\nmaking it particularly suitable for scenarios requiring explicit structural manipulation. For example, GPTSwarm [ 651]\\ndemonstrated the effectiveness of graph-based workflow representation in coordinating multiple LLM components\\nthrough topology-aware optimization. Neural network architectures provide another powerful representation paradigm\\nthat excels in capturing non-linear relationships between nodes. Dylan [ 725] showed that neural network-based\\nworkflows can exhibit adaptive behavior through learnable parameters, making them especially effective for scenarios\\nrequiring dynamic adjustment based on input and feedback. Code-based representation offers the most comprehensive\\nexpressiveness among current approaches. AFLOW [ 773] and ADAS [ 741] established that representing workflows\\nas executable code supports linear sequences, conditional logic, loops, and the integration of both graph and network\\nstructures. This approach provides precise control over workflow execution and leverages LLMs’ inherent code\\ngeneration capabilities.\\nThe choice of edge space representation significantly influences both the search space dimensionality and the applicable\\noptimization algorithms. [ 728] focused solely on prompt optimization while maintaining a fixed workflow topology,\\nenabling the use of textual feedback-based optimization techniques. In contrast, [ 651] developed reinforcement\\nlearning algorithms for joint optimization of individual node prompts and overall topology. [ 773] leveraged code-based\\nrepresentation to enable direct workflow optimization by language models, while recent advances by [ 787] and [ 788]\\nintroduced methods for problem-specific topology optimization.\\n9.3.3 Optimizing Workflow Nodes\\nThe node space Nconsists of four key dimensions that influence node behavior and performance. The output format\\nspace Fsignificantly impacts performance by structuring LLM outputs, with formats like XML and JSON enabling\\nmore precise control over response structure. The temperature parameter τcontrols output randomness, affecting the\\nstability-creativity tradeoff in node responses. The prompt space Pinherits the optimization domain from prompt-level\\noptimization, determining the core interaction patterns with LLMs. The model space Mrepresents available LLMs,\\neach with distinct capabilities and computational costs.\\nFor single-node optimization, existing research has primarily focused on specific dimensions within this space. [ 773]\\nconcentrated exclusively on prompt optimization, while [ 741] extended the search space to include both prompts and\\ntemperature parameters. Taking a different approach, [ 789] fixed prompts while exploring model selection across\\ndifferent nodes. Output format optimization, though crucial, remains relatively unexplored [790].\\nCompared to edge space optimization, node space optimization poses unique scalability challenges due to the typically\\nlarge number of nodes in agentic workflows. The dimensionality of the search space grows multiplicatively with\\neach additional node, necessitating efficient optimization strategies that can effectively handle this complexity while\\nmaintaining reasonable computational costs.\\n106\\n9.4 Tool Optimization\\nUnlike conventional usage of LLMs that typically operate in a single-turn manner, agents are equipped with advanced\\nmulti-turn planning capabilities and the ability to interact with the external world via various tools. These unique\\nattributes make the optimization of tool usage a critical component in enhancing an agent’s overall performance and\\nadaptability. Tool optimization involves systematically evaluating and refining how an agent selects, invokes, and\\nintegrates available tools to solve problems with higher efficiency and lower latency. Key performance metrics in this\\ncontext include decision-making accuracy, retrieval efficiency, selection precision, task planning, and risk management.\\nCentral to this optimization are two complementary strategies: tool learning andtool creation .\\n9.4.1 Learning to Use Tools\\nUnlike prompting-based methods that leverage frozen foundation models’ in-context learning abilities, training-based\\nmethods optimize the model that backs LLM agents with supervision. Drawing inspiration from developmental\\npsychology, tool learning can be categorized into two primary streams: learning from demonstrations andlearning from\\nfeedback [714]. The other way to elicit the power of LLMs (agents) using tools is by using prompt-based or in-context\\nlearning methods for better reasoning abilities.\\nLearning from demonstrations involves training models backed LLM agents to mimic expert behaviors through\\nimitation learning. Techniques such as behavior cloning allow models to learn policies in a supervised manner by\\nreplicating human-annotated tool-use actions. Formally, given a dataset D={(qi, a∗\\ni)}N−1\\ni=0, where qiis a user query\\nanda∗\\niis the corresponding human demonstration, the controller’s parameters θCare optimized as:\\nθ∗\\nC= arg max\\nθCE(qi,a∗\\ni)∈DTiY\\nt=0pθC(a∗\\ni,t|xi,t, Hi,t, qi)\\nwhere a∗\\ni,tis the human annotation at timestep tfor query qi, and Tiis the total number of timesteps.\\nLearning from feedback leverages reinforcement learning to enable models to adapt based on rewards derived from\\nenvironment or human feedback. The optimization objective for the controller’s parameters θCis:\\nθ∗\\nC= arg max\\nθCEqi∈QE{ai,t}Ti\\nt=0h\\nR\\x10\\n{ai,t}Ti\\nt=0\\x11i\\nwhere Rrepresents the reward function based on the sequence of actions {ai,t}.\\nIntegrating tool learning into the optimization framework enhances the system’s ability to generalize tool usage across\\ndiverse tasks and environments. By incorporating both demonstration-based and feedback-based learning, the model\\ncan iteratively improve its tool invocation strategies, selection policies, and execution accuracy.\\nOptimization Reasoning Strategies for Tool Using Optimizing the aforementioned metrics for better LLM agents’\\nabilities requires a combination of advanced retrieval models, fine-tuned reasoning strategies, and adaptive learning\\nmechanisms. Reasoning strategies, such as Chain-of-Thought (CoT) [ 46], Tree-of-Thought [ 72], and Depth-First\\nSearch Decision Trees (DFS-DT) [ 690], facilitate more sophisticated decision-making processes regarding tool usage.\\nFine-tuning the model’s understanding of tools, including parameter interpretation and action execution, enables more\\nprecise and effective tool interactions. Additionally, learning from the model’s outputs allows for better post-processing\\nand analysis, further refining tool utilization efficacy.\\n9.4.2 Creation of New Tools\\nBeyond the optimization of existing tools, the ability to create new tools dynamically [ 703,702,772] based on a deep\\nunderstanding of tasks and current tool usage can significantly enhance the LLM Agent framework’s adaptability and\\nefficiency. In recent work, several complementary approaches have been proposed. ToolMakers [ 702] establishes a\\nclosed-loop framework where a tool-making agent iteratively executes three phases: (1) Proposing Python functions\\nvia programming-by-example using three demonstrations, (2) Verifying functionality through automated unit testing\\n(3 validation samples) with self-debugging of test cases, and (3) Wrapping validated tools with usage demonstrations\\nfor downstream tasks. This rigorous process ensures reliability while maintaining full automation. CREATOR [ 703]\\nadopts a four-stage lifecycle: Creation of task-specific tools through abstract reasoning, Decision planning for tool\\ninvocation, Execution of generated programs, and Rectification through iterative tool refinement—emphasizing tool\\ndiversity, separation of abstract/concrete reasoning, and error recovery mechanisms. In contrast, CRAFT [ 772] employs\\nan offline paradigm that distills domain-specific data into reusable, atomic tools (e.g., object color detection) through\\nGPT-4 prompting, validation, and deduplication. Its training-free approach combines human-inspectable code snippets\\n107\\nwith compositional problem-solving, enabling explainable toolchains while avoiding model fine-tuning—particularly\\neffective when decomposing complex tasks into modular steps.\\nThe integration of these complementary approaches presents rich research opportunities. Hybrid systems could merge\\nCRAFT’s pre-made tool repositories with ToolMakers’ on-demand generation, using functional caching to balance\\nefficiency and adaptability. Future frameworks might implement multi-tier tool hierarchies where primitive operations\\nfrom CRAFT feed into ToolMakers’ composite tools, while CREATOR-style rectification handles edge cases. Advances\\nin self-supervised tool evaluation metrics and cross-domain generalization could further automate the tool lifecycle.\\nNotably, the interplay between tool granularity (atomic vs. composite) and reusability patterns warrants systematic\\ninvestigation—fine-grained tools enable flexible composition but increase orchestration complexity. As agents evolve,\\nbidirectional tool-task co-adaptation mechanisms may emerge, where tools reshape task representations while novel\\ntasks drive tool innovation, ultimately enabling self-improving AI systems.\\n9.4.3 Evaluation of Tool Effectiveness\\nThe evaluation metrics and benchmarks discussed below offer a comprehensive basis for quantifying an agent’s tool\\nusage capabilities. By assessing aspects such as tool invocation, selection accuracy, retrieval efficiency, and planning\\nfor complex tasks, these benchmarks not only measure current performance but also provide clear, concrete objectives\\nfor optimizing tool usage. Such metrics are instrumental in guiding both immediate performance enhancements and\\nlong-term strategic improvements in agent-based systems. In the following sections, we first review the evolution\\nof agent tool use benchmarks and then consolidate the key evaluation metrics that serve as targets for further tool\\noptimization.\\nTool Evaluation Benchmarks Recent efforts in LLM-as-Agent research have spawned diverse benchmarks and\\nframeworks for evaluating tool-use capabilities. Early studies such as Gorilla [ 727] and API-Bank [ 791] pioneered\\nlarge-scale datasets and methods for testing LLM interactions with external APIs, shedding light on issues like argument\\naccuracy and hallucination. Subsequent works like T-Bench [ 792] and ToolBench [ 690] introduced more extensive task\\nsuites and stressed the importance of systematic data generation for tool manipulation. StableToolBench [ 793] further\\nextended this line of inquiry by highlighting the instability of real-world APIs, proposing a virtual API server for more\\nconsistent evaluation. Meanwhile, ToolAlpaca [ 794] investigated the feasibility of achieving generalized tool-use in\\nrelatively smaller language models with minimal in-domain training. Additional efforts like ToolEmu [ 795] assessed\\nthe safety and risk aspects of tool-augmented LM agents through emulated sandbox environments. MetaTool [ 796] then\\nintroduced a new benchmark focused on whether LLMs know when to use tools and can correctly choose which tools to\\nemploy. It provides a dataset named ToolE that covers single-tool and multi-tool usage scenarios, encouraging research\\ninto tool usage awareness and nuanced tool selection. ToolEyes [ 797] pushed the evaluation further by examining\\nreal-world scenarios and multi-step reasoning across a large tool library. Finally, τ-bench [ 798] introduced a human-\\nin-the-loop perspective, emphasizing dynamic user interactions and policy compliance in agent-based conversations.\\nTogether, these benchmarks and frameworks underscore the evolving landscape of tool-augmented LLM research,\\nmarking a shift from isolated reasoning tasks to comprehensive, real-world agent evaluations.\\nMetrics for Tool Invocation Deciding whether to invoke an external tool is a critical step that can significantly affect\\nboth the efficiency and the effectiveness of a system. In many scenarios, the model must determine if its own reasoning\\nis sufficient to answer a query or if additional external knowledge (or functionality) provided by a tool is required. To\\nformalize this process, we introduce a labeled dataset\\nDinv={(qi, yi)}N−1\\ni=0,\\nwhere qirepresents the i-th user query and yi∈ {0,1}is a binary label indicating whether tool invocation is necessary\\n(yi= 1) or not ( yi= 0). Based on this dataset, the model learns a decision function d(qi)defined as:\\nd(qi) =\\x1a1,ifPθ(y= 1|qi)≥τ,\\n0,otherwise ,\\nwhere Pθ(y= 1|qi)denotes the predicted probability (from a model parameterized by θ) that a tool should be invoked\\nfor query qi, and τis a predetermined threshold.\\nIn addition to this decision rule, several metrics can be used to evaluate the model’s ability to correctly decide on tool\\ninvocation. For example, the overall invocation accuracy Ainvcan be computed as:\\nAinv=1\\nNN−1X\\ni=01{d(qi) =yi},\\n108\\nwhere 1{·}is the indicator function. Other metrics such as precision, recall, and F1 score are also applicable. Moreover,\\nifCinvrepresents the cost incurred by invoking a tool and R(qi)the benefit or reward obtained when a tool is correctly\\nused, one can define a net benefit score:\\nBinv=N−1X\\ni=0(1{d(qi) = 1} ·R(qi)−Cinv).\\nThis formulation not only emphasizes accuracy but also considers the cost-effectiveness of invoking external tools.\\nTool Selection Among Candidates Once the decision to invoke a tool is made, the next challenge is to select the most\\nappropriate tool from a pool of candidates. Let the candidate toolset be represented as:\\nT={t1, t2, . . . , t M}.\\nFor a given query qi, assume that the optimal tool (according to ground truth) is t∗\\niand the model selects ˆti. The\\nsimplest measure of selection performance is the tool selection accuracy AS:\\nAS=1\\n|Q|X\\nqi∈Q1{ˆti=t∗\\ni}.\\nHowever, many scenarios involve ranking multiple candidate tools by their relevance. In such cases, ranking-based\\nmetrics such as Mean Reciprocal Rank (MRR) and normalized Discounted Cumulative Gain (nDCG) offer a more\\nnuanced evaluation. [690] use those two when evaluating the tool retriever system.\\nTool Retrieval Efficiency and Hierarchical Accuracy Tool retrieval involves both the speed of identifying a suitable\\ntool and the accuracy of that selection. Efficient retrieval methods reduce latency and computational overhead, while high\\nretrieval accuracy ensures that the most relevant tool is identified for the task. To evaluate tool usage comprehensively,\\nwe adopt a hierarchical framework that distinguishes between retrieval accuracy and selection accuracy. Retrieval\\naccuracy ( AR) reflects how precisely the system retrieves the correct tool from the repository, typically measured by\\nmetrics such as Exact Match (EM) and F1 score, which capture both complete and partial matches. In contrast, selection\\naccuracy ( AS) assesses the system’s ability to choose the optimal tool from a set of candidates, again using similar\\nmetrics. Overall tool usage awareness is further evaluated by accuracy, recall, precision, and F1 score.\\nThe overall retrieval efficiency ERetis thus can be expressed as:\\nERet=AR×AS×AP×AU\\nCR\\nwhere CRis the cost associated with retrieval. Optimization strategies may involve training embedding models with\\nfeedback mechanisms to enhance both efficiency and each hierarchical component of accuracy.\\nFor a more nuanced evaluation of tool selection, Metatool [ 796] introduces the Correct Selection Rate (CSR), which\\nquantifies the percentage of queries for which the model selects the expected tool(s). This evaluation framework\\naddresses four aspects: selecting the correct tool among similar candidates, choosing appropriate tools in context-\\nspecific scenarios, ensuring reliability by avoiding the selection of incorrect or non-existent tools, and handling\\nmulti-tool queries. Together, these metrics and sub-tasks provide a robust measure of both the efficiency and precision\\nin tool retrieval and selection.\\nTool Planning for Complex Tasks Complex tasks often require the sequential application of multiple tools to reach an\\noptimal solution. A tool plan can be represented as an ordered sequence\\nΠ = [ t1, t2, . . . , t K],\\nwhere Kis the number of steps. The quality of such a plan is typically evaluated by balancing its task effectiveness\\n(e.g., via a metric Rtask(Π)) against the plan’s complexity (or length). This balance can be captured by a composite\\nplanning score of the form\\nSplan=α·Rtask(Π)−β·K,\\nwhere αandβare coefficients that adjust the trade-off between the benefits of high task performance and the cost\\nassociated with plan complexity. When ground truth plans Π∗are available, similarity metrics such as BLEU or\\nROUGE can be used to compare the predicted plan ΠwithΠ∗, and an overall planning efficiency metric can be defined\\naccordingly.\\nIn addition, recent work such as ToolEyes [ 797] highlights the importance of behavioral planning in tool usage. Beyond\\nselecting tools and parameters, it is crucial for LLMs to concisely summarize acquired information and strategically plan\\n109\\nsubsequent steps. In this context, the behavioral planning capability is evaluated along two dimensions. First, the score\\nSb-validity ∈[0,1]is computed by assessing (1) the reasonableness of summarizing the current state, (2) the timeliness\\nof planning for the next sequence of actions, and (3) the diversity of planning. Second, the score Sb-integrity ∈[0,1]is\\ncalculated by evaluating (1) grammatical soundness, (2) logical consistency, and (3) the ability to correct thinking. The\\ncomposite behavioral planning score is then determined as\\nSBP=Sb-validity·Sb-integrity ,\\nproviding a holistic measure of the model’s planning capability. This integrated framework ensures that tool planning\\nfor complex tasks not only focuses on the selection and ordering of tools but also on maintaining coherent, effective,\\nand strategically sound planning processes.\\nIn summary, optimizing tool performance within an Agent system necessitates a comprehensive approach that balances\\ndecision-making accuracy, retrieval efficiency, hierarchical selection precision, strategic planning, rigorous risk man-\\nagement, and robust tool learning mechanisms. By implementing targeted optimization and learning strategies, it is\\npossible to enhance both the effectiveness and efficiency of tool-assisted machine learning workflows.\\n9.5 Towards Autonomous Agent Optimization\\nIn addition to optimizing individual modules in agent evolution, such as prompts, tools, and workflows—which are\\nsusceptible to local optima that can compromise the overall performance of the agentic system, a significant body of\\nresearch focuses on optimizing multiple components within the entire agentic systems. This holistic approach enables\\nlarge language model (LLM) agents to evolve more comprehensively. However, optimizing the entire system imposes\\nhigher requirements. The algorithm must not only account for the impact of individual components on the agentic\\nsystem but also consider the complex interactions between different components.\\nADAS [ 741] is one of the most representative works that first formally defines the research problem of automated\\ndesign in agentic systems. It integrates multiple agentic system components into the evolutionary pipeline. Specifically,\\nADAS introduces a meta-agent capable of iteratively designing the agentic system’s workflow, prompts, and potential\\ntools within the overall optimization process. As demonstrated in the experiments, the automatically designed agentic\\nsystems outperform state-of-the-art hand-designed baselines.\\nAdditionally, [ 726] proposes an agent symbolic learning framework for training language agents, inspired by connec-\\ntionist learning principles used in neural networks. By drawing an analogy between agent pipelines and computational\\ngraphs, the framework introduces a language-based approach to backpropagation and weight updates. It defines a\\nprompt-based loss function, propagates language loss through agent trajectories, and updates symbolic components\\naccordingly. This method enables structured optimization of agentic workflows and naturally extends to multi-agent\\nsystems by treating nodes as independent agents or allowing multiple agents to act within a single node.\\n[799] proposes an approach to optimize both prompts and the agent’s own code, enabling self-improvement. This aligns\\nwith the concept of self-reference, where a system can analyze and modify its own structure to enhance performance.\\nSimilarly, [ 773], [787], [800] and [ 788] focus on optimizing both the workflow and prompts within agentic systems. In\\nparticular, [285] introduces an approach that trains additional large language models (LLMs) to generate prompts and\\nworkflows, enabling the automated design of agentic system architectures.\\nIn summary, optimizing the workflow of an entire agentic system is not merely a straightforward aggregation of\\nindividual component optimizations. Instead, it requires carefully designed algorithms that account for complex\\ninterdependencies among components. This makes system-wide optimization a significantly more challenging task,\\nnecessitating advanced techniques to achieve effective and comprehensive improvements.\\n110\\nChapter 10\\nLarge Language Models as Optimizers\\nIn this chapter, we present and discuss existing works that conceptualize LLMs as optimizers. First, we note that most\\nexisting studies focus on the prompt optimization problem defined in Equation (9.1), as optimizing other components of\\nagentic workflows remains an emerging research area. To proceed, we draw parallels with classical iterative algorithms\\nand examine their integration into modern optimization workflows.\\n10.1 Optimization Paradigms\\nTraditional optimization methods differ in their assumptions about objective function accessibility. We categorize them\\ninto three broad classes, each with an expanding level of input space: gradient-based optimization , which relies on\\nexplicit function gradients; zeroth-order optimization , which operates without gradient information; and LLM-based\\noptimization , which extends beyond numerical functions to optimize over structured and high-dimensional input spaces.\\n•Gradient-Based Optimization. These methods assume access to gradient information and iteratively refine\\nparameters. Techniques such as stochastic gradient descent (SGD) and Newton’s method [ 801] are widely used but\\nrequire differentiability, limiting their applicability to discrete problems like prompt tuning and structured decision\\nworkflows, often endowed with a graph structure.\\n•Zeroth-Order Optimization. These methods bypass the need for explicit gradients by estimating search directions\\nfrom function evaluations [ 802]. Examples include Bayesian optimization [ 803], evolutionary strategies [ 804],\\nand finite-difference methods [ 805], which are effective when gradients are unavailable or expensive to compute.\\nHowever, they still rely on well-defined numerical objectives and structured search spaces, which constrains their\\napplicability to language-based tasks.\\n•LLM-Based Optimization. LLMs optimize broader solution spaces by leveraging natural language as both the\\noptimization domain and feedback mechanism. By incorporating structured reasoning and human-like iteration,\\nLLMs excel in refining prompts, generating adaptive workflows, and iteratively improving task performance based\\non user feedback.\\nWhile gradient-based and zeroth-order methods are typically applied to numerical objectives, their core principles,\\nsuch as iterative refinement, search heuristics, and adaptive learning, also underlie LLM-based optimization strategies.\\nBuilding on these insights, we highlight a rapidly emerging class of LLM-based optimization powered by reinforcement\\nlearning, which has become the backbone of slow thinking reasoning models [ 90,806,89]. As these models continue\\nto evolve, we anticipate them driving the next wave of agentic applications, enabling LLMs to navigate complex\\nenvironments with greater adaptability and strategic foresight.\\n10.2 Iterative Approaches to LLM Optimization\\nSome LLM-based optimization methods directly draw inspiration from classical optimization theory by adapting key\\ncomponents to address discrete and structured challenges. A central characteristic of these approaches is the iterative\\nupdate step, in which model-generated modifications are selected from a range of possible improvements to refine\\nthe objective. Using the prompt optimization objective from Equation (9.1) as a running example, a general iterative\\n111\\nArgTopKℒ(i)∇LLMAgg{ℒ(i)}Mi=1Tp∼ℙS(⋅|{ℒ(i)})̂yt+1←TF(x1,y1,…,xt,yt,xt+1)\\nMechanistic InterpretabilityABCC1 O1 C2 O2 C3 O3 Implicit Bayesian InferenceHypothesis LearningIn-Context LearningLLM OptimizersRandom SearchGradient ApproximationSurrogate ModelingTheoretical SupportFigure 10.1: A taxonomy of LLM-based optimization methods, categorized into random search, gradient approximation,\\nand surrogate modeling. We also highlight some theoretical explanations of in-context learning, which includes\\nhypothesis learning, implicit Bayesian inference, and mechanistic interpretability, which underpin the optimization\\ncapabilities of LLMs.\\nalgorithm can be expressed as follows:\\nSample: T∼ D\\nEvaluation: L(T;Tp)←ϕeval(ϕexe(Q, T p), T)\\nUpdate: T′\\np←ϕopt(L(T;Tp))\\nHere, the Sample andUpdate steps are defined based on the agent’s task. In the simplest case, such as optimizing an\\ninstruction for binary classification of movie reviews, the objective Lis measured by classification accuracy. In more\\ncomplex agentic workflows, the decision variable may include prompts at different workflow stages, tool selections,\\nagent topologies, or a combination thereof. As discussed in Chapter 9, a common characteristic of these decision\\nvariables is their combinatorial nature-such as the set of all strings from an LLM’s vocabulary Vor all possible role\\nassignments for agents in a workflow. Since enumerating all possible solutions is often intractable, this necessitates\\ndesigning approximate update steps ϕopt, which we discuss next.\\n•Random Search. Early LLM-based optimization methods leveraged random search variants to optimize prompts in\\ndiscrete natural language spaces [ 774,807,651,732,808,809,810]. These methods often resemble evolutionary\\nalgorithms that iteratively sample candidate decision variables and select the top-performing ones from each\\niteration. The general formulation follows:\\nSample: T∼ D\\nEvaluation: L(i)←ϕeval(ϕexe(Q, T(i)\\np), T), i= 1, . . . , M\\nUpdate: {T(k)′\\np}K\\nk=1←ArgTopKi∈[M]L(i),\\nReplenishment (Optional): {T(j)\\np}M\\nj=K+1∼Mutate( {T(k)\\np}K\\nk=1).\\nWe briefly override previous notations and let Mdenote the total number of candidate prompts sampled per\\niteration, and K(with K < M ) control the number of top-performing candidates-selected with ArgTopK in\\nour algorithm-retained for the next step. This algorithm can optionally incorporate a replenishment step to\\nmaintain diversity in the candidate pool across iterations. Random search methods are simple to implement, highly\\nparallelizable, and particularly effective for single-prompt workflows. Beyond prompt optimization, they have\\nalso demonstrated strong performance in selecting in-context demonstrations [ 811,812]. However, their efficiency\\ncomes at a cost—each iteration requires O(M)parallel API queries, which can become prohibitively expensive for\\ncomplex workflows involving multiple queries.\\n112\\n•Gradient Approximations. Several methods approximate gradient-based updates by iteratively refining solutions.\\nFor instance, [ 779,730,728] generate refinements at different workflow stages. StraGO [ 722] estimates descent\\ndirections using central-difference heuristics, while Trace [ 813] optimizes composed programs by modeling them\\nas computation graphs, similar to backpropagation. The key analogy between gradient updates in continuous\\noptimization and prompt-space refinement is the concept of a “descent direction”—a systematic modification\\nof the decision variable to improve the objective. In contrast, random search methods propose new decision\\nvariables independently at each step, without accessing past update trajectories. Gradient-based approaches, by\\ncontrast, exploit this historical information, often leading to faster convergence. A general iteration for gradient\\napproximation methods is given below:\\nSample: T(i)∼ D, i= 1, . . . , M\\nEvaluation: L(i)←ϕeval(ϕexe(Q, T p), T(i)), i= 1, . . . , M\\nGradient Approximation: g← ∇ LLMAgg\\x10\\nL(1), . . . ,L(M)\\x11\\nUpdate: T′\\np←ϕopt(Tp, g),\\nwhere Mis the minibatch size, Agg(·)is an aggregation function that combines feedback signals (e.g., in numerical\\noptimization, Agg is typically the average operator), ∇LLMrepresents an abstract “LLM-gradient operator” [728]\\nthat generates textual refinement directions based on the feedback signal and the current minibatch (e.g., the agent\\nshould consider the edge case of . . . ). Additionally, ϕoptcan be instantiated as an LLM query, allowing the agent to\\nupdate its prompt based on g.\\nCompared to random search methods, gradient-based approaches offer two key advantages: they enable the\\nincorporation of past refinement directions into ϕopt, analogous to momentum-based techniques in first-order\\noptimization algorithms [ 814,815], and they facilitate backpropagation-like techniques for optimizing computation\\ngraphs [ 651,813,780], making them particularly effective for multi-stage workflows with interdependent opti-\\nmizable modules. However, this flexibility comes at the cost of increased design overhead, such as the need for\\nmeta-prompts to aggregate feedback and apply refinement directions. We further discuss the feasibility of using\\nLLMs to optimize these hyperparameters below. Some approaches also explored direct gradient-based optimization\\nof soft prompts [ 816,817,818]. While effective for simple input-output sequence learning, these methods struggle\\nwith multi-step workflows and sequential decision-making [630, 300].\\nFinally, while these methods leverage first-order optimization insights, the extension of second-order techniques\\n(e.g., quasi-Newton methods) to LLM-based optimization remains largely unexplored. Fortunately, recent works\\nsuch as Revolve [ 780] have taken a step in this direction by introducing a structured approach for second-order\\noptimization, modeling the evolution of response patterns over multiple iterations. By incorporating higher-order\\nrefinements, Revolve enables more stable and informed optimization, effectively mitigating stagnation in complex\\ntasks. We are also excited by emerging trends in leveraging inference-time compute [ 90,89] to incorporate\\nhistorical refinement directions and investigate the benefits of momentum.\\n•Bayesian Optimization and Surrogate Modeling. While the aforementioned approaches achieved significant\\nprogress in LLM-based optimization, they often entail substantial financial and environmental costs due to the high\\nnumber of required LLM interactions. Moreover, these methods can be sensitive to noise, and the optimization\\nlandscape of discrete prompts, among other decision variables, remains poorly understood [ 819,820]. Under these\\nconstraints, Bayesian Optimization (BO) emerges as a compelling alternative, as it builds a noise-resilient surrogate\\nmodel of the optimization objective:\\nSample: T∼ D\\nProposal: {T(i)\\np}M\\ni=1∼S.Propose\\nEvaluation: L(i)←ϕeval(ϕexe(Q, T(i)\\np), T), i= 1, . . . , M\\nUpdate: S←S.UpdatePrior( {L(i)}M\\ni=1,{T(i)\\np}M\\ni=1),\\nwhere Srepresents a probabilistic surrogate model of the optimization objective, equipped with a proposal operator\\n(e.g., posterior sampling from a Gaussian Process BO procedure [ 803]) and an update mechanism based on observed\\nevidence from prompt evaluations. For instance, MIPRO [ 821] employs a Tree-Structured Parzen Estimator as\\nits surrogate [ 822], while PROMST [ 823] trains a score-prediction model to guide prompt tuning. Leveraging a\\nsurrogate model for LLM-based optimization aligns with the emerging trend of amortized optimization for non-\\ndifferentiable objectives [ 824]. For instance, [ 825] trains a prompt-generator LLM to amortize the computational\\ncost of instantiating a beam search problem for discovering jailbreak attack prefixes.\\n113\\nFinally, several other works fit an additional lightweight module-such as a Bayesian belief posterior or a utility\\nfunction-from LLM outputs, to aid the optimization of domain-specific workflows, such as decision-making and\\nmulti-agent negotiations [ 826,827]. This type of amortized methods-those that fit a parameterized model that is\\nreusable for unseen inputs-have found increasing usage in LLM-based optimization, such as jailbreaking [ 828,825].\\n10.3 Optimization Hyperparameters\\nSimilar to traditional optimization, LLM-based methods are highly sensitive to hyperparameters that influence search\\nefficiency and generalization. A key consideration in gradient-based LLM optimizers is the choice of the aggregation\\nfunction Agg(·), which determines how textual feedback is synthesized to guide prompt updates. An improper choice\\ncan lead to loss of critical information or misalignment in iterative refinements. Additionally, [ 813] introduces a\\n“whiteboard” approach, where an LLM program is decomposed into human-interpretable modules. However, design\\nchoices in structuring such modular workflows remain largely unexplored, which poses an open challenge for optimizing\\nLLM-driven decision-making pipelines.\\nHyperparameters in LLM optimization often parallel those in numerical optimization. For example, batch size plays a\\ncrucial role: just as minibatch updates enhance stability and efficiency in classical optimization, LLM-based approaches\\nlike TextGrad [ 728] aggregate feedback across multiple generated samples before making updates. Another key factor\\nis momentum—while it stabilizes updates in gradient-based methods by incorporating past gradients, LLM-based\\noptimizers similarly leverage historical refinements to improve performance over time [ 728,813]. Despite progress in\\nnumerical optimization, hyperparameter selection for LLM-based optimizers remains largely heuristic, often relying on\\nad hoc, trial-and-error tuning.\\nIn agentic system design, hyperparameters proliferate across various components, including role assignments of agents,\\nselection of in-context demonstrations, and scheduling of tool invocations. Each of these choices has a profound\\nimpact on downstream performance, yet principled methods for optimizing them remain underdeveloped. While\\ntraditional hyperparameter tuning techniques, such as grid search and Bayesian optimization, can be applied to discrete\\nLLM-driven workflows, their computational cost scales poorly due to the high variance in language model outputs.\\nAdditionally, the combinatorial nature of these hyperparameters, where agent configurations, prompting strategies, and\\nreasoning structures interact in complex ways, makes an exhaustive search infeasible. Recent work has attempted to\\naddress this challenge by embedding agentic workflows into structured frameworks such as finite state machines [ 729],\\noptimal decision theory [ 826], and game theory [ 827]. However, these approaches often fail to generalize across diverse\\nenvironments. A promising direction for addressing these challenges is meta-optimization, where LLMs are used\\nto optimize their own hyperparameters and decision-making strategies. For example, an LLM-based optimizer can\\niteratively refine its own prompting strategies by treating past decisions as experience, akin to learned optimizers in deep\\nlearning [ 829]. Moreover, amortized approaches train auxiliary models to predict effective hyperparameters, which\\ncan reduce the computational cost of exhaustive search [ 821,823]. While these techniques offer exciting possibilities,\\nthey also introduce new challenges, such as balancing exploration with exploitation in adaptive tuning and ensuring\\ngeneralization across diverse optimization tasks. Investigating principled meta-optimization strategies tailored to\\nLLM-driven workflows remains a critical area for future research.\\n10.4 Optimization across Depth and Time\\nUnlike conventional optimizers that update parameters in a static setting, LLMs optimize workflows dynamically,\\nconsidering both depth (single-pass workflows) and time (recurrent updates). In terms of depth, LLMs function similarly\\nto feedforward networks, sequentially optimizing workflows as they pass through different modules—most existing\\nLLM-based optimizers follow this paradigm. Beyond single-pass execution, LLMs can also optimize over time, akin\\nto recurrent architectures such as RNNs or Universal Transformers [ 830], by iteratively refining decision-making.\\nFor instance, StateFlow [ 729] enhances workflows by incorporating feedback across multiple iterations, enabling\\ndynamic refinement and adaptation over time. While these analogies are compelling, many well-established engineering\\noptimization techniques—such as checkpointing [ 831] and truncated backpropagation [ 832]—remain underexplored\\nin LLM-based optimization. We see this as a promising avenue for future research, echoing previous calls for deeper\\ninvestigation [813].\\n114\\n10.5 A Theoretical Perspective\\nRecent studies suggest that transformers inherently perform optimization-like computations, supporting their potential\\nas general-purpose optimizers for computational workflows. However, a significant gap remains between their empirical\\nsuccess and theoretical understanding. Here, we provide a brief overview of recent progress in bridging this gap.\\n•In-Context Learning. A fundamental perspective on transformers as optimizers emerges from in-context learning,\\nparticularly in few-shot settings [ 2]. [733] demonstrated that transformers can in-context learn diverse regression\\nhypotheses, including regularized linear models, decision trees, and shallow neural networks. Building on this,\\nlater works [ 734,833,735] provided constructive proofs that transformers can implement iterative optimization\\nalgorithms, such as gradient descent and second-order updates. However, while these theoretical models characterize\\ntransformers’ optimization capabilities, they do not fully explain in-context learning in large-scale LLMs, which\\noperate in discrete input-output spaces. Empirical analyses [ 819,834,820] instead sought to understand how\\npre-trained LLMs generalize in-context. [ 834] proposed that in-context learning resembles a hidden Markov\\nmodel (HMM) performing implicit Bayesian inference, while [ 819,820] challenged the conventional view that\\nin-context demonstrations serve as new test-time samples for hypothesis formation. In-context learning remains the\\ncentral emergent ability [ 835] enabling self-improvement and optimization from context, yet it continues to elude\\ncomprehensive theoretical analysis.\\n•Mechanistic Interpretability. Parallel to theoretical analyses, mechanistic interpretability aims to uncover internal\\ntransformer computations by identifying subgraphs, also known as circuits, responsible for specific behaviors.\\nEarly studies mapped circuits for stylized language tasks in pre-trained GPT-2 models [ 836,837,838], while more\\nrecent efforts have scaled up by identifying semantically meaningful features using sparse autoencoders [ 839,\\n736,840,841]. These methods have been largely successful in eliciting causal and controllable behavior from\\nfrontier-class LLMs, but they also reveal an unintended consequence: in-context learning capabilities often entangle\\nbeneficial generalization with harmful behaviors when conditioned on many-shot demonstrations [ 842]. This raises\\nchallenges for optimizing LLM workflows safely and reliably.\\n•Limitations Under Uncertainty. While LLMs demonstrate moderate capabilities in sequential decision-making\\nwhen provided with in-context information, they struggle to make optimal choices under uncertainty [ 843,844,845,\\n846]. In particular, [ 826] found that LLM-based optimizers exhibit difficulty in adapting to stochastic environments,\\noften failing to explore optimally. These findings serve as a cautionary note for deploying LLM-based optimizers\\nin dynamic or uncertain settings where exploration and robust decision-making are critical.\\nLLMs redefine optimization by integrating structured reasoning, natural language processing, and in-context learning,\\nexpanding beyond traditional numerical methods. Despite strong empirical performance in structured search spaces,\\nopen questions remain about the theoretical underpinnings of LLM-based optimization, particularly the emergence of\\nin-context learning from standard gradient-based training.\\n115\\nChapter 11\\nOnline and Offline Agent Self-Improvement\\nIn the pursuit of self-improvement, intelligent agents leverage optimization as both a mechanism for refining individual\\ncomponents—such as prompt design, workflow orchestration, tool utilization, reward function adaptation, and even\\nthe optimization algorithms themselves—and as a strategic framework that ensures these individual improvements are\\naligned toward coherent performance enhancement. For instance, optimizing the reward function and prompt design\\nin isolation might yield conflicting outcomes, but a strategic approach coordinates these optimizations to maintain\\ncoherence and maximize overall effectiveness. We categorize self-evolution into two primary paradigms: online and\\noffline self-improvement. Additionally, we explore hybrid optimization strategies that integrate both approaches to\\nmaximize efficiency and adaptability.\\n11.1 Online Agent Self-Improvement\\nOnline self-improvement refers to real-time optimization in which an agent dynamically adjusts its behavior based on\\nimmediate feedback. This paradigm ensures that agents remain responsive to evolving environments by continuously\\noptimizing key performance metrics—such as task success, latency, cost, and stability—in an iterative feedback loop.\\nOnline self-improvement is particularly effective in applications that require dynamic adaptability, such as real-time\\ndecision-making, personalized user interactions, and automated reasoning systems. Key optimization strategies in\\nonline self-improvement can be classified into the following four categories: Iterative Feedback and Self-Reflection,\\nActive Exploration in Multi-Agent Systems, Real-Time Reward Shaping, and Dynamic Parameter Tuning.\\nIterative Feedback and Self-Reflection These methodologies [ 48,67,72,70,847,47] focus on enabling agents to\\ncritique and refine their own outputs iteratively. Reflexion [ 48], Self-Refine [ 67], and Tree of Thoughts [ 72] introduce\\nself-critique loops, where the model identifies errors and proposes revisions in real-time. ReAct [ 70] combines\\nchain-of-thought “reasoning” with “acting”, allowing the model to revise steps iteratively after observing external\\nfeedback. In addition, other methods either rely on self-consistency [ 78] to select the most coherent solution or leverage\\na process reward model (PRM)Lightman et al. [847] to choose the best solution from the candidates. Collectively, these\\nframeworks reduce error propagation and support rapid adaptation without requiring a separate offline fine-tuning cycle.\\nActive Exploration in Multi-Agent Systems These approaches [ 626,848,627,152] actively explore and dynamically\\nsearch for novel patterns and workflow improvements in multi-agent systems. MetaGPT [ 626], CAMEL [ 848], and\\nChatDev [ 627] showcase multi-role or multi-agent ecosystems that interact in real-time, exchanging continuous feedback\\nto refine each other’s contributions. Similarly, HuggingGPT [ 152] coordinates specialized models (hosted on Hugging\\nFace) through a central LLM controller, which dynamically routes tasks and gathers feedback. These collaborative\\nstrategies further highlight how online updates among agents can incrementally refine collective outcomes.\\nReal-Time Reward Shaping Rather than relying on fixed or purely offline reward specifications, some frame-\\nworks [ 731,91,105,849] integrate immediate feedback signals not only to correct errors, but also to adapt internal\\nreward functions and policies. This enables self-adaptive reward calibration that balances trade-offs between perfor-\\nmance, computational cost, and latency, allowing agents to optimize reward mechanisms dynamically in response to\\nuser interactions.\\n116\\nHybridActionEnvironment\\nOptimizer\\nOptimized\\nAgentic\\nSystemsImmediate\\nFeedbackOnlineDynamic\\nAdaptabilityOffline\\nCurated\\nDatasets\\nOptimizer\\nOptimized\\nAgentic\\nSystemsAction\\nEnvironmentStructured\\nBatch -based\\nFeedbackLong -Term\\nRobustnessFigure 11.1: An illustration of self-improvement under three different utilization scenarios, including Online, Offline,\\nand Hybrid self-improvement.\\nDynamic Parameter Tuning In this category, agents autonomously update their internal parameters (including prompt\\ntemplates, tool invocation thresholds, search heuristics, etc.) in real time, leveraging gradient-free or approximated\\ngradient methods. These updates optimize both computational efficiency and decision accuracy, allowing for seamless\\nadaptation to evolving contexts. Self-Steering Optimization (SSO) [ 850] eliminates the need for manual annotation\\nand maintains signal accuracy while keeping training on-policy by autonomously generating preference signals during\\niterative training.\\nOnline self-improvement fosters a continuously evolving agent framework where learning is embedded within task\\nexecution, promoting enhanced real-time adaptability, user-centric optimization, and robust problem-solving capabilities.\\n11.2 Offline Agent Self-Improvement\\nOffline self-improvement, in contrast, leverages structured, batch-based optimization. This paradigm utilizes scheduled\\ntraining sessions with high-quality curated datasets to systematically improve the agent’s generalization capabilities [ 851,\\n667,852,853,854]. Unlike online approaches, offline approaches accommodate more computationally intensive\\nmethodologies, including Batch Parameter Updates and Fine-Tuning, Meta-Optimization, and Systematic Reward\\nModel Calibration.\\nBatch Parameter Updates and Fine-Tuning In this category, agents undergo extensive fine-tuning using supervised\\nlearning or reinforcement learning (RL) techniques, optimizing performance across large-scale datasets over multiple\\ntraining epochs. Retrieval-augmented generation (RAG) is often integrated to enhance contextual understanding and\\nlong-term memory retrieval [ 740,741]. Such methods allow agents to optimize retrieval strategies, thereby improving\\nreasoning over extensive knowledge corpora.\\nMeta-Optimization of Agent Components Here offline training is not limited to improving task performance but\\nextends to refining optimization algorithms themselves. Meta-learning strategies that optimize hyperparameters or\\neven restructure the optimization process dynamically have demonstrated promising outcomes [ 731,91]. These\\nmeta-optimization approaches enable agents to discover the most effective learning parameters for new problem\\ndomains.\\n117\\nSystematic Reward Model Calibration Offline settings facilitate the precise calibration of reward models, incorporat-\\ning hierarchical or listwise reward integration frameworks (e.g., LIRE [ 855]) to align agent behavior with long-term\\nobjectives through gradient-based reward optimization. Such calibration ensures that reward functions reflect real-world\\ntask complexity, thereby mitigating bias and enhancing generalization.\\nThe structured nature of offline optimization results in a robust agent baseline, whose performance is fine-tuned\\nto optimize stability, efficiency, and computational cost before real-world deployment. Offline training allows for\\nhigh-fidelity model refinement and is essential for mission-critical applications requiring predictable performance\\nguarantees.\\n11.3 Comparison of Online and Offline Improvement\\nOnline and offline optimization offer complementary benefits, each excelling in different aspects of self-improvement.\\nOnline optimization thrives in dynamic environments, where real-time feedback enables continuous adaptation. It is\\nwell-suited for applications that require immediate responsiveness, such as interactive agents, real-time decision-making,\\nand reinforcement learning systems. However, frequent updates may introduce instability or drift, requiring mechanisms\\nto mitigate performance degradation over time.\\nIn contrast, offline optimization emphasizes structured, high-fidelity training using pre-collected datasets, ensuring\\nrobust and stable performance before deployment. By leveraging computationally intensive learning methods such\\nas batch training, fine-tuning, and meta-optimization, offline approaches provide strong generalization and long-term\\nconsistency. However, they lack the agility of online learning and may struggle to adapt efficiently to novel scenarios\\nwithout additional retraining. Table 11.1 summarizes the key distinctions between these two paradigms.\\nFeature Online Optimization Offline Optimization\\nLearning Process Continuous updates based on real-time\\nfeedbackBatch updates during scheduled training\\nphases\\nAdaptability High, capable of adjusting dynamically Lower, adapts only after retraining\\nComputational Effi-\\nciencyMore efficient for incremental updates More resource-intensive due to batch\\ntraining\\nData Dependency Requires real-time data streams Relies on curated, high-quality datasets\\nRisk of Overfitting Lower due to continuous learning Higher if training data is not diverse\\nStability Potentially less stable due to frequent\\nupdatesMore stable with controlled training set-\\ntings\\nTable 11.1: Comparison of Online vs. Offline Optimization Strategies in Self-Improvement Agents.\\nWhile both approaches have inherent strengths and trade-offs, modern intelligent systems increasingly integrate them\\nthrough hybrid optimization strategies. These hybrid frameworks leverage the stability of offline training while\\nincorporating real-time adaptability, enabling agents to maintain long-term robustness while continuously refining their\\nperformance in dynamic environments.\\n11.4 Hybrid Approaches\\nRecognizing that both online and offline methods have inherent limitations, many contemporary systems adopt hybrid\\noptimization strategies. These hybrid methods integrate structured offline optimization with responsive online updates\\nto achieve continuous incremental agent enhancement.\\nHybrid optimization explicitly supports self-improvement by empowering agents to autonomously evaluate, adapt, and\\nenhance their behaviors through distinct yet interconnected stages:\\n•Offline Pre-Training: In this foundational stage, agents acquire robust baseline capabilities through extensive\\noffline training on curated datasets. This stage establishes essential skills, such as reasoning and decision-making,\\nrequired for initial autonomous performance. For instance, frameworks such as the one introduced by Schrittwieser\\net al. [ 856] illustrate how offline pretraining systematically enhances initial agent capabilities, ensuring subsequent\\nonline improvements are built upon a stable foundation.\\n•Online Fine-Tuning for Dynamic Adaptation: Agents actively refine their capabilities by autonomously evaluating\\ntheir performance, identifying shortcomings, and dynamically adjusting strategies based on real-time feedback.\\nThis adaptive fine-tuning stage directly aligns with the agent self-improvement paradigm by allowing real-time\\n118\\noptimization of agent-specific workflows and behaviors, exemplified by Decision Mamba-Hybrid (DM-H) [ 857],\\nwhere agents efficiently adapt to complex, evolving scenarios.\\n•Periodic Offline Consolidation for Long-Term Improvement: periodic offline consolidation phases, agents\\nsystematically integrate and solidify improvements identified during online interactions. This ensures that incremental,\\nonline-acquired skills and improvements are systematically integrated into the agent’s core models, maintaining\\nlong-term stability and effectiveness. The Uni-O4 framework [ 858] exemplifies how this process enables seamless\\ntransitions between offline knowledge consolidation and online adaptive improvements.\\nHybrid optimization thus explicitly supports autonomous, continuous evolution by seamlessly interweaving structured\\noffline learning with proactive, real-time online adaptation. This cyclical approach equips agents with both immediate\\nresponsiveness and stable long-term improvement, making it ideally suited for complex, real-world scenarios such as\\nautonomous robotics, personalized intelligent assistants, and interactive systems.\\n119\\nChapter 12\\nScientific Discovery and Intelligent Evolution\\nIn previous chapters, we primarily discussed the evolution of agentic systems from a technical perspective, focusing on\\nhow to develop systems that can effectively perform well-defined tasks traditionally executed by humans. However, a\\nfundamental and important question remains: can these agents drive a self-sustaining innovation cycle that propels both\\nagent evolution and human progress?\\nScientific knowledge discovery is a compelling example of self-evolution in intelligent beings, as it helps them adapt\\nto the world in a sustainable way. Agents capable of discovering scientific knowledge at different levels of autonomy\\nand in a safe manner will also play important roles in technological innovation for humanity. In this section, we\\nsurvey progress in autonomous discovery using agentic workflows and discuss the technological readiness toward fully\\nautonomous, self-evolving agents. Within this scope, the goal of the agent is to uncover, validate, and integrate data,\\ninsights, and principles to advance an objective scientific understanding of natural phenomena. Instead of altering the\\nworld, the agent seeks to better understand nature as a Scientist AI [ 859] and assist humans in extending the boundaries\\nof knowledge.\\nWe first define the concept of knowledge and intelligence to clarify our discussion, then introduce three typical scenarios\\nwhere agents and scientific knowledge interact. We also highlight existing successes and examples of self-enhancing\\nagents applied to theoretical, computational, and experimental scientific research. Lastly, we summarize the current\\nchallenges for a future outlook.\\n12.1 Agent’s Intelligence for Scientific Knowledge Discovery\\nKnowledge, traditionally defined as justified true belief , traces back to Plato [ 860] and has been further refined by\\nEdmund Gettier [ 861], who argued that knowledge must be produced by a reliable cognitive process—though its\\nprecise definition remains debated [ 862]. In our discussion, we describe scientific knowledge discovery as the process\\nof collecting data and information to either justify or falsify rational hypotheses about target scientific problems. To\\ndiscuss the capability of agents in scientific knowledge discovery, we first explore a general framework for measuring\\nan agent’s intelligence through the lens of information theory.\\n12.1.1 KL Divergence-based Intelligence Measure\\nThe agent’s intelligence can be measured by the KL divergence between its predicted and real-world probability\\ndistributions of unknown information. A long-standing goal in both artificial intelligence and the philosophy of\\nscience is to formalize what it means for an agent to “understand” the world. From Jaynes’ view of probability theory as\\nextended logic for reasoning under uncertainty [ 863], to Parr et al.’s framing of intelligence as minimizing model-world\\ndivergence under the free energy principle [ 864], many frameworks converge on a common theme: intelligent behavior\\narises from making accurate predictions about an uncertain world. Clark [ 344], for instance, argues that intelligent\\nagents constantly engage with the world through prediction and error correction to reduce surprise. Chollet [ 865]\\nemphasizes that intelligence should reflect skill-acquisition efficiency, because of the dynamic nature of task adaptation.\\nTogether, these views suggest that intelligence involves building predictive and adaptable models—an idea formalized\\nhere through a probabilistic framework that links reasoning to knowledge acquisition and enables comparison across\\nagents in scientific discovery.\\n120\\nBuilding on this foundation, we consider intelligence in the specific context of scientific knowledge discovery, where\\nthe agent’s primary objective is to infer unknown aspects of the physical world from limited data. From the agent’s\\nperspective in knowledge discovery, the world Wis characterized by an ensemble of datasets x={x1, x2, ..., x n}\\nrelated to the scientific problem the agent aims to understand. During the agent’s interaction with W, each dataset\\nappears in the experimental measurements or observations with a probability PW(x). Here we assume that individual\\ndata points ximay or may not be correlated. For example, in a task of text generation using a language model, xi\\nrepresents a chunk of tokens forming a meaningful proposition, and xis a coherent text constructed from known and\\ninferred propositions. In this context, the “world” is the ensemble of all propositions.\\nLetθdenote the parameter that parameterizes the agent’s world model, Mwm\\nt, as defined in Table 1.2. For instance, in\\na transformer model with a fixed architecture, θrepresents its weights. Given θand a dataset x, the agent predicts a\\nprobability distribution Pθ(x). In general, different AI agents could be optimized for different goals. For scientific\\nknowledge discovery, we assume that the agent’s goal is to produce a good description of the real world, i.e., a world\\nmodel that predicts yet-to-be-explored natural phenomena as accurately as possible. A more intelligent agent produces\\na better approximation of the real-world distribution PW(x). The agent’s intelligence can thus be measured by the KL\\ndivergence, or relative entropy, between these two probability distributions:\\nD0(θ) =X\\nx⊆WPW(x) logPW(x)\\nPθ(x)(12.1)\\nD0(θ)describes the difference between PW(x)andPθ(x). More precisely, in the context of hypothesis testing, if we\\nsample PW(x)Ntimes and compare the results with the predictions from Pθ(x), the probability of mistaking PW(x)\\nforPθ(x)scales as e−ND 0(θ)[866]. In other words, an agent with a lower D0(θ)produces predictions that align more\\nclosely with reality.\\nFor example, consider two materials synthesis agents whose goal, Mgoal\\nt, is to understand whether or not an inorganic\\ncompound of interest, CaFe 2(PO4)2O, is synthesizable. The agents can predict either (1) x1={CaFe 2(PO4)2O is\\nsynthesizable}, and (2) x2={CaFe 2(PO4)2O is not synthesizable}. In reality, since CaFe 2(PO4)2O is a natural mineral,\\nPW(x1)= 1 and PW(x2)= 0. However, this mineral was only recently reported on October 4, 2023[ref], after the\\nknowledge cutoff of many LLMs; thus, the agents lacks that knowledge. Compare Agent 1, which guesses randomly\\nPθ1(x1) =Pθ1(x1)= 0.5, yielding D0(θ1) = log 2 . In contrast, Agent 2 uses first-principles calculations and\\nfinds that CaFe 2(PO4)2O (assume structure is xx [cite: Materials Project ID]) is the lowest-energy phase among its\\ncompetitors [ref], indicating stability. Thereby, Agent 2 predicts that CaFe 2(PO4)2O is likely synthesizable, suggesting\\nPθ2(x1)>0.5> P θ2(x2). Consequently, D0(θ2) =−logPθ2(x1)< D 0(θ1), meaning that Agent 2 has a more\\naccurate understanding of the real world.\\nNow, let us assume the agent has conducted some measurements and determined specific values for a subset of data\\npoints xi. LetxKdenote this known subset and xUthe remaining unknown part. Correspondingly, we define the\\nspace of all existing knowledge as Kand the space of all unknown information as U, satisfying xK⊆ K,xU⊆ U,\\nandK ∪ U =W. For example, in text generation, the the prompt text xKrepresents already known information. The\\nefficiency of the language model is then measured by its predictive accuracy for the generated text xUbased on xK.\\nMore generally, the agent’s intelligence is measured by the relative entropy of the conditional probability distribution:\\nDK(θ,xK) =X\\nx⊆UPW(x|xK) logPW(x|xK)\\nPθ(x|xK)(12.2)\\nIn practice, all of the agent’s knowledge is stored in its memory Mmem\\nt, i.e.,xK=K=Mmem\\nt andU=W \\\\Mmem\\nt,\\nwe define the agent’s intelligence as:\\nIQagent\\nt≡ −DK(θ, Mmem\\nt) =−X\\nx⊆UPW(x|Mmem\\nt) logPW(x|Mmem\\nt)\\nPθ(x|Mmem\\nt)(12.3)\\nIn other words, the the agent’s intelligence IQagent\\nt is determined by its memory Mmem\\nt and the parameter θof its\\nworld model Mwm\\nt. A schematic plot is shown in Figure 12.1. At time t= 0, when the Mmem\\nt is very limited or lack\\nrelevant information to a new target scientific problem, IQagent\\nt is primarily determined by the zero-shot predictive\\nability of Mwm\\nt, corresponding to fluid intelligence [ 867]. Over time, as more relevant knowledge is incorporated\\nintoMmem\\nt,IQagent\\nt becomes increasingly dependent on the knowledge-augmented predictive capability of Mwm\\nt,\\nreflecting crystallized intelligence [868].\\n121\\nFigure 12.1: Schematic representation of agent intelligence and knowledge discovery. The agent’s intelligence,\\nmeasured by the KL divergence DKbetween predictions and real-world probability distributions, evolves from fluid\\nintelligence (zero-shot predictions for new problems) to crystallized intelligence (knowledge-augmented predictions\\nafter learning) as it accumulates data in its memory Mmem\\nt over time t. Given Mmem\\nt, the evolution of DKvaries\\nwithin the world model’s parameter space Θ, as illustrated by θ1andθ2in the solid lines. The expressive limitation of\\nΘis characterized by the envelope Dmin\\nK,Θ. Given Θ,Dmin\\nK,Θis influenced by different knowledge expansion strategies,\\nsuch as1Mmem\\nt and2Mmem\\nt, shown as dash lines.\\n12.1.2 Statistical Nature of Intelligence Growth\\nThe agent’s intelligence, in a statistical sense, is a non-decreasing function of acquired knowledge. Roughly\\nspeaking, IQagent\\nt quantifies both the amount of knowledge an agent has acquired and how effectively the agent can\\napply that knowledge after learning from Mmem\\nt. Intuitively, if the agent gains additional information at time t—which\\ncorresponds to enlarging Mmem\\nt and shrinking U—its intelligence should increase.\\nTo understand this process, consider a small region ∆⊆ U and examine the effect of adding a dataset x∆from ∆to\\nMmem\\nt. Denote U=U′∪∆, where U′represents the remaining unknown part of the world. The agent’s intelligence at\\ntimet+ 1is given by:\\nIQagent\\nt+1≡ −DK(θ, Mmem\\ntx∆) =−X\\nx′⊆U′PW(x′|Mmem\\ntx∆) logPW(x′|Mmem\\ntx∆)\\nPθ(x′|Mmem\\ntx∆)(12.4)\\nDirectly comparing IQagent\\nt andIQagent\\nt+1 is challenging. Instead, we can compare the expected value of IQagent\\nt+1,\\naveraging over x∆with probability PW(x∆|Mmem\\nt). This expectation represents the average amount of knowledge\\ngained by measuring ∆, given prior knowledge in Mmem\\nt. We obtain:\\nX\\nx⊆∆PW(x|Mmem\\nt)IQagent\\nt+1=−X\\nx′⊆U′,x⊆∆PW(x′x|Mmem\\nt) logPW(x′|Mmem\\ntx)\\nPθ(x′|Mmem\\ntx)\\n=IQagent\\nt +X\\nx⊆∆PW(x|Mmem\\nt) logPW(x|Mmem\\nt)\\nPθ(x|Mmem\\nt)(12.5)\\nThe second term is the relative entropy of the conditional probability distribution of x∆conditioned on Mmem\\nt, which\\nis always non-negative. Therefore, on average, IQagent\\nt is non-decreasing as Mmem\\nt acquires new knowledge over time.\\nNote that IQagent\\nt+1 can be further increased by leveraging the newly acquired knowledge to optimize θwithin Mwm\\nt.\\nInterestingly, the expected gain in intelligence at time tis determined by the discrepancy between the actual distribution\\nPW(x|Mmem\\nt)and the model-predicted distribution Pθ(x|Mmem\\nt). In other words, the rate of intelligence growth in\\nFigure 12.1 is higher when the new measurement result is more unexpected. This observation identifies scientist agents\\n122\\n[859] as a special type of curiosity-driven agent [ 869], prioritizing exploration over exploitation to expand the frontiers\\nof knowledge for deeper understanding of nature. Unlike agents that leverage existing knowledge to achieve predefined\\nobjectives, curiosity-driven agents can learn without extrinsic rewards [ 387,870] (see Section 5.3 for details), enabling\\ndiscoveries beyond human-planned search spaces and revealing knowledge in unexplored domains. This potential also\\nunderscores the importance of equipping curiosity-driven agents with fundamental perception and action tools that can\\nbe transferred to explore new knowledge domains.\\n12.1.3 Intelligence Evolution Strategies\\nThe strategy for expanding known information determines how quickly an agent’s intelligence evolves. For a\\ngiven knowledge base Mmem\\nt, the parameter θcan be optimized over a space of world models Θcharacterized by the\\narchitecture of Mwm\\nt. The optimal agent is the one that minimizes DK(θ, Mmem\\nt), thereby maximizing IQagent\\nt :\\nθ∗\\nK,t≡arg supθIQagent\\nt = arg infθDK(θ, Mmem\\nt) (12.6)\\nand\\nDmin\\nK,Θ(Mmem\\nt)≡DK(θ∗\\nK,t, Mmem\\nt) (12.7)\\nHere, Dmin\\nK,Θ(Mmem\\nt)represents the minimum unknown after learning from Mmem\\nt for this family of models, quantifying\\nthe expressive limitations of Θ. As shown in Figure 12.1, Dmin\\nK,Θ(Mmem\\nt)forms the envelope of the family of functions\\nDK(θ, Mmem\\nt), where θranges over Θ.\\nFor a given model family Θ,Dmin\\nK,Θ(Mmem\\nt)measures the best possible prediction of residual unknowns in addressing\\nthe target scientific problem based on Mmem\\nt. In other words, the knowledge content in Mmem\\nt is captured by\\nDmin\\nK,Θ(Mmem\\nt). One can prove that Dmin\\nK,Θ(Mmem\\nt)is monotonically non-increasing as Mmem\\nt expands, since it forms\\nthe envelope of a family of non-increasing functions DK(θ, Mmem\\nt). This expansion process is tied to how the agent\\nacts and gains information, driven by Mwm\\nt, which determines the optimal expansion and executes it through the action\\nat∈ A at time t(see Table 1.2).\\nDuring knowledge discovery, different strategies can be employed to expand Mmem\\nt. The optimal expansion strategy is\\nthe one that results in the steepest decrease of Dmin\\nK,Θ(Mmem\\nt). For instance, in Figure 12.1, we illustrate two strategies\\nfor expanding Mmem\\nt, denoted as1Mmem\\nt and2Mmem\\nt. The first strategy,1Mmem\\nt, represents random exploration,\\nwhile the second,2Mmem\\nt, follows a hypothesis-driven approach [ 871] in which the agent first formulates a hypothesis\\nabout the underlying mechanism of the target problem and then designs an experiment to justify or falsify this hypothesis\\n[749]. In practice, experimentalists typically adopt the hypothesis-driven strategy because it enables them to guide\\nthe expansion of Mmem\\nt in a way that maximizes the reduction of Dmin\\nK,Θ(Mmem\\nt), subject to resource constraints.\\nThis approach is generally more efficient than random exploration for expanding Mmem\\nt, leading to Dmin\\nK,Θ(2Mmem\\nt)\\ndescending faster than Dmin\\nK,Θ(1Mmem\\nt).\\nIn general, the knowledge discovery process proceeds iteratively, repeatedly optimizing the world model parameter θto\\napproach θ∗\\nK,tand expanding Mmem\\nt in a rational manner to accelerate the decrease of Dmin\\nK,Θ(Mmem\\nt). The ideal state\\nis achieving epistemic completeness, i.e., Dmin\\nK,Θ(Mmem\\nt) = 0 , meaning zero discrepancy between the agent’s prediction\\nand the real-world phenomena. However, for a specific agent, a discovery bound may exist, where Dmin\\nK,Θ(Mmem\\nt)\\napproaches zero but remains positive. These discrepancies arise from practical constraints and the limitations of Θ,A,\\nand other design spaces of the agent [ 872]. Achieving a low discovery bound requires designing an adaptive world\\nmodel architecture, an efficient knowledge expansion strategy, and a sufficient action space.\\n12.2 Agent-Knowledge Interactions\\nTypical forms of scientific knowledge include observational knowledge (e.g., experimental measurements, computational\\nresults), methodological knowledge (e.g., experimental methods, computational techniques, protocols), and theoretical\\nknowledge (e.g., theories, laws, predictive models). These forms of knowledge can contribute to scientific understanding\\nas long as they consist of data and information processed in a way that affects the probability distribution of unknown\\ninformation Pθ(xU|Mmem\\nt), reduces DK(θ, Mmem\\nt), and facilitates decision-making.\\nIn principle, external scientific knowledge has been shown to be useful in improving agent performance in reasoning\\nand decision-making [ 873,874]. However, the scope of this survey lies in how agents can autonomously discover\\nand utilize knowledge to enhance themselves. Scientific knowledge discovery workflows typically involve hypothesis\\ngeneration, protocol planning, conducting experiments and computations, analyzing data, deriving implications, and\\n123\\nrevising hypotheses—often as part of an iterative cycle. An agent that can perceive, learn, reason, and act has the\\npotential to drive such workflows in an autonomous manner, for example by using application programming interfaces\\n(APIs) to interact with physical instruments to acquire scientific knowledge and iteratively enhance its knowledge base\\n(Figure 12.2). The agent will use the acquired knowledge to update its mental states Mtto make better decisions when\\ninteracting with the world W. We will now highlight three scenarios where agents discover scientific knowledge and\\nenhance themselves.\\nFigure 12.2: Closed-loop knowledge discovery for sustainable self-evolution. The agent aims to iteratively enhance\\nits intelligence IQagent\\nt through hypothesis generation and testing, as well as through data analysis and implication\\nderivation. When interacting with the physical world W, the agent generates hypotheses as an explicitly or implicitly\\npredicted distribution ( Pθ) of unknown information, takes actions ( at) for hypothesis testing, observes experimental\\nresults ( ot), and updates beliefs based on perception of the real-world distribution ( PW). When not interacting with W,\\nthe agent distills knowledge from existing data and premises, updating mental states Mtdirectly. Inspired by Figures\\n2.3 and 2.5 in [864].\\n12.2.1 Hypothesis Generation and Testing\\nHypothesis generation and testing (Figure 12.2) is a critical application of agents in autonomous scientific discovery,\\nas it has the potential to enable outside-the-box innovations [ 749]. In essence, hypothesis generation is the formation\\nof potential rules that govern data distribution—ranging from single observations to large datasets—pertaining to\\nunobserved scientific phenomena. According to Sir Karl Popper, a scientific hypothesis must be falsifiable [ 875,876];\\nin this discussion, we define a hypothesis that survives falsification as a justified true hypothesis [877,860]. Typically,\\nscientists test hypotheses by conducting experiments to either justify or falsify them. A hypothesis is considered more\\nvaluable if it is broad enough to explain a wide range of data and is highly likely to be true.\\nTo tackle a scientific problem, the agent formulates one or a small number of high-value hypotheses based on its mental\\nstateMt, which contains only incomplete information about the partially observable world W. After testing through\\nexperiments or computations, a justified true hypothesis becomes instructive knowledge, expanding Mmem\\nt in a way\\nthat rapidly minimizes Dmin\\nK,Θ(Mmem\\nt). Hence, generating and testing high-value hypotheses can quickly promote\\nknowledge discovery and increase IQagent\\nt . In this scenario, the agent employs the learning function, L, to process\\nobservations from hypothesis testing, ot, into knowledge and update its mental states Mt.\\nGenerating physically meaningful hypotheses is a key step. The agent typically uses LLMs along with collaborative\\narchitectures and domain knowledge for hypothesis generation [ 878]. Si et al. [ 742] conducted a large-scale human study\\ninvolving over 100 NLP researchers, and found that LLM-generated ideas were rated as more novel ( p <0.05) than\\nhuman expert ideas, albeit slightly weaker in feasibility. Ghafarollahi et al. [ 743] developed SciAgents, which generates\\n124\\nand refines materials science hypotheses to elucidate underlying mechanisms, design principles, and unexpected\\nproperties of biologically inspired materials. Based on large-scale ontological knowledge graphs, SciAgents samples a\\nviable path between concepts of interest, formulates a pertinent hypothesis, and expands it into a full research proposal\\nwith detailed hypothesis-testing methods and criteria. It employs two dedicated agents to review, critique, and improve\\nthe proposed hypothesis, but does not include the step of hypothesis testing through actual experiments. Similarly, Su et\\nal. [879] and Baek et al. [ 880] proposed leveraging teamwork—such as collaborative discussions and agent critics—to\\nproduce novel and effective scientific hypotheses. In addition, Gower et al. [ 881] introduced LGEM+, which utilizes a\\nfirst-order logic framework to describe biochemical pathways and generate 2,094 unique candidate hypotheses for the\\nautomated abductive improvement of genome-scale metabolic models in the yeast S. cerevisiae .\\nHypotheses only become knowledge after being justified through computational or experimental observations.\\nLu et al. [ 745] introduced the AI Scientist, a system designed for fully automated scientific discovery. The AI Scientist\\ncan conduct research independently and communicate its findings, as demonstrated in three machine learning subfields—\\ndiffusion modeling, transformer-based language modeling, and learning dynamics. It generates original research\\nideas, writes code, performs computational experiments, visualizes results, drafts complete scientific papers, and even\\nsimulates a peer review process for evaluation. For instance, it proposed the hypothesis that “adaptive dual-scale\\ndenoising can improve diffusion models by balancing global structure and local details in generated samples,” which\\nwas justified through image generation tests on four 2D datasets. Similarly, Schmidgall et al. [ 746] developed the\\nAgent Laboratory to autonomously carry out the entire research process, including literature review, computational\\nexperimentation, and report writing. They evaluated Agent Laboratory’s capability for knowledge discovery by\\naddressing five research questions in computer vision and natural language processing, achieving an average human-\\nevaluated experiment quality score of 3.2 out of 5. In addition, Tiukova et al. [ 744] developed Genesis, an automated\\nsystem capable of controlling one thousand µ-bioreactors, performing mass spectrometry characterization, accessing a\\nstructured domain information database, and applying experimental observations to improve systems biology models.\\nGenesis can initiate and execute 1,000 hypothesis-driven closed-loop experimental cycles per day. Using a similar\\napproach, the Genesis team has advanced the yeast ( S. cerevisiae ) diauxic shift model, outperforming the previous\\nbest and expanding its knowledge by 92 genes (+45%) and 1,048 interactions (+147%) [ 882]. This knowledge also\\nadvances our understanding of cancer, the immune system, and aging. Similarly, Gottweis et al. [ 749] introduced the\\nAI co-scientist, which autonomously generates and refines novel research hypotheses, with in vitro validation in three\\nbiomedical areas: drug repurposing, novel target discovery, and mechanisms of bacterial evolution and antimicrobial\\nresistance.\\nDiscovered knowledge enhances the agent’s mental states, such as Mmem\\nt,Mwm\\nt, and Mrew\\nt.Tang et al. [ 747]\\ndeveloped ChemAgent, which improves chemical reasoning through a dynamic, self-updating memory, Mmem\\nt.\\nChemAgent proposes hypothetical answers to chemistry questions in a development dataset, evaluates them against the\\nground truth, and simulates the hypothesis-testing process used in real-world research. Correct answers are then stored\\nas knowledge in its memory to support future chemistry question answering. This self-updating memory resulted in\\nperformance gains of up to 46% (with GPT-4) when ChemAgent was applied to four chemical reasoning datasets from\\nSciBench [ 883]. Wang et al. [ 884] introduced Molecular Language-Enhanced Evolutionary Optimization (MOLLEO),\\nwhich iteratively proposes hypotheses for modifying candidate drug molecules in Mmem\\nt, evaluates their drug-likeness\\nand activity, and updates the candidates in Mmem\\nt to enhance drug discovery. Similarly, Jia et al. [ 885] developed\\nLLMatDesign, which employs hypothesis-guided structure generation and a self-updating Mmem\\nt to design inorganic\\nphotovoltaic materials, whose ideality is defined by matching the target band gap and having the most negative formation\\nenergy.\\nSim et al. [ 748] introduced ChemOS 2.0, which orchestrates closed-loop operations in chemical self-driving laboratories\\n(SDLs). ChemOS 2.0 integrates ab initio calculations, experimental orchestration, and statistical algorithms for the\\nautonomous discovery of high-performance materials. A case study on discovering organic laser molecules demonstrates\\nits capabilities. It employs a Bayesian optimizer, Altas, as its world model Mwm\\ntto predict the optical properties of\\nhypothetical molecules—specifically Bis[(N-carbazole)styryl]biphenyl (BSBCz) derivatives—including gain cross\\nsection and spectral grain factor. Based on these predictions, ChemOS 2.0 recommends molecules with a higher\\nprobability of success in the experimental campaign. It then utilizes an optical characterization platform and the AiiDA\\nsoftware package to measure and simulate the properties of test molecules. The results are used to update Mwm\\nt,\\nimproving the accuracy of future experimental predictions.\\nHysmith et al. [ 886] published a perspective highlighting the crucial role of reward function design in developing\\nforward-looking workflows for SDLs. Agents can be highly effective at solving POMDP problems in simulated\\nenvironments, such as computer games or simulations, but often struggle with real-world applications. A well-defined\\nreward function is essential for iterative self-evolution. However, in many real-world scientific research problems,\\nreward functions are ill-defined or absent at the end of experimental campaigns due to the lack of direct measurements,\\n125\\nthe complexity of experimental results, and the need to balance multiple objectives. The discovery of new knowledge\\ncan serve as a valuable resource for refining Mrew\\nt, guiding hypothesis exploration and experimental data collection.\\n12.2.2 Protocol Planning and Tool Innovation\\nThe capability to plan experimental protocols and optimize tool usage enables the agent to solve complex scientific\\npuzzles within the autonomous discovery loop. As introduced in Section 9.4, the agent can systematically evaluate and\\nrefine its approach to selecting, invoking, and integrating available tools—and even develop new tools tailored to specific\\ntask requirements. While optimized protocols and tool usage do not directly reduce DK(θ, Mmem\\nt), they enhance\\nexecution efficiency and effectiveness in refining the probability distribution of unknown information, Pθ(xU|Mmem\\nt),\\nthereby accelerating knowledge discovery. In this scenario, the agent leverages the reasoning function Rto translate its\\nevolving mental states Mt, continuously updated with new knowledge, into real-world actions atfor more effective and\\nfaster hypothesis testing (Figure 12.2).\\nScheduling and orchestrating the selection and recombination of existing tools is critical. Scientific experiments\\ntypically depend on diverse instruments for analyzing reaction products, with decisions rarely rely on just one\\nmeasurement. Effectively utilizing necessary instruments without wasting resources and time requires the agent to\\nlearn to use tools in an integrated and adaptive manner. Dai et al. [ 750] designed a modular workflow that integrates\\nmobile robots, an automated synthesis platform, and various characterization instruments for autonomous discovery.\\nThey exemplified this system across three domains: structural diversification chemistry, supramolecular host-guest\\nchemistry, and photochemical synthesis. The mobile robot follows a synthesis-analysis-decision cycle to mimic human\\nexperimental strategies, autonomously determining subsequent workflow steps. It selects appropriate instruments, such\\nas the Chemspeed ISynth platform for synthesis, a liquid chromatography-mass spectrometer (UPLC-MS) for measuring\\nmass spectra corresponding to chemical peak signals, and a benchtop nuclear magnetic resonance spectrometer (NMR)\\nfor tracking chemical transformations from starting materials to products.\\nBeyond individual laboratories, tool orchestration is essential for delocalized and asynchronous scientific discovery.\\nStrieth-Kalthoff et al. [ 751] demonstrated a closed-loop integration of five materials science laboratories across\\nthree continents, advancing delocalized and democratized scientific discovery. These five laboratories have varying\\nstrengths—for example, the University of British Columbia specializes in continuous preferential crystallization, while\\nKyushu University excels in thin film fabrication and characterization. Strieth-Kalthoff et al. employed a cloud-based\\nexperiment planner to continuously learn from the incoming data and effectively prioritize informative experiments\\nacross the five laboratories, resulting in the discovery of 21 new state-of-the-art materials for organic solid-state lasers.\\nMoreover, the agent can optimize existing tools and even create new ones to enhance its capabilities. Swanson et\\nal. [752] developed the Virtual Lab, an AI-driven research environment that facilitated the design and experimental\\nvalidation of new SARS-CoV-2 nanobodies. Within the Virtual Lab, AI agents conduct scientific discussion in team\\nmeetings and execute specialized tasks in individual sessions. One key agenda for the agents was developing tools to aid\\nin the design of nanobody binders [ 887], including: (1) a sequence analysis tool that ranks candidate point mutations\\nusing log-likelihood ratios from the ESM protein language model [ 888]; (2) a structure evaluation tool that extracts\\ninterface pLDDT scores from AlphaFold-Multimer predictions [ 889], offering a proxy for antibody-antigen binding\\naffinity; and (3) an energy estimation tool built on Rosetta [ 890] to quantify binding strength between nanobody variants\\nand the spike protein. These agent-generated tools enabled the Virtual Lab to discover two novel nanobodies with\\nenhanced binding to the JN.1 or KP.3 SARS-CoC-2 variants, while preserving strong affinity for the ancestral viral\\nspike protein.\\n12.2.3 Data Analysis and Implication Derivation\\nAlthough most knowledge discovery processes rely on generating hypotheses and testing them in the real world—where\\nobservations otare essential—a significant portion of knowledge can be derived purely through internal actions such\\nas iterative reasoning and deep thinking, which are common in theoretical disciplines. For example, all theorems in\\nEuclidean geometry can be deduced from just five axioms, but these theorems do not explicitly exist in the mental\\nstate before they are derived. Given all necessary premises, such as Euclid’s five postulates, the true probability of a\\nhypothesis may remain elusive. However, using deductive and inductive reasoning to draw implications from known\\npremises and data can help either justify or falsify hypotheses, thus reducing DK(θ, Mmem\\nt)and enhancing IQagent\\nt\\n(Figure 12.2). In this scenario, the agent employs the cognition function Cto use prior mental states Mt−1and internal\\nactions atto derive new knowledge and update mental states to Mt.\\nDeductive reasoning enables knowledge derivation through logic. Trinh et al. [ 753] developed AlphaGeometry\\nfor the forward deduction of new mathematical theorems based on existing theorems in Euclidean plane geometry.\\nAlphaGeometry employs a neural language model to construct auxiliary points in plane geometry problems and\\n126\\nintegrates specialized symbolic engines to exhaustively deduce new true statements, thereby expanding the joint closure\\nof known truths. By leveraging this expanded closure, it alternates between auxiliary constructions and symbolic\\nreasoning engines to uncover further implications. AlphaGeometry demonstrated remarkable performance on a test\\nset of 30 recent Olympiad-level problems, solving 25—more than double the 10 problems solved by the previous best\\nmethod—and coming close to the level of an average International Mathematical Olympiad (IMO) gold medalist.\\nInductive reasoning enables knowledge derivation through pattern recognition and statistical learning. Liu et al.\\n[754] introduced the Team of AI-made Scientists (TAIS) to simulate the role of a data scientist for streamlined data\\nanalysis. TAIS decomposes a complex data analysis problem into different computational tasks, including coding,\\nself-critique, and regression analysis, to extract meaningful insights from complex datasets. When applied to identifying\\ndisease-predictive genes, TAIS achieved an overall success rate of 45.73% on a benchmark dataset containing 457\\ngenetic questions. Ideally, the extracted insights should be logically sound; otherwise, they must be discarded to\\nensure only accurate findings are safely integrated into mental states. However, limitation in data coverage and the\\nimplementation of analysis algorithms may lead to hallucinated insights, underscoring the need for reliable data\\nanalyzers and reasoning tools to prevent over-analysis.\\n12.3 Technological Readiness and Challenges\\nThe self-evolution of agents, which in turn drives the advancement of human knowledge, is promised by their early\\nsuccess in the innovation cycle. This cycle involves generating meaningful hypotheses, designing real-time testing\\nprotocols, coordinating various experimental and computational tools, analyzing data, deriving implications, and\\nengaging in self-reflection. However, achieving fully autonomous self-evolution remains a significant challenge, given\\nthe current technology readiness levels (TRLs) of three fundamental capabilities: real-world interaction, complex\\nreasoning, and the integration of prior knowledge. Further technological progress is required to improve the cycle of\\nself-driven innovation.\\n12.3.1 Real-World Interaction Challenges\\nAgents interact with the real world primarily through application programming interfaces (APIs). While numerous\\ndemonstrations [ 891] have shown their strong capability to use various APIs, a significant bottleneck in autonomous\\nknowledge discovery remains: the lack of APIs that allow agents to directly execute tasks in a physical laboratory.\\nPhysical APIs—interfaces that enable direct control of lab equipment—are far less abundant than computational\\nAPIs due to the significant investment of time, expertise, and cost required to develop them. Although existing\\nautonomous laboratories have shown promise, they remain in an early developmental stage (typically TRL 4–6), where\\nstraightforward replication or scale-up is challenging. Consequently, building further systems or broadening their\\napplication across additional scientific domains still requires substantial customization to address domain-specific needs,\\nalong with specialized expertise.\\nTwo key tasks are essential for enabling real-world interaction: operating lab devices andtransferring samples between\\ndevices . Seamless integration of physical hardware and experimental samples is crucial to maintaining uninterrupted\\nworkflows. However, most experimental instruments are originally designed for human operation. Making them\\naccessible to agents requires extensive efforts across multiple disciplines, including robotics, electrical engineering,\\nmechanical engineering, and software programming. The rising prominence of SDLs is catalyzing the transformation\\nof human-operated devices into agent-accessible systems through APIs. In autonomous labs conducting complex\\nexperiments, two parallel and often complementary approaches are commonly adopted to integrate hardware with\\nagentic systems. Both approaches are modular, reconfigurable, and valuable, yet they require ongoing, dedicated\\ndevelopment.\\nApproach 1: API Integration via Direct Device Adaptation. This approach involves equipping individual devices\\nwith dedicated mechanical adaptations and I/O controllers, enabling them to receive and execute commands from a\\ncentral control PC. For example, to achieve solid-state synthesis and structural characterization of inorganic materials,\\nA-lab has implemented 16 types of devices to automate experimental tasks such as powder dosing, heating, and\\ndiffraction [ 892]. This approach allows laboratories to function as fully integrated entities by maximizing device\\nutilization, optimizing space and resources, and enabling bespoke tools. However, it is costly, time-consuming, and\\nrequires expert knowledge to prototype or retrofit devices for automation. Large language models (LLMs) have been\\napplied to facilitate access to diverse tools, as illustrated by CACTUS, a Chemistry Agent Connecting Tool-Usage to\\nScience [893].\\nA more accessible alternative for small teams is the cloud lab orscience factory [894], where responsibility for\\ndevice engineering shifts from individual laboratories to dedicated user facilities or commercial service providers. For\\n127\\ninstance, Boiko et al. [ 895] demonstrated an autonomous chemical research agent, Coscientist, capable of carrying out\\ncross-coupling Suzuki and Sonogashira reactions using experimental setups at the Emerald Cloud Lab [ 896]. However,\\ncloud labs offer only a fixed set of pre-built devices optimized for common procedures, posing potential challenges\\nfor researchers whose experiments require equipment customization, as integrating non-standard tools may involve a\\nlengthy process of negotiation and development.\\nApproach 2: Robotic Operation of Experimental Devices. This approach involves using mobile robots or robotic\\narms to operate existing devices and transfer samples. In many cases, robots can interact with instruments without\\nmodification, apart from minor adjustments such as adding specialized actuators, grippers, or holders. For example, Dai\\net al. [ 750] employed mobile robots to explore synthetic chemistry. In their autonomous laboratory, mobile robots enable\\nphysical linkages between synthesis and analysis devices that are spatially separated, automating sample transportation\\nand handling. In principle, the robots can perform all actions human researchers require in the laboratory. However,\\ncurrent robotic systems still rely on human pre-programming to map the lab layout, define movement trajectories, and\\nregister device positions. Handling unexpected or adaptive situations remains a challenge, as pre-programming cannot\\nanticipate every possible state of an experimental setup. Real-time learning and adaptive manipulation are active areas\\nof research that require further technological advancements. In the long term, embodied AI [ 897] is expected to enhance\\nrobotic learning, allowing agents to quickly adapt to new environments and tools.\\nThe two approaches can be combined. For example, Vescovi et al. [ 894] define a modular laboratory robotics\\narchitecture that allows for translating high-level commands into specific operations for a variety of different robotic\\napparatus and laboratory equipment, and for linking robotic apparatus with other elements of an AI-driven discovery\\narchitecture, such as high-performance computing [ 898]. This architecture has been used to automate experiments\\nin both the biological and physical sciences [ 899]. Similarly, Fernando et al. [ 900] integrate a Robotic Operating\\nSystem 2 (ROS2) compatible robot into the Bluesky experimental orchestration framework. Lo et al. [ 901] argue for\\nthe development and integration of low-cost “frugal twins” of more expensive equipment to facilitate experimentation\\nand democratize access.\\n12.3.2 Complex Reasoning Challenges\\nA fundamental philosophical question is whether agents, often powered by LLMs, can truly perform reasoning. By\\ndefinition, languages models generate outputs by predicting the next token, a mechanism fundamentally different\\nfrom human reasoning. From an outcome-driven perspective, these input-output systems exhibit reasoning ability\\nphenomenologically, as they produce meaningful outputs compared to a reference system generating arbitrary responses\\n[902]. However, regardless of the perspective taken, this capability remains imperfect—particularly when handling\\ncomplex logical and numerical problems, which are crucial for scientific knowledge discovery.\\nAgents and LLMs struggle with hard reasoning tasks. Glazer et al. [ 903] introduced FrontierMath, a benchmark\\ncomprising hundreds of original and challenging mathematics problems covering most major branches of modern\\nmathematics. Evaluation of state-of-the-art LLM-driven agents—including o1-preview (OpenAI), o1-mini (OpenAI),\\nGPT-4o (OpenAI, 2024-08-06 version), Claude 3.5 Sonnet (Anthropic, 2024-10-22 version), Grok 2 Beta (XAI),\\nand Gemini 1.5 Pro 002 (Google DeepMind)—revealed that no model achieved even a 2% success rate on the full\\nbenchmark. Chen et al. [ 873] presented ScienceAgentBench, a benchmark designed to evaluate language agents in\\ndata-driven scientific discovery. Among 102 tasks derived from 44 peer-reviewed publications across four disciplines,\\nOpenAI o1 successfully solved only 42.2% of them. Chollet [ 865] proposed the Abstraction and Reasoning Challenge\\n(ARC) to assesss LLMs’ ability to perform abstract inductive reasoning without relying on memorization or external\\nknowledge. Even with careful prompting, GPT-4o correctly solved only 19% of the tasks, far below the ∼75% average\\nhuman performance [ 904,905]. Zhu et al. [ 906] suggested a four-level classification of AI intelligence, including L1\\n(arbitrating isputes), L2 (auditing a review), L3 (reviewing a paper), and L4 (authoring a paper). They classify the\\ncurrent state-of-the-art LLM-driven agents as approaching L2-level capabilities. To enhance agents’ reasoning abilities,\\nresearchers have introduced techniques such as chain-of-thought [ 907], tree-of-thoughts [ 72], and [ 70]. Although new\\nmethods continue to emerge, as discussed in Section 2.2, further advancements in reasoning capacity remain crucial for\\nachieving reliable causal inference in scientific research.\\nAgents and LLMs also struggle with quantitative and symbolic problems. For example, GPT-4 and GPT-3.5 often\\nstruggle with reliably performing complex arithmetic such as multiplying 12,345×98,765, or translating IUPAC\\nchemical names into accurate molecular graphs [ 908,697]. A common approach to overcoming these limitations\\nis to use external tools rather than relying on the LLM itself for reasoning. In mathematical problem-solving, for\\nexample, tools like symbolic solvers are preferred over direct LLM inference [ 753]. However, this mitigation does\\nnot resolve the intrinsic deficiency in numerical understanding, which poses a potential risk to scientific reasoning.\\nMoreover, Yu et al. [ 909] found that tool-augmented LLMs do not consistently outperform base LLMs without tools in\\nchemistry problem-solving. For instance, for specialized chemistry tasks, such as synthesis prediction, augmenting\\n128\\nLLMs with specialized tools can boost the performance substantially; however, tool augmentation is less effective for\\ngeneral chemistry questions, such as those in exams, where no specific tools can directly solve a given question. In\\nthese scenarios, an agent’s ability to reason correctly by using multiple pieces of chemistry knowledge becomes more\\nimportant.\\nThe preceding discussion emphasizes the importance of developing robust methodologies for evaluating AI agents as\\nscientific research assistants, a topic discussed at length by Cappello et al. [910].\\n12.3.3 Challenges in Integrating Prior Knowledge\\nPrior knowledge is a crucial factor for higher intelligence. As discusses in Section 12.1, the agent’s prior knowledge,\\nMmem\\nt, helps decrease DK(θ, Mmem\\nt)and increase the agent’s intelligence, IQagent\\nt . Human-led scientific discoveries\\nfrequently achieve breakthroughs with relatively small datasets, thanks to the vast prior knowledge humans possess. The\\nstart-of-the-art LLMs that power autonomous agents are trained on nearly all publicly available textual data, including\\nwebsites, books, and other sources, thereby encompassing most common knowledge as well as publicly accessible\\nspecialized knowledge. However, achieving an agent that can seamlessly integrate all existing human knowledge\\nremains a significant challenge.\\nAt least three types of knowledge sources may not be included in LLM pre-training: (1) Paywalled or unpublished\\nknowledge, including non-open-access publications, industry-specific data, and failed experiments [ 911]. They are\\noften not accessible to public models despite their potential value in refining domain-specific insights. (2) Empirical\\nknowledge. Heuristic decisions by experts are often effective, particularly in scenarios where no existing data is\\navailable for a new problem. However, large amounts of expert heuristics are typically not accessible as textual data. (3)\\nContextual or situational knowledge. Knowledge related to real-world conditions, such as safety protocols in chemical\\nreactions or equipment handling, is often absent from pre-trained models but is essential for practical applications.\\nAdditionally, integrating diverse knowledge sources presents challenges in reconciling conflicting information. For\\nexample, OpenAI’s Deep Research [ 912] actively gathers online information and performs multi-step reasoning,\\nachieving state-of-the-art performance on Humanity’s Last Exam and the GAIA benchmark. However, it still struggles\\nto distinguish between authoritative information and rumors and exhibits limitations in confidence calibration, often\\nmisrepresenting its level of certainty [ 912]. Establishing a system to assess the levels of evidence [ 913] of different\\nknowledge fragments—such as quantifying reliability and verifying references—may be necessary for effective\\nknowledge fusion.\\n129\\nPart III\\nCollaborative and Evolutionary Intelligent\\nSystems\\n130\\nThe concepts of collaboration andevolution lie at the heart of intelligent multi-agent systems (MAS). Inspired by\\nbiological ecosystems and human societal dynamics, these systems leverage collective intelligence to solve complex\\nchallenges that exceed the capabilities of individual agents [ 914]. Human societies exemplify how cooperation,\\nspecialization, and distributed decision-making significantly enhance collective problem-solving effectiveness. Similarly,\\nMAS adopts these strategies, integrating specialized agents to address intricate tasks collaboratively. The foundational\\nprinciple of collective intelligence – the “Wisdom of Crowds” by [ 915] – suggests diverse, independent agents often\\nyield superior decisions compared to solitary experts, directly underpinning the design philosophy of MAS. Cognitive\\ntheories, such as Minsky’s society of mind [ 17] and the theory of mind [ 916,917], further reinforce this paradigm by\\nproposing that intelligence emerges from structured interactions among specialized units.\\nRecently, advancements in large language models (LLMs) have introduced new possibilities for collaborative and\\nevolutionary multi-agent systems (LLM-MAS). Benefiting from powerful reasoning, planning, and decision-making\\ncapabilities, these models enable the creation of sophisticated MAS architectures mirroring the cooperative and\\nadaptive characteristics found in human societies. Agents within LLM-MAS often assume distinct identities and\\nroles, reflecting human-like division of labor and specialized collaboration. By embracing structured communication,\\ndynamic knowledge sharing, and coordinated decision-making, these systems emulate human social dynamics to\\nachieve common goals. Moreover, LLM-MAS is inherently evolutionary; agents continuously adapt and improve\\nthrough interactions, feedback, and iterative learning, resulting in enhanced system performance over time. Roadmap\\nIn this chapter, we systematically survey the emerging field of LLM-based multi-agent systems, focusing specifically\\non their collaborative mechanisms and evolutionary capabilities. We first examine how distinct system objectives\\nshape agent roles, behavior patterns, and collaborative strategies in Chapter 13. Next, in Chapter 14, we analyze\\nvarious communication structures, including interaction protocols that facilitate effective agent-agent and human-agent\\ncommunication. Additionally, we explore collaborative decision-making methodologies and how agents leverage their\\nunique expertise and perspectives in Chapter 15, and discuss the collective intelligence and evolution mechanism\\nin Chapter 16. Finally, in Chapter 17, we discuss evolutionary processes, highlighting adaptive learning methods,\\ncontinuous knowledge sharing, and mechanisms for iterative improvement that collectively enhance MAS performance.\\nThrough this comprehensive survey, we identify current achievements, discuss existing challenges, and highlight\\npromising research directions for collaborative and evolutionary intelligent systems.\\n131\\nLLM-based MASApplicationStrategic LearningRECONCILE [ 918] LLM-Game-\\nAgent [ 919] BattleAgentBench [ 920]\\nModeling and\\nSimulationGenerative Agents [ 50] Agent hospi-\\ntal [921] MedAgents [ 922] MEDCO [ 923]\\nCollaborative\\nTask SolvingMetaGPT [ 626] ChatDev [ 627] Agent\\nLaboratory [ 746] The virtual lab [ 752]\\nComposition\\nand ProtocolAgent Composition HomogeneousCoELA [ 924] VillagerAgent [ 925]\\nLLM-Coordination [ 926]\\nHeterogeneousMetaGPT [ 626] ChatDev [ 627] Gen-\\nerative Agents [ 50] S-Agents [ 927]\\nInteraction\\nProtocolsMessage TypesSciAgents [ 743] AppA-\\ngent [ 636] MetaGPT [ 626]\\nCommunication\\nInterfacesAgentBench [ 706] V AB [ 928]\\nTaskWeaver [ 929] HULA [ 930]\\nNext Genera-\\ntion ProtocolMCP [ 931] Agora [ 932] IoA [ 933]\\nTopology Static TopologyMEDCO [ 923] Agent hospital [ 921] Wel-\\nfare Diplomacy [ 934] MedAgents [ 922]\\nDynamic Topology DyLAN [ 725] GPTSwarm [ 651] CodeR [ 935] Oasis [ 936]\\nCollaborationAgent-Agent\\nCollaborationConsensus-orientedAgent Laboratory [ 746] The\\nvirtual lab [ 752] OASIS [ 936]\\nCollaborative\\nlearningGenerative Agents [ 50]\\nWelfare Diplomacy [ 934]\\nLLM-Game-Agent [ 919]\\nBattleAgentBench [ 920]\\nTeaching/MentoringMEDCO [ 923]\\nAgent Hospital [ 921]\\nTask-oriented MedAgents [ 922] S-Agents [ 927]\\nHuman-AI\\nCollaborationDittos [ 937] PRELUDE [ 938]\\nEvolutionCollective\\nIntelligenceGenerative Agents [ 50] Welfare Diplomacy [ 934]\\nLLM-Game-Agent [ 919] BattleAgentBench [ 920]\\nIndividual\\nAdaptabilityAgent Hospital [ 921] Agent Lab-\\noratory [ 746] MEDCO [ 923]\\nEvaluationBenchmark for\\nspecific tasksMBPP [ 939] HotpotQA [ 940] MATH [ 941]\\nSV AMP [ 942] MultiArith [ 943]\\nBenchmark\\nfor MASCollab-Overcooked [ 944] REALM-Bench [ 945]\\nPARTNR [ 946] VillagerBench [ 925] Au-\\ntoArena [ 947] MultiagentBench [ 948]\\nFigure 12.3: Taxonomy of LLM-based Multi-Agent Systems.\\n132\\nChapter 13\\nDesign of Multi-Agent Systems\\nIn the context of LLM-based multi-agent systems (LLM-MAS), collaboration goals andcollaboration norms serve\\nas foundational elements that shape system behavior, interaction patterns, and overall effectiveness. Collaboration\\ngoals specify the explicit objectives agents aim to achieve – whether individually, collectively, or competitively –\\nwhile collaboration norms define the rules, constraints, and conventions that govern agent interactions within the\\nsystem. Together, these components establish a robust framework guiding effective communication, coordination, and\\ncooperation among agents.\\nThis section categorizes LLM-MAS into three broad classes based on distinct combinations of collaboration goals\\nand norms: strategic learning ,modeling and simulation , and collaborative task solving . Although not exhaustive,\\nthese categories cover a wide spectrum of LLM-MAS designs and clearly reflect how system objectives shape agent\\ninteractions and outcomes.\\n•Strategic Learning systems embed agents within a game-theoretic context, where agents pursue individual or\\npartially conflicting goals. The interactions can be cooperative, competitive, or mixed, guided explicitly by\\npredefined game rules and interaction norms. This setting often aligns with non-cooperative (strategic) and\\ncooperative concepts in traditional game theory. Please refer to Section 13.1 for details.\\n•Modeling and Simulation contexts focus on agents acting independently, driven by diverse environmental\\nor social factors. Here, interactions emerge organically without necessarily converging on common goals,\\nreflecting the complex dynamics seen in large-scale social or economic simulations. Please refer to Section 13.2\\nfor details.\\n•Collaborative Task Solving emphasizes systematic cooperation among agents to achieve explicitly shared\\nobjectives. Agents typically adopt structured workflows, clear role definitions, and highly predefined collabo-\\nration norms to synchronize their actions toward collective goals. Please refer to Section 13.3 for details.\\nIn the remainder of this chapter, we elaborate on each category, examining how LLMs enable, influence, and enhance\\nagent behaviors, interactions, and collective intelligence within our scope.\\nIn the following, we examine these categories in detail, highlighting how each leverages the capabilities of large\\nlanguage models to shape agent behaviors and interactions.\\n13.1 Strategic Learning: Cooperation vs.Competition\\nStrategic learning refers to agents’ capabilities to dynamically anticipate, interpret, and influence the actions of other\\nagents within game-theoretic settings—whether competitive, cooperative, or mixed [ 949]. Agents iteratively adjust their\\nstrategies based on new information, commonly modeled using foundational concepts such as Nash equilibria [ 950],\\nBayesian games [ 951,914,952], or repeated interactions [ 953,954]. With LLMs enabling nuanced linguistic reasoning,\\nstrategic learning increasingly integrates “soft” signals – including dialogue, persuasion, and implicit negotiation – thus\\nenriching traditional game-theoretic reasoning frameworks [952, 955, 956, 957].\\nIn economic applications, multi-agent strategic simulations provide valuable insights into market behaviors and\\nnegotiation tactics, highlighting both competitive and cooperative dynamics. For example, [ 958] and [ 951] demonstrate\\nhow LLM-empowered agents can simulate hiring processes, exhibit rational decision-making in controlled economic\\n133\\nLLM-Based Multi-Agent Systems\\n(Collaboration Goals &\\nCollaboration Norms)\\nModeling & Simulation\\n•Agents act largely\\nindependently\\n•Heterogeneous behav-\\niors and states\\n•Emergent social, eco-\\nnomic, or political\\nphenomenaStrategic Learning\\n•Divergent or conflict-\\ning goals\\n•Competitive & cooper-\\native game rules\\n•Dynamic adaptation\\nand anticipatory strate-\\ngiesCollaborative Task Solving\\n•Shared goals & struc-\\ntured workflows\\n• Clear role assignment\\n•Multi-round coopera-\\ntion and coordination\\nFigure 13.1: An overview of three major collaboration types in LLM-based MAS: Modeling & Simulation ,Strategic\\nLearning , and Collaborative Task Solving . Each category is distinguished by how agents’ goals and norms are set\\n(independent vs. divergent vs. shared) and how they coordinate.\\nexperiments, and even forecast stock movements. [ 959] introduces a GPT-4-based competitive environment to illustrate\\nhow restaurant and customer agents compete to optimize profits and satisfaction, showcasing realistic bidding and\\npricing strategies. Meanwhile, [960] investigate Buyer–Seller bargaining in LLM-based negotiations, while [961] use\\nultimatum game simulations to illuminate policymaking decisions grounded in human-like strategic behavior.\\nBeyond conventional markets, strategic learning applies broadly wherever resource allocation, alliances, or competitive-\\ncooperative trade-offs are present. Examples include multi-commodity competitions [ 962,959], in which agents\\nstrategically negotiate terms to maximize individual benefits, or sustainability-focused contexts where agents coordinate\\nresource consumption [ 963]. In gaming, social deduction games such as Werewolf, Chameleon, Avalon, and Jubensha\\nrequire agents to manage the complex interplay between deception and collaboration [ 964,965,966,153,919,967,968,\\n969,970]. Studies by [ 971,965] highlight LLM-based agents that excel at orchestrating subtle deceit and collaboration,\\nwhile [ 967,972,968,969] emphasize adaptive, multi-round strategy in Avalon. [ 970] further pushes this boundary by\\nshowcasing autonomous, multi-agent interactions in the Jubensha murder mystery genre, re-creating complex narratives.\\nSimilarly, diplomatic simulations ([ 973] and [ 974]) employ LLM-based agents to emulate sophisticated geopolitical\\nnegotiation and alliance formation dynamics at global scales.\\nSummary A key advantage of LLM-driven strategic learning lies in effectively combining rigorous game-theoretic logic\\nwith natural language reasoning. This fusion enables agents to interpret sophisticated instructions, engage in persuasive\\ndialogue, and adapt more flexibly to novel or unstructured settings. Consequently, LLM-based strategic agents hold\\nsignificant promise for accurately modeling complex real-world interactions – spanning economic competition, social\\nnegotiation, and geopolitical strategy – far more effectively than conventional rule-based or numeric-only approaches.\\n13.2 Modeling Real-World Dynamics\\nModeling and simulation represents another crucial area of application for LLM-based multi-agent systems (LLM-\\nMAS), aiming to replicate complex social, economic, and political phenomena at scale. By utilizing LLMs’ sophisticated\\nlanguage understanding and contextual reasoning, these simulations can feature highly heterogeneous agents whose\\nevolving behaviors mirror real-world dynamism. Unlike strategic learning environments that emphasize explicit\\ncompetitive or cooperative goals, agents in modeling and simulation scenarios operate independently, guided by their\\ndomain-specific roles, preferences, and interactions with the simulated environment [975].\\nIn healthcare, for example, [ 921] introduces Agent Hospital , where LLM-powered doctor agents iteratively refine\\ntreatment strategies through realistic interactions with virtual patients. This enables researchers to test management\\nprotocols, training paradigms, and “what-if” scenarios in a controlled yet realistic setting. Similarly, in economic\\ncontexts, [ 976] present EconAgents , leveraging LLM-driven agents to realistically model individual-level behaviors\\nsuch as employment decisions, consumption patterns, and savings strategies. These agents facilitate expressive macroe-\\n134\\nconomic simulations, surpassing traditional numeric or strictly rule-based methods in adaptability and realism [ 977].\\nIn addition, political science applications also benefit from this approach. For example, [ 978] and [ 977] successfully\\nsimulate election processes and policymaking dynamics, revealing how public discourse, candidate strategies, and voter\\ninteractions shape real-world political outcomes.\\nBeyond economics and politics, LLM-based simulation accommodates a variety of social and cultural phenomena. For\\nexample, [ 979] and [ 255] use simulations of linguistic and emotional propagation in social networks to investigate how\\nopinions, beliefs, or sentiment clusters form online. Research by [ 980] explores how opinion dynamics evolve under\\nvarious topological and interaction patterns, while [ 981] examines the conditions under which fake news spreads or\\nstalls in heterogeneous agent populations. Large-scale simulation platforms such as GenSim [ 982] and OASIS [ 936]\\npush the boundary further by scaling to tens of thousands or even millions of user agents, thus enabling the study of\\nemergent group behaviors and systemic effects—such as viral information diffusion, echo-chamber formation, or group\\npolarization—under realistic constraints.\\nSummary The strength of LLM-based simulation lies in capturing both the structural dynamics (e.g., network topology\\nor institutional rules) and the cognitive or linguistic nuances that drive real-world behavior. By embedding language-\\nbased reasoning into agent models, researchers can examine complex social processes—like persuasion, framing, or\\ncultural transmission—that would be difficult to capture through purely numeric or rule-based approaches.\\n13.3 Collaborative Task Solving with Workflow Generation\\nCollaborative task solving orchestrates multiple agents toward a clearly defined objective through structured workflows.\\nIn contrast to strategic learning (which may involve competing interests) or open-ended modeling and simulation\\n(where agents act independently), collaborative agents function as part of a unified problem-solving pipeline. Agents\\ntypically follow clearly defined roles (e.g., “Planner”, “Implementer”, or “Evaluator”) and stage-based processes to\\nensure efficient and accurate task completion.\\nSystems such as MetaGPT [ 626], CAMEL [ 848], Communicative Agents [ 983], and frameworks described in [ 924]\\nexemplify how clearly defined roles, responsibilities, and decision flows allow LLM-based agents to coordinate\\neffectively. A typical workflow might involve one agent analyzing a problem statement, another proposing a solution\\noutline, a third implementing partial solutions, and a fourth verifying correctness. Communication among these agents\\nis often carried out through iterative rounds of natural language “dialogue”, leveraging the inherent language-generation\\nstrengths of LLMs. This structured approach also proves beneficial for scaling to more ambitious projects, as sub-tasks\\ncan be delegated to specialized agents with domain-specific prompts or training.\\nRecently, collaborative task-solving systems have been explored extensively in software development scenarios (e.g.,\\nmulti-agent coding, debugging, and testing). However, scientific discovery represents a particularly prominent and\\ncompelling application. For example, the Agent Laboratory [746] employs agents in structured scientific workflows:\\nproposing hypotheses, designing experiments, analyzing results, and refining subsequent inquiries, which effectively\\nmirrors the iterative nature of the scientific investigation. Similar multi-agent designs can be adapted to tasks such as\\nliterature review, policy drafting, or large-scale data analysis, using well-defined protocols to maintain coherence and\\navoid duplication of effort.\\nSummary Compared to other LLM-based multi-agent paradigms, collaborative task-solving inherently prioritizes\\nclarity and predictability: Each agent’s role and objective are predefined, limiting emergent or chaotic behaviors. This\\nstructure is particularly advantageous in domains requiring precision, accountability, or sequential decision-making.\\nAt the same time, research is ongoing to strike the right balance between structure and flexibility, which ensures that\\nagents have enough autonomy to creatively contribute solutions while adhering to a shared workflow that ultimately\\nguarantees reliable, high-quality task completion.\\nDiscussion The aforementioned three dimensions— strategic learning ,modeling and simulation , and collaborative task\\nsolving —reflect the breadth of LLM-based multi-agent systems. Each category addresses distinct research questions and\\nreal-world applications, leveraging language-based reasoning to tackle challenges that extend beyond the capabilities of\\nconventional, purely numeric, or rule-driven agent designs.\\n13.4 Composing AI Agent Teams\\nIn MAS, agents are the core units that interact within the system and are critical to its functionality. These agents can be\\ncategorized as either homogeneous or heterogeneous, depending on whether they share identical or differing personas,\\ncapabilities, and action spaces.\\n135\\nHomogeneous Homogeneous agents that share identical capabilities, action spaces, and observation spaces. Compared\\nto single-agent systems, the primary advantage lies in task parallelization, allowing multiple agents to handle different\\nparts of a task simultaneously and improve overall efficiency. They are often used in simpler, coordinated tasks where\\nuniformity across agents can drive improved performance.\\nSeveral studies have applied homogeneous agents to simulate teamwork in games like Overcooked and Minecraft, as\\nwell as real-world tasks such as household labor division. [ 924] proposed a cognitive-inspired modular framework that\\nenables LLM-based agents to communicate through natural language to perform labor division, request assistance from\\none another, and collaboratively complete object transportation tasks. [ 984] introduced prompt-based organizational\\nstructures into the framework, reducing communication costs between agents and improving team efficiency in household\\ntasks such as preparing afternoon tea, washing dishes, and preparing a meal. Furthermore, several studies [ 926,925]\\nhave employed multiple LLM-based agents in popular games such as Overcooked and Minecraft to experiment with\\ntheir ability to cooperate and complete tasks. According to the game settings, these agents are also homogeneous.\\nHeterogeneous Agent diversity plays a crucial role in improving collaboration outcomes. Research shows that hetero-\\ngeneity among agents can enhance problem-solving capabilities, as diverse agents bring varied perspectives and skills to\\nthe task at hand [ 985,986]. Heterogeneity contributes to richer problem-solving strategies and improves overall collabo-\\nration in MAS. The heterogeneous characteristics of agents can be reflected in the following dimensions: personas-level\\nheterogeneity, observation-space heterogeneity, and action-space heterogeneity. Note that these heterogeneities are not\\nmutually exclusive—a heterogeneous agent may exhibit one or more of these characteristics.\\n•Personas-level heterogeneity. Refers to diversity in agent profiles, which influences how agents approach\\nproblem-solving and interact with one another. Most current LLM-based heterogeneous multi-agent systems\\nfall into this category [ 987,627,50,970]. For example, in software development, agents may take on\\npersonas such as programmers, product managers, or testers. In medical diagnostics, agents may represent\\ncardiologists, oncologists, or paediatricians, each with distinct areas of expertise. The distinct perspectives\\nand expertise of each persona contribute to more robust decision-making. While these heterogeneous agents\\nmay share the same action space—such as writing documents [ 626] (e.g., code, requirement reports, or\\ntest reports) or providing diagnostic advice [ 922]—their personas influence the outcomes of these actions,\\nwhere role-specific enhancements within multi-agent architectures have shown to significantly streamline\\nand optimize task execution. For instance, a product manager performing the action of writing a document\\nwould produce a requirements report, whereas a programmer performing the same action would produce\\nsoftware implementation code [ 626]. This diversity leads to better decision-making and innovation, especially\\nin complex, multidisciplinary tasks.\\n•Observation-space heterogeneity. In MAS, the ability of agents to perceive and interpret their environment\\ncan vary. Observation-space heterogeneity refers to these differences in what agents can observe or perceive\\nwithin their environment. For example, in the game Werewolf, some agents, like werewolves, can see the\\nidentities of their teammates, and the seer can obtain the identity of a designated player, while others, like\\nvillagers, cannot see the true identity of any player [ 971]. Similarly, in the Avalon game, different roles have\\ndistinct observation spaces [ 919,972], thus influencing the strategies and communications of the players. In\\nthese settings, each agent’s perceptual ability or observation space is directly linked to their role in the system.\\nIn a multi-agent system, this variation in what agents can observe often influences their decision-making,\\ncommunication, and coordination with other agents.\\n•Action-space heterogeneity. On the other hand, this refers to fundamental differences in the actions agents can\\nperform due to physical or functional constraints. This is particularly relevant in both virtual and physical\\nenvironments where agents may have different capabilities based on their design or purpose. In the virtual\\nenvironments of games like Werewolf [ 965,971,966] and Avalon [ 919,967], different roles have distinct\\nabilities or skills [ 971,919,972]. For example, in Werewolf, while werewolves may have the ability to\\ncommunicate secretly with each other, villagers might be limited to voting or observing only. This dynamic\\nrequires agents to collaborate based on their unique capabilities and promotes the learning of strategies such\\nas teamwork, trust, and deception in their interactions. Meanwhile, in robotics, agents may exhibit diverse\\nphysical capabilities. For instance, as described in [ 988], some robots lack mobility and can only manipulate\\nobjects, while others are specialized for movement but cannot manipulate objects. In such cases, agents with\\ndifferent action spaces must divide tasks effectively, leveraging their specific abilities to take on the parts of\\nthe task they are suited for, ultimately collaborating to complete the overall task. This type of heterogeneity\\nrequires agents to collaborate and coordinate their actions efficiently, often dividing tasks based on their\\nindividual strengths.\\nHomogeneity to Heterogeneous Evolution In some LLM-based multi-agent systems, agents have the ability to evolve\\nautonomously and continuously adapt through interactions with their environment. Due to the inherent randomness\\n136\\nin both LLM models and the environment, the evolution of these agents often follows different trajectories. This can\\nlead to heterogeneous behaviors emerging over multiple simulations, even when agents initially have homogeneous\\npersonas and action spaces. For example, as shown in [ 989], agents with identical action spaces and personas at the\\nstart developed differentiated roles after multiple rounds of interactions with the environment and other agents. Some\\nagents, for instance, specialized in food gathering, while others focused on crafting weapons. Similarly, [ 990] observed\\nthat initially homogeneous agents developed distinct language usage patterns, emotional expressions, and personalities\\nafter group interactions. These emergent behaviors demonstrate the possibility of transitions from homogeneous to\\nheterogeneous systems.\\n13.5 Agent Interaction Protocols\\nIn this section, there will initially be classification of typical kinds of messages, providing a clear view regarding\\nthe content and exchange modes for agent interactions. Next, agent-environment, agent-agent, and agent-human\\ncommunications interface designs will be addressed. Architectural issues and protocol specifications for transparent\\ninformation exchange will also be addressed. Interface standardization will have a special focus, which is essential for\\nproviding interoperability, scalability, and efficiency for multi-agent systems. The section will end with unification\\nof communication protocol discussions, where agent-environment or agent-user interacting design principles and\\nrequirements are addressed, as well as providing clarity, consistency, and functional coherence for various applications\\nfor LLM-based systems.\\n13.5.1 Message Types\\nStructured: Structured messages, either in JSON ([ 991,992]), XML ([ 993,636]), or as a code ([ 626,627,994]), are\\na crucial aspect of multi-agent system communication with LLM. The primary advantages of structured messages\\nare their syntactically and semantically defined structure, enabling unambiguous understanding and straightforward\\nparsing. With their lack of ambiguity, they facilitate unerrant information extraction and processing with much less\\noverhead on computation and greater system dependability. For example, JSON and XML can represent specific-task\\nconfiguration parameters or facilitate data exchange as a machine-readable mode, and messages written as a code can\\neven be executable several times directly, which makes workflow and automation simpler.\\nStructured messages are particularly well-suited for high-efficiency, deterministic applications. They are useful for\\nsub-task decomposition, sub-task assignment, and coordination among agents for cooperative multi-agent architecture\\nbecause they explicitly state operational commands. Moreover, as structured messages have a prescribed form, retrieving\\ndata as well as storing data is facilitated and system optimization and longitudinal analysis are also feasible.\\nUnstructured: In contrast, unstructured messages, e.g., natural text ([ 971,970,919]), visual data, e.g., images, videos,\\nand audio signals, e.g., speech, ambient sounds ([ 995,996,762]), have higher information density and representational\\ncapability. Such modalities are best suited for communication with nuanced and context-dependent information. Images,\\nfor instance, communicate spatial relationships, illumination, and facial expressions, and videos communicate dynamic\\ntemporally-organized sequences, e.g., state or behavior changes over time. Similarly, audio signals also communicate\\nnot just linguistic information but also paralinguistic information, e.g., tone, emotion, and intonation, which are critical\\nfor natural and context-aware interactions.\\nUnstructured messages are well-adapted for ambiguity tasks, as well as for complex, real-world settings. The fact that\\nthey can express abstract ideas as well as affective subtlety, or implicit contextual suggestions, makes unstructured\\nmessages well-suited for creative, as well as discovery-oriented, problem spaces. Unstructured data’s complexity,\\nhowever, calls for advanced processing techniques, for example, feature extraction based on deep learning, for one\\nto tap into their full potential. Advances with pre-trained LLMs as well as multi-modal large language models have\\nalleviated these complexities to a large extent, enabling novel applications for unstructured communication within\\nmulti-agent systems [533, 513, 997].\\nSummary: Unstructured and structured messages have complementary roles for multi-agent communication with\\nLLM-based. While structured messages offer accuracy, consistency, and computation efficiency and are appropriate for\\noperational and deterministic operations, unstructured messages offer rich, contextualized representations enabling\\nagents to negotiate vague, creative, highly dynamic situations. Together, these modes offer a foundation for adaptive,\\neffective multi-agent cooperation.\\n137\\n13.5.2 Communication Interface\\nAgent-Environment Interface LLM-based agents will typically have to act on their environment once or several times\\nin order to perform a range of operations. From the agent’s point of view, its output into the environment is something\\nthat it would prefer, e.g., a UI click, web request, or a move for a computer graphic’s character. Environments differ\\nwith regard to what actions they will accept, and so as not have its actions not get executed, the agent must find out\\nwhat actions are for a specific environment that it is acting within and perform actions that are for a specific task as\\nwell as valid for a specific environment. After the agent outputs its chosen action, the agent will have a return from\\nthe environment. It will consist of observations if successful, or a feedback on error if there was one. The agent will\\nhave to act on this feedback. There are nowadays various types of environments where an agent can act, e.g., operating\\nsystems, computer games, database, and e-commerce websites. To make agent-environment interfaces share a common\\ninterface and have agents trained on various LLMs plug into various environments with minimal further adaption,\\nvarious frameworks have been proposed. These frameworks make for easier tests on agents’ capability on various\\nexecutable environments [706].\\nAgent-Agent Communication In MAS, communication through natural language is predominant. This is likely\\nbecause large language models possess strong linguistic capabilities due to pretraining on massive natural language\\ncorpora. Another possible reason is that, for many tasks, natural language communication is already sufficient to meet\\nthe requirements. Based on the type of information exchanged, multi-agent systems can be categorized as follows:\\nNatural Language-Based Systems Among LLM-based multi-agent systems utilizing natural language, text-based\\ncommunication is the most common [ 922,924,987,970,998]. There are also some systems that use voice as the\\nmedium of communication [ 996,762,999,1000 ]. In these systems, agents engage in behaviors such as discussions,\\nnegotiations, persuasion, or critique through natural language to achieve their objectives. Structured Information-\\nBased Systems Compared to natural language, structured information has characteristics such as higher consistency,\\nlower parsing complexity, and reduced ambiguity, making it more suitable for efficient and low-cost communication\\nbetween agents [626]. In some implementations, the information exchanged between agents is structured into distinct\\ncomponents to facilitate easier parsing and utilization by the receiving agent. For instance, the exchanged information\\nmight include fields specifying the sender ,receiver ,message type , and instructions on how the recipient should\\nparse or use the content [929].\\nHuman-Agent Communication The purpose of developing multi-agent systems is to expand the boundaries of human\\ncapabilities and cognition, ultimately serving human well-being. While in some social simulation multi-agent systems,\\nhumans primarily exist as observers [ 50,1001 ], most multi-agent systems allow human participation in various forms.\\nDuring this participation, humans need to communicate with agents, and this communication can take the form of either\\nnatural language or structured information [ 924,930]. When human-to-agent communication primarily relies on natural\\nlanguage, a single LLM often acts as a hub to parse human natural language into structured information that agents\\ncan process more effectively for subsequent operations. This hub LLM can either exist within the multi-agent system\\nor function independently of it. To save time and enhance communication efficiency, humans can also use structured\\ninformation to communicate with the multi-agent system through programming or similar methods. By following\\npredefined communication protocols, humans can send messages containing the required data to the multi-agent system.\\nThe system will then process the messages and data according to its internal logic and return the results. [931]\\n13.5.3 Next-Generation Communication Protocols\\nThe field of LLM-based agents is still in its infancy. Developers typically design agent architectures and communication\\nmechanisms tailored to specific domains or tasks, including agent-to-environment, agent-to-human, and inter-agent\\ninteractions. However, most existing systems lack a unified communication framework, resulting in fragmented, siloed\\necosystems. Multi-agent systems, tools, environments, and data sources often operate independently, making it difficult\\nfor agents to interoperate or share capabilities. Furthermore, the burden of learning and implementing bespoke protocols\\nfalls on humans, and almost all current protocols are manually designed—a labor-intensive process that often lacks\\nsemantic flexibility or scalability.\\nTo address these issues, several new agent communication protocols have been proposed, each targeting different\\naspects of the protocol design stack.\\nInternet of Agents (IoA) [933] introduces an internet-inspired, instant-messaging-like communication architecture that\\nsupports dynamic team formation and task-driven collaboration. Agents register with a central coordination server,\\nwhich handles identity management and discovery. Communication flows are orchestrated using FSM (Finite State\\nMachine)-based dialogue templates. IoA supports multiple message types, including discussion, task assignment,\\nand triggering mechanisms, and provides structured fields for controlling speaker turns, nested group formation, and\\n138\\nmaximum dialogue length. This allows agents to select and adapt message formats to match specific coordination\\nphases, offering flexibility within a fixed schema.\\nModel Context Protocol (MCP) [931], developed by Anthropic, focuses on enabling LLM agents to access structured\\ntools and data. It adopts a fully centralized approach based on OAuth identity authentication, and interactions are\\nconstrained to JSON-RPC 2.0 messages. While it lacks a meta-protocol layer or semantic negotiation capabilities, its\\nsimple and rigid architecture makes it a practical choice for tool use cases with well-defined APIs. However, MCP\\nsacrifices flexibility and extensibility, requiring manual registration of supported functions.\\nAgent Network Protocol (ANP) [1002 ] aims to achieve full decentralization. Agents identify themselves through\\nW3C-compliant decentralized identifiers (DIDs) and communicate over encrypted peer-to-peer channels. The protocol\\nincludes a meta-protocol layer that enables agents to negotiate which application-level protocol to adopt, supporting\\nsemantic protocol selection based on agent capabilities. ANP also allows for multi-protocol support at the application\\nlayer (e.g., HTTP, JSON-RPC, natural language), providing strong extensibility and decentralization but does not yet\\nexplicitly support public protocol reuse.\\nAgora [932] offers a highly flexible and language-driven protocol mechanism. Instead of registering pre-defined\\nAPIs, agents can generate and share Protocol Descriptions (PDs), which are free-text descriptions of communication\\nsemantics. Using a large language model, agents can dynamically interpret and execute any PD at runtime. This allows\\nprotocols to be created, deployed, and used entirely through language, without any manual registration or configuration.\\nAgora avoids centralized registries and supports decentralized protocol sharing: agents may publish or retrieve PDs\\nfrom peer-distributed repositories to enable cumulative learning and interoperability across systems.\\nSummary: As shown in Table 13.1, next-generation agent communication protocols differ along key dimensions\\nsuch as identity and security mechanisms, meta-protocol negotiation capabilities, application-layer flexibility, and the\\ndegree of centralization. A unified, secure, scalable, and dynamic protocol infrastructure—where agents can negotiate\\nand co-create protocols on the fly—is critical for enabling large-scale, interoperable agent ecosystems. While current\\nframeworks such as MCP, ANP, Agora, and IoA represent early but promising steps, protocol design remains a rapidly\\nevolving frontier in the development of intelligent agent systems.\\nTable 13.1: Comparison of four agent communication protocols (MCP, ANP, Agora, IoA) across identity, negotiation,\\nand execution layers.\\nPD= Protocol Description; DID:Decentralized Identifier; LLM :Large Language Model; FSM :Finite State Machine.\\nLayer MCP ANP Agora IoA\\nIdentity & Security OAuth-based centralized\\nidentity authentication.DID-based decentralized\\nidentity with encrypted\\nchannels.No centralized registra-\\ntion. Identity derived\\nfrom PD hash.Agents register with a\\ncentral server for identity\\nand discovery.\\nMeta-Protocol\\nLayerNo meta-protocol layer;\\nrelies on pre-defined in-\\nterfaces.Uses DID document to\\nnegotiate and select ap-\\npropriate protocol via se-\\nmantics.LLM interprets PD text\\nto automatically negoti-\\nate and deploy communi-\\ncation protocols.A centralized discovery\\nmechanism combined\\nwith FSM-based dia-\\nlogue flow control.\\nApplication Proto-\\ncol LayerSupports only JSON-\\nRPC 2.0.Supports multiple proto-\\ncols such as HTTP and\\nnatural language.Allows arbitrary PD-\\ndriven protocols with\\nhigh flexibility.Task-driven protocol\\ncoordination supporting\\nmultiple message for-\\nmats.\\nDegree of Central-\\nizationHighly centralized archi-\\ntecture.Fully decentralized. Decentralized: no regis-\\ntration or fixed ID, with\\noptional peer-to-peer PD\\nsharing.Highly centralized archi-\\ntecture with a central co-\\nordination server.\\nProtocol Flexibility Fixed and rigid; hard\\nto adapt beyond JSON-\\nRPC.Highly flexible with se-\\nmantic negotiation.Extremely flexible; any\\nPD can define a new pro-\\ntocol dynamically.Moderately high flexibil-\\nity; agents can select and\\nadapt message formats\\nbased on task phases and\\ncoordination needs.\\n139\\nTable 13.2: Classification framework for LLM-based multi-agent systems, highlighting different aspects of system\\ndesign, communication, collaboration, and evolution. Below are our abbreviations, for ease of reference:\\nM&S = Modeling & Simulation, CTS = Collaborative Task Solving, SL = Strategic Learning, S-D = Static-\\nDecentralized, S-L = Static-Layered, Hom = Homogeneous, Het = Heterogeneous, T/M = Teaching/Mentoring,\\nC-O = Consensus-Oriented, T-O = Task-Oriented, CL = Collaborative Learning, Dict = Dictatorial, D-B = Debate-\\nBased, CI = Collective Intelligence, Ind = Individual.\\nPaper System Design Communication Collaboration Evolution\\nCategory Typology Interface Agent Type Interaction Decision Type\\nAgent Hospital [921] M&S S-D Text Het T/M, C-O Dict Ind\\nWelfare Diplomacy [934] M&S S-L Code, JSON, Text Hom CL V oting CI\\nMEDCO[923] M&S S-L Text Het T/M, C-O Dict Ind\\nMedAgents[922] M&S S-L Text Hom T-O Dict CI\\nGenerative Agents [50] M&S S-D Visual Hom CL Dict Ind\\nRECONCILE [918] SL S-D Text Hom CL D-B CI\\nAgent Laboratory [746] CTS S-L Code, Text Het C-O, T-O Dict Ind\\nCoELA[924] CTS S-D Text Hom T-O\\nThe virtual lab [752] CTS S-L Text Het C-O, CL Dict Ind\\nSciAgents [743] CTS S-L Text Het T-O Dict CI\\nS-Agents [927] CTS S-D Text Het T-O, CL Dict\\nGPT-Bargaining [1003] CTS S-D Text Het C-O D-B CI\\nFORD [1004] M&S S-D Text Het C-O D-B CI\\nMADRA [1005] CTS S-D Text Het C-O D-B\\nMultiagent Bench [948] CTS S-D Text Hom T-O, CL D-B CI, Ind\\nOASIS [936] M&S D Text Het C-O\\nS3[255] M&S S-D Text Het C-O\\nFPS [981] M&S S-D Text Het C-O\\nGPTSwarm [1006] CTS D Code, JSON, Text Hom T-O Dict CI, Ind\\nChatEval [1007] CTS D Text Hom T-O V oting CI\\nMetaGPT [626] CTS S-L Code, JSON, Text,\\nVisualHet T-O Dict CI\\nAutoAgents [1008] CTS D Text Het T-O C-O CI\\nSWE-agent [628] CTS D Text Hom T-O Dict Ind\\nAgentCoder [994] CTS D Code, Text Het T-O D-B CI\\nMASTER [1009] CTS S-L Text Hom T-O D-B CI\\nReflexion [48] CTS D Text Het T-O D-B Ind\\nMACM [1010] CTS D Text, Code Het T-O D-B CI\\nDebate [985] CTS S-D Text Het C-O D-B CI\\n140\\nChapter 14\\nCommunication Topology\\n14.1 System Topologies\\n(a) Centralized(b) Distributed(c) Hierarchical\\nFigure 14.1: Different types of topological structure for multi-agent collaboration.\\n(a) Cooperation(a) Competition\\nFigure 14.2: Collaborative and competitive agents.\\nThis section examines the interaction typology in LLM-based multi-agent systems (MAS) and its impact on commu-\\nnication, collaboration, and task execution. We first analyze static topologies—where connectivity patterns are fixed\\nby domain knowledge—and then explore dynamic (adaptive) topologies that adjust inter-agent connections based on\\nperformance metrics, workload variations, or strategic constraints. We conclude with a discussion of scalability chal-\\nlenges and trade-offs in balancing system cost, performance, and robustness, drawing on recent research in distributed\\nprocessing, self-organization, and emergent collaborative behaviors.\\n14.1.1 Static Topologies\\nStatic topologies are defined by predetermined structural patterns that remain largely unchanged during system execution.\\nIn these configurations, connections among agents—or between agents and a central coordinator—are established using\\nfixed rules and heuristics, ensuring predictable communication flows and simplified coordination. Three canonical\\nforms are typically considered: layered (hierarchical), decentralized, and centralized architectures.\\nLayered (Hierarchical) Structures Layered topologies arrange agents hierarchically, with high-level agents coordinat-\\ning or supervising lower-level ones. This approach mirrors traditional management frameworks—such as Standard\\n141\\nOperating Procedures (SOP) or the Waterfall model—where tasks are decomposed into sequential, well-defined stages.\\nFor instance, the AutoAgents [ 1008 ] framework assigns roles (e.g., Planner, Agent Observer, and Plan Observer)\\nto synthesize execution plans, while ChatDev [ 983] leverages hierarchical task decomposition to streamline soft-\\nware development [ 626,921,627]. Although hierarchical structures facilitate debugging, performance monitoring,\\nand modularity, they can create bottlenecks when upper-tier agents are overloaded [ 1011 ]. Recent studies in story-\\ntelling [ 1012 ,1013 ,1014 ] and data science applications including data cleaning [ 1015 ,1016 ], visualization [ 1017 ,1018 ]\\nand auto machine learning [ 1019 ,1020 ], highlight the trade-off between consistency and the emergence of adaptive\\nreal-time behaviors.\\nDecentralized Structures In decentralized topologies, agents interact on a peer-to-peer basis without a central\\ncoordinator, forming networks that are often modeled as chains, rings, small-world, or random graphs [ 1021 ,971]. This\\nstructure enhances fault tolerance since the failure of a single agent does not compromise the network. For example,\\n[1022 ] show that distributing graph reasoning tasks among multiple agents enables scalability beyond the context length\\nlimits of individual LLMs. Additionally, [ 1023 ] propose decomposition strategies that allow an orchestrating LLM\\nto delegate subtasks effectively. However, maintaining a coherent global state in decentralized systems necessitates\\nsophisticated consensus and synchronization protocols.\\nCentralized Structures Centralized topologies rely on a master coordinator that gathers information and directs\\nperipheral agents hierarchically. Such a setup allows for better control over handling resources and sharing a global\\nview, such as with culture parks and Lyfe Agents [ 1024 ,1025 ]. With additional agents, however, a bottleneck at the\\ncenter node may occur, with increased communication overhead and susceptibility to failures. Current studies on\\ncoordinator-agent configurations [ 971] and research on ensuring autonomy for centralized configurations [ 1026 ] point\\nout problems with scalability with consistency. While consistency is guaranteed for centralized architectures, there may\\nnot necessarily be flexibility for dynamic adaptation.\\nBriefly, static topologies have advantages of determinism and predefinition. With pre-defined structural patterns,\\nthese systems have predictable communication patterns and effective coordination among agents. Topologies of\\nthese structures are typically defined on structural knowledge or static rules, and, as such, they suit domains where\\nworkflow for the tasks is static, there are predefined roles, and system requirements are well defined. The second\\nprimary advantage is design, implementation, and maintenance ease. With structure predefined, design as well as\\nexecution procedures are made simpler, and, as a result, maintenance is a simpler process. Resource handling as well as\\nmodularization gets simpler due to well-defined, static structure.\\nHowever, static topologies themselves are nonflexible, grounded on pre-specified patterns of connectivity that do not\\nrespond to real-time changes. Well suited for a specific purpose at design time but entirely lacking flexibility for reacting\\nto unforeseen challenges, including sudden agent breakdown, varying degrees of task complexity, and system goal\\nmodification, static topologies do not have real-time response flexibility potential. Real-time response inflexibility\\ninhibits runtime system reconfiguration and decreases system effectiveness in dynamic settings where circumstances\\noccur. Failure to self-organize and morph according to emerging conditions may equate to inefficiency as well as low\\nsystem performance, particularly where dynamic or emergent settings are at hand.\\n14.1.2 Dynamic and Adaptive Topologies\\nWhile static topologies provide determinism and predictability—illustrated by static topologies such as hierarchical\\nor centralized ones performing well with stable-task domains and well-defined roles—static topologies do not fit\\nopen-ended or novel domains. Real domains, from real-time collaborative plan, to dynamic social simulations, often\\ndemand that agents make changes on their patterns of interaction as work continues, available resources vary, or\\nfeedback from the environment is received. Such structural tension with adaptative malleability generates dynamic\\ntopologies, which, at runtime, recast inter-agent relationships as a response to feedback on performance, workload, or\\nstrategic constraints, striking a balance between consistency and responsiveness.\\nFor example, DyLAN framework [ 725] supports inference-time agent selection through a two-step process: a forward-\\nbackward team optimization step with unsupervised Agent Importance Scores, followed by dynamic team refor-\\nmulation at runtime. Similarly, OPTIMA [ 1027 ] optimizes inter-agent connectivity iteratively through a gener-\\nate–rank–select–train framework, utilizing reward functions as a means for determining a balance among task quality,\\ntoken efficiency, and readability, with communication actions further optimized through strategies such as Direct\\nPreference optimization. The MAD framework [ 649] illustrates flexibility through a joint optimization among three\\nprompt phases and structure, with dynamic role assignment (such as verifiers and debate participants) within pruned\\nspaces for structure.\\n142\\nTopological control also becomes tractable through technological advancements. GPTSwarm [ 651] conceptualizes\\nagents as computation graphs and uses evolutionary strategies and reinforcement learning for adjusting adjacency matri-\\nces for optimizing nodes based on feedback for the task. MACNET [ 1028 ] uses a directed acyclic graph architecture\\nwith supervisory instructors managing edges and executive assistants managing nodes for more complex coordina-\\ntion domains, facilitating adaptive communication through topological ordering and sensitive propagation of output.\\nApplication-specific versions also emphasize architecture diversity. Open-world environments have DAMCS [ 1029 ],\\nwhich couples hierarchical knowledge graphs (A-KGMS) with structured communication schemes (S-CS) for co-\\noperative planning as a function of messages passed based on context. AutoAgents [ 1030 ] leverages a dynamic\\ndrafting-execution pipeline with pre-defined agents jointly sketching out expert teams, a design that’s highly effective\\nfor creative applications such as novel generation through parallel processing and internal supervision. Noticeably,\\nsmall-world development within large-scale MACNET [ 1028 ] systems corresponds with graph reasoning ideas shown\\nin [1022 ], where distributed architecture bypasses local limitations of LLM through structured collaboration. In\\nterms of collaborative task solving, several paradigms have emerged that emphasize the role of dynamic topologies.\\nThese paradigms include search-based methodologies, LLM-based generation, and configurations utilizing external\\nparameters.\\nSearch-based Methods A number of works adopt search-based methodologies to iteratively optimize communication\\nstructures. For example, ADAS [ 741] employs a Meta Agent Search algorithm that iteratively generates and tests\\nnew agent designs within a code space, archiving superior configurations and thereby updating subsequent generation\\nstrategies. Similarly, Aflow [ 773] models each LLM call as a node in a graph and utilizes Monte Carlo Tree Search\\n(MCTS) to dynamically extend and refine the workflow. Other frameworks, such as MAD [ 1031 ] and OPTIMA [ 1027 ],\\nintegrate iterative generate–rank–select–train paradigms that echo MCTS principles to balance task performance with\\nefficiency.\\nLLM-based Methods Complementing search-based methods, several recent works leverage the generative capacity of\\nLLMs to construct and adapt dynamic topologies. Dylan [ 725] introduces a temporal feed-forward network (T-FFN)\\nmodel that treats each communication step as a network layer, using forward-backward propagation to compute Agent\\nImportance Scores for dynamic team selection. In related work, DAMCS [ 1029 ], AutoAgents [ 1030 ], and TDAG [ 1032 ]\\ndynamically generate specialized sub-agents or update hierarchical knowledge graphs, enabling cooperative planning and\\ntask decomposition. Further, frameworks such as AutoFlow [ 773] and Flow [ 1033 ] represent task workflows in natural\\nlanguage programs or activity vertex graphs (AOV), allowing continuous refinement through reinforcement learning\\nsignals. ScoreFlow [ 788] complements these approaches by applying gradient-based (loss-gradient) optimization to\\ncontinuously reconfigure agent workflows.\\nExternal Parameters Given that fine-tuning LLM-based agents is often resource-intensive, a considerable number of\\nresearchers advocate configuring inter-agent topologies by training parameters independent of the LLM-agent. This\\napproach is initiated by GPTSwarm [ 651], in which the inter-agent topologies are represented as a directed acyclic graph\\n(DAG), with edge weights serving as the sole trainable component of the system. Further advancing this paradigm,\\nAgentPrune provides a unified modeling framework from the spatial-temporal graph perspective for mainstream MAS,\\nwhere communication redundancy, i.e., unnecessary edges, is identified and pruned through magnitude-based pruning.\\nFollow-up works in this line of research include G-Safeguard [ 1034 ], which similarly trains GNN outside of the MAS\\nto detect and eliminate malicious communication paths. Although these methods are parameter-efficient, their relatively\\nsmall parameter space and low coupling with LLM-agents often result in performance limitations to some extent.\\nDiscussion Dynamic topologies extend beyond task-solving and play a crucial role in simulating complex social\\ninteractions. As detailed in a recent survey [ 975], LLM-based agent models can evolve inter-agent links to capture\\nreal-time changes in autonomy, social behaviors, and environmental feedback across various domains, including cyber,\\nphysical, and mixed environments. Systems such as [ 50], OASIS [ 936] and ProjectSid [ 989] simulate dynamic social\\nnetworks. [ 50] employs generative natural language memory retrieval to adjust social ties based on agents’ experiences,\\nwhile OASIS constructs a real-time social media environment with continuously updated user relationships and\\ninformation flows. Project Sid [ 989] introduces the PIANO (Parallel Information Aggregation via Neural Orchestration)\\narchitecture, enabling over 1,000 autonomous AI agents to interact in real-time within a Minecraft environment, leading\\nto the emergence of complex societal structures such as specialized roles, collective rule adherence, and cultural and\\nreligious transmission. Additionally, architectures like AgentScope-scability [ 1035 ] and Social Survey [ 975] support\\nlarge-scale multi-agent simulations, enabling studies of cultural dissemination, collective decision-making, and emergent\\ngroup dynamics in environments with hundreds or thousands of interacting agents. Additionally, dynamic topologies\\nare also tailored to specific application domains such as medical and open-domain embodied AI. In the medical field,\\nAI hospital [ 1036 ] and agent hospital [ 921] simulate real medical workflows, where iterative cycles of diagnosis,\\ntreatment, and feedback continuously reshape communication patterns among various roles, such as intern doctors,\\n143\\npatients, examiners, and supervising physicians. These frameworks dynamically adjust inter-agent communication to\\noptimize collaboration and decision-making. Similarly, in open-domain and embodied AI applications, frameworks\\nlike IOA [ 933] support heterogeneous, cross-device agent interactions, facilitating dynamic team formation and task\\nallocation in real-world scenarios.\\nAlthough the aforementioned dynamic multi-agent topologies have made substantial progress in performance metrics,\\nthey still face the following three limitations, which we believe should be the focal points for future research on dynamic\\ntopologies:\\n(1) Generalizability. Current MAS topologies are typically optimized for a single-task domain. For example,\\nAFlow [ 773] is dedicated to search and optimization within math or code benchmarks, producing a fixed workflow\\nthat is difficult to adapt to new task domains. Other dynamic topologies, such as ADAS [ 741], GPTSWarm [ 651], and\\nAgentPrune, face the same challenge. We argue that MAS should be capable of lifelong learning, wherein the system\\ngeneralizes across different task domains with minimal resources (e.g., API calls, FLOPs, GPU hours).\\n(2) Resource Efficiency. Present dynamic topologies often tend to optimize for complex, resource-intensive structures.\\nTheir training processes are typically exorbitantly costly, as exemplified by ADAS [ 741], where training with GPT-3.5\\nincurs a cost of approximately $300 per session. Such expenses severely constrain their large-scale applicability in\\nreal-world scenarios. Future developments should focus on achieving better test-time topology optimization with\\nsignificantly reduced costs.\\n(3) Inference Efficiency. As MaAS [ 787] has incisively observed, multi-agent topologies of excessive complexity,\\nwhile capable of consistently delivering satisfactory performance, are lamentably deficient in task adaptability . That is\\nto say, they are unable to dynamically allocate reasoning resources (i.e., tools, the number of agents, and reasoning\\nsteps) in response to the difficulty of a given task. Consequently, this may lead to a certain lack of efficiency in the\\ninference process. Although MaAS has, to a certain extent, achieved task dynamism through the designed agentic\\nsupernet, their applicability and scalability in large-scale deployment still remain to be tested.\\n14.2 Scalability Considerations\\nScalability is a critical challenge in LLM-based multi-agent systems (MAS), especially as the number of agents grows.\\nIn fully connected networks, the number of communication paths grows quadratically, leading to a communication explo-\\nsion that increases token usage and computational costs [ 1037 ,626]. Centralized and layered topologies can experience\\nsynchronization bottlenecks if supervisory nodes are inundated by messages, whereas decentralized networks—while\\nmore fault tolerant—necessitate complex consensus algorithms to achieve a coherent global state.\\nRecent work such as [ 1028 ] demonstrates that when multi-agent collaboration is structured as a directed acyclic graph\\n(DAG), the system can scale efficiently to handle large graphs—up to 1,000 nodes or more—without significant perfor-\\nmance degradation. Similarly [ 1022 ] shows that distributing graph reasoning tasks among many agents circumvents\\nthe limitations imposed by long textual inputs and context-length constraints. Moreover, studies on self-organized\\nagents[ 1038 ] reveal that dynamic multiplication and task distribution allow the system to maintain a constant workload\\nper agent while increasing overall processing capacity. Finally, the multi-dimensional taxonomy proposed by [ 1039 ]\\nprovides a valuable framework for analyzing trade-offs between agent autonomy and alignment, offering insights into\\nhow to balance centralized control with decentralized flexibility to optimize scalability.\\nIn addition to these foundational studies, recent advances in practical multi-agent platform design further enrich the\\nscalability discussion. For example, AgentScope [ 1035 ] offers a developer-centric platform that leverages an actor-based\\ndistributed framework to enable seamless migration between local and distributed deployments. Its unified workflow\\nand automatic parallel optimization significantly reduce the communication overhead and synchronization challenges\\nthat typically emerge as agent numbers increase. By incorporating fault-tolerance mechanisms and intelligent message\\nfiltering, AgentScope illustrates how system-level supports can be designed to maintain performance even in dynamic\\nand heterogeneous deployment environments.\\nAnother complementary approach is presented in Project Sid [ 989], which explores scalability within the realm of\\nsimulating agent civilizations. Here, the focus shifts from isolated task solving to the simulation of complex societal\\ndynamics. The proposed PIANO (Parallel Information Aggregation via Neural Orchestration) architecture allows\\nagents to operate concurrently by decoupling slower cognitive processes from rapid reactive modules. A dedicated\\ncognitive controller is introduced to ensure coherence among multiple parallel outputs. This design not only enables\\nscalability from small groups to simulations involving over a thousand agents but also effectively addresses the inherent\\ncoordination challenges arising from high-frequency interactions.\\n144\\nTaking scalability to an even larger scale, AgentSociety [ 1040 ] demonstrates a comprehensive framework for simulating\\nrealistic social environments with up to 10,000 agents. By integrating LLM-driven social generative agents within a\\nrealistic urban, social, and economic setting, AgentSociety employs distributed computing and a high-performance\\nmessaging system (e.g., MQTT) to support millions of daily interactions. This platform exemplifies how emerging\\nhybrid architectures can support macro-level phenomena—such as economic market dynamics, opinion diffusion,\\nand urban planning simulations—by effectively managing the trade-offs between communication cost, coordination\\noverhead, and emergent behavior fidelity.\\nDespite the theoretical advantages of scaling up agent populations, it is imperative to question whether pursuit of\\nlarge-scale agent deployments is inherently valuable for all task-solving scenarios. Although the total computational\\ncapacity scales with the number of agents, when memory overhead and inter-agent communication costs are factored\\nin, the marginal utility of adding additional agents may demonstrate diminishing returns. This phenomenon arises\\nfrom the fundamental constraint that, while the overall workload is the product of individual task complexity and the\\ndegree of labor division, coordination costs tend to increase super-linearly with agent count. Therefore, for many\\nbounded problem domains, there is likely an optimal agent population size beyond which performance plateaus—or\\neven deteriorates—due to excessive coordination overhead.\\nConversely, in simulation scenarios where the objective is to model complex social dynamics, emergent behaviors, or\\nlarge-scale collective intelligence, scaling to numerous agents becomes not merely beneficial but essential. In these\\ncontexts, the research focus shifts from optimizing computational efficiency for task solving to accurately reproducing or\\npredicting macro-level patterns emerging from micro-level agent interactions. Such simulations—covering domains like\\neconomic market behavior, social network evolution, and urban infrastructure planning—often require the computational\\noverhead of managing vast agent populations in order to capture realistic population-level phenomena.\\nHybrid architectures that combine centralized oversight with decentralized sub-teams offer a promising solution to\\nthese scalability challenges [ 921,918]. In these designs, supervisory agents handle global objectives and coordination,\\nwhile worker agents focus on executing specific subtasks. This hierarchical organization helps to mitigate information\\noverload at any single node and allows for dynamic adjustment of agent team sizes based on task demands, thereby\\noptimizing resource utilization. Furthermore, advanced techniques such as graph search algorithms, reinforcement\\nlearning-based updates, and evolutionary methods are critical for iteratively refining the network structure as the system\\nscales. Intelligent message filtering, prioritization, and aggregation mechanisms can significantly reduce communication\\noverhead without sacrificing the quality of inter-agent collaboration. In addition, asynchronous communication protocols\\nand partial knowledge sharing strategies show promise in minimizing coordination bottlenecks while maintaining\\nsufficient global awareness among agents.\\nConcluding Remarks on Scalability Overall, the study of system topology and scalability in LLM-based MAS\\nreveals a spectrum of design choices—from static configurations that offer simplicity and predictability to dynamic\\narchitectures that provide flexibility and adaptability. While foundational works (e.g., [ 1028 ], [1038 ]) emphasize\\nscalable graph structures and self-organizing principles, the practical advances demonstrated by AgentScope, Project\\nSid, and AgentSociety illustrate how integrated distributed frameworks, concurrent processing, and realistic environment\\nsimulations can collectively address the challenges of scaling multi-agent systems. The context-dependent nature\\nof scalability requirements—contrasting between task-solving and simulation scenarios—highlights the importance\\nof purpose-specific design in multi-agent architectures. As research continues to evolve, the development of more\\nsophisticated adaptive algorithms, distributed architectures, and multi-dimensional evaluation frameworks will be\\nessential for advancing the scalability and practical viability of LLM-based multi-agent systems.\\n145\\nChapter 15\\nCollaboration Paradigms and Collaborative\\nMechanisms\\nIn this chapter, we offer a detailed exploration of these purposeful interactions, examining how one agent influences\\ncollaboration within MAS. We reference the diverse interaction behaviors that emerge from human social structures,\\nfurther explaining multi-agent collaboration through interaction purposes, interaction forms, and the relationships that\\nform.\\nMulti-Agent Systems (MAS) comprise multiple agents that interact in a shared environment, autonomously making\\ndecisions to accomplish tasks collaboratively or compete with each other [ 1041 ]. In our context, we focus on\\ncollaborative phenomenons because they widely appeared in most practical applications. Basically, each agent in MAS\\nis equipped with different roles and initial knowledge and its own set of goals.\\nWhen engaged in problem solving or communication, agents interact with other agents or the environment to collect and\\nprocess information, independently making decisions based on their objectives, existing knowledge, and observations,\\nand subsequently executing actions [ 975,1041 ,1042 ,1043 ]. Knowledge, memory, and environmental observations\\nform the agents’ beliefs, while varying motivations influence their approach to tasks and decision making [ 1041 ].\\nConsequently, effective problem solving requires diverse purposeful interactions, including agent-agent and agent-\\nenvironment. These interactions may involve multiple rounds and occur in various directions, depending on the system\\ndesign.\\n15.1 Agent-Agent collaboration\\nConsidering the categorizations of MAS collaborations, we focus on more details on the granularity needed to capture\\nthe nuanced dynamics in complex multi-agent interactions. Sepecifically, we categorize inter-agent interactions into four\\ntypes, inspired by sociological insights from human-to-human interaction patterns and applying them to agent-agent\\ninteractions in MAS. Sociological theories on human interaction, which include consensus building ,skill learning ,\\nteaching , and task division collaboration , provide a more refined way of classifying agents. interactions. These\\ninteractions form collaborative paradigms, which enable diverse intelligent agents to work together effectively in\\nsolving complex problems, and they are shaped by various forms of goals, contexts and outcomes. Each paradigm\\naddresses unique challenges related to cooperation,competition, coordination, and decision-making. Additionally, MAS\\nimplementations involve agents with different types of interactions, rather than a single type or unidirectional process,\\nforming complex interaction networks that evolve over time. In collaborative software development [ 626,627], a senior\\ndeveloper agent may interact task-wise with an architect agent, guide junior agents through multi-round dialogues.\\nThey work together on code reviews for decision-making and learn with a testing expert agent to improve test coverage.\\nExamining the objectives and results of these interactions reveals the crucial techniques and technologies shaping agent\\nbehavior and decision-making, thereby enhancing our comprehension of multi-agent dynamics.\\nConsensus-oriented Interaction Consensus-oriented interactions concentrate on harmonizing the MAS’s final target\\nvia negotiation, voting, and social choice frameworks [ 1044 ]. This interaction is significant for incorporating diverse\\nknowledge and ensuring agents shift their views towards a unified understanding to achieve consensus [ 1045 ]. In\\nthis interaction, agents integrate knowledge to establish a unified understanding, which largely helps joint decision-\\n146\\nLLM-Based Multi-Agent Systems\\n(Agent-Agent Collaboration Types)\\nConsensus-oriented\\nInfo Flow: Multi-directional\\nPurpose: Align goals,\\nsynthesize per-\\nspectives\\nKnowledge: High (diverse\\nexpertise)\\nOutput: Shared under-\\nstandingCollaborative Learning\\nInfo Flow: Peer-to-peer\\nPurpose: Mutual improve-\\nment via sharing\\nKnowledge: Medium (individ-\\nual experiences)\\nOutput: Skill growthTeaching / Mentoring\\nInfo Flow: Unidirectional\\n(expert→novice)\\nPurpose: Transfer knowl-\\nedge and skills\\nKnowledge: Low (established\\nknowledge)\\nOutput: Learner develop-\\nmentTask-oriented\\nInfo Flow: Sequential /\\npipeline\\nPurpose: Coordinate for\\nshared goals\\nKnowledge: Medium (com-\\nbined output)\\nOutput: Task completion\\nFigure 15.1: An overview of four agent-agent collaboration types in LLM-based MAS: Consensus-oriented ,Collabora-\\ntive Learning ,Teaching/Mentoring , and Task-oriented . Each type is described along four key dimensions: information\\nflow, collaboration purpose, knowledge integration, and output focus.\\nmaking in complex problem-solving situations that demand different viewpoints. For instance, MedAgents [ 922],\\nMDAgents [ 1046 ], and AI Hospital [ 1036 ] demonstrate how collaborative dialogue among multidisciplinary agents\\nimproves problem solving by sharpening reasoning skills and accessing inherent knowledge.\\nThese dialogues allow agents to ensemble expertise into coherent outcomes, frequently outperforming conventional\\nmethods like zero-shot or few-shot reasoning. The importance of consensus-driven teamwork is particularly evident in\\nscientific environments, where addressing complex challenges requires diverse perspectives and meticulous validation.\\nAgent Laboratory [ 746], serves as an example where PhD and postdoctoral agents collaborate to agree on research\\nobjectives, interpret experiments, and consolidate research findings. Similarly, Virutal Lab [ 752] organize a series of\\nteam to conducts scientific research, where all agents discuss a scientific agenda, and individual meetings, where an\\nagent accomplishes a specific task.\\nMethods for multi-agent consensus typically include several approaches, including Discussing ,debating ,negotiating ,\\nreflecting , and voting . Common methods for reaching consensus encompass an array of structured techniques. The\\nprimary mechanisms involved are discussing ,debating ,negotiating ,reflecting , and voting . Debates allow agents to\\nobtain competing hypotheses, while negotiation helps resolve conflicting priorities and resource limitations. Specific\\nframeworks have been created to support these consensus-building activities. During these processes, agents gather\\noutputs from peers tackling the same issue, and include environmental feedback as numerical data and contextual details.\\nThese interactions enable agents to share viewpoints, assumptions, and progressively achieve a common understanding.\\nFor example, GPTSwarm [ 651] formulates the collaboration between agents with graph design, that the information\\nflow and edge connections build the basic group discussion. In GPTSwarm, if an agent consistently provides incorrect\\nopinions, it will be excluded. RECONCILE [ 918] uses a round-table discussion format with several discussion\\ncycles and voting systems based on confidence levels. It integrates reflection by learning from past discussions,\\nusing confidence metrics and human insights to improve their responses. Furthermore, debates are quite important\\nfor achieving agreement, reducing hallucinations and also addressing complex issues [ 985,1047 ,1031 ,1003 ]. In\\nGOVSIM [ 1048 ], agents collaborate to achieve a balance, and it suggests using a shared resource and conserving it\\nfor future needs. The negotiations went beyond simple information exchange and relationship-focused interactions.\\nThe Multi-Agent Debate (MAD) framework [ 1031 ] promotes creative thinking by having agents deliver arguments\\nin a “tit-for-tat” pattern, with a judge overseeing the process to finalize a solution. The Formal Debate framework\\n(FORD) [ 1004 ] enhances consistency among language models through organized debates, enabling stronger models\\nto steer consensus, while weaker ones adjust their perspectives. Similarly, AutoAgents [ 1030 ] define a collaborative\\nrefinement action in which each agent updates its chat record. In the process, it also appends the previous statements of\\nthe other agent and refines its action to achieve consensus.\\nCollaborative Learning Interaction In collaborative learning, interaction usually happens among similar agents.\\nAlthough architecturally alike, accumulate distinct memories and experiences due to their unique behaviors and varied\\nenvironmental interactions. By solving problems together, these agents share experiences to boost their strategy\\nlearning, task-solving, and skill acquisition capabilities. Over time, each agent enhances its skills through ongoing\\ninteraction, leading to the evolution of individuals. The key difference between collaborative learning and consensus-\\n147\\noriented interactions lies in their fundamental goals and processes. While consensus-oriented interaction focuses on\\nknowledge integration and belief alignment through synthesizing diverse viewpoints to reach agreement, collaborative\\nlearning interaction emphasizes peer knowledge construction and experience sharing, prioritizing mutual improvement\\nand individual growth. When engaged in collaborative learning interaction, agents update their context or memory\\nfrom observing others’ behavior. For example, agents can learn optimal strategies by observing the deliveration\\nfrom peers, adapting their own approach based on these observations without necessarily agreeing on a single “best”\\nstrategy [ 961,962,963,971,965,967,972,968,969]. As highlighted in [ 966], the effective discussion tactics\\nsignificantly impact learning outcomes among agents. In these interactions, agents collaborate to learn and address\\nproblems, focusing on mutual understanding and enhancement rather than reaching unanimous decisions. This method\\nrefines personal responses and knowledge via ongoing feedback.\\nThe methods commonly employed in collaborative learning interaction include: 1). Experience sharing. , Agents\\nexchange personal insights and best practices. As described in [ 303], iterative experience refinement enables LLM\\nagents to achieve adaptive improvement in software development via continual acquisition and utilization of team\\nexperience in successive pattern and the cumulative pattern. Furthermore, MAS-CTC [ 301] is a scalable multi-team\\nframework that enables orchestrated teams to jointly propose various decisions and communicate with their insights\\nin a cross-team collaboration environment. It enables different teams to concurrently propose various task-oriented\\ndecisions as insights, and then communicate for insights interchange in important phases (multi-team aggregation).\\nDifferent agent teams utilize a greedy pruning mechanism and aggregation mechanisms to eliminate low-quality content,\\nthus improve the performance in software development. Differently, in MOBA [ 1049 ], a novel MLLM-based mobile\\nmulti-agent system, global agent reflects on local agent execution results to support adaptive planning to align with the\\nenvironment. AutoAgents [ 1030 ] employs a knowledge sharing mechanism where agents exchange execution results\\nto enhance communication and feedback, where agents can obtain long-term, short-term and dynamic memory from\\nothers. 2). Peer discussions. Peer discussions allow agents to articulate their reasoning processes and learn from\\nothers’ approaches. MEDCO [ 923] create a dynamic environment where clinical reasoning and decision-making\\nskills are strengthened through collaborative problem-solving among student agents. Moreover, In [ 1050 ], agents\\nengage in structured peer discussions after initializing their output, reviewing each other’s reasoning step by step.\\nThrough feedback exchange and confidence scoring, agents refine their decision-making, learn from diverse approaches,\\nand iteratively enhance their reasoning accuracy, fostering collaborative knowledge acquisition. 3). Observational\\nlearning. Observational learning occurs when agents monitor others’ behaviors and outcomes to inform their own\\nstrategies. AgentCourt [ 1051 ] develops lawyer agents that participate in court debates and improve through accumulated\\nexperiences, demonstrating improved reasoning and consistency through experiential learning. In iAgents [ 1046 ],\\nthe human social network is mirrored in the agent network, where agents proactively exchange human information\\nnecessary for task resolution, thereby overcoming information asymmetry. iAgents employs a novel agent reasoning\\nmechanism, InfoNav, to navigate agents’ communication towards effective information exchange. Together with\\nInfoNav, iAgents organizes human information in a mixed memory to provide agents with accurate and comprehensive\\ninformation for exchange. Additional experimental phenomenon indicates difficulty of certain tasks making agents\\ncontinuously refine their strategies in pursuit of the required information. MARBLE [ 948] designs a cognitive evolve\\nplanning combining the ‘expectation’ of the agent and its actual action results to update the overall planning experience\\nfor better planning in the next round.\\nDespite its benefits, collaborative learning interaction faces several challenges. These include ensuring equitable knowl-\\nedge exchange among agents with varying capabilities, preventing the propagation of errors or biases across the system,\\nmaintaining agent diversity while facilitating learning, and developing effective mechanisms for agents to selectively\\nincorporate others’ knowledge based on relevance and reliability. Overcoming these challenges requires the meticulous\\ncreation of interaction frameworks and learning strategies. And it should balance individual advancement with the\\nbroader development of the system. Although issues such as knowledge fairness, bias propagation, and scalability\\npresent difficulties, there is great potential to improve MAS, particularly in dynamic and complex environments. By\\nusing iterative learning processes and providing opportunities, collaborative learning enables agents to develop richer\\nknowledge bases and more refined problem-solving abilities.\\nTeaching/Mentoring Interaction To tackle these challenges, it is important to carefully develop interaction protocols\\nand learning frameworks that harmonize individual development with overall system progress. In the context of MAS,\\nteaching and mentoring interactions are fundamental mechanisms in collaborative environments, especially in scenarios\\nwhere knowledge transfer is essential for growth and collective intelligence. Unlike collaborative learning, where\\nknowledge is exchanged reciprocally among agents, teaching and mentoring interactions focus on the unidirectional\\nflow of knowledge from an experienced agent to a less experienced one. The mechanisms and methods used in\\nteaching/mentoring interactions include several key strategies:\\n148\\n•Criticism and Feedback. The mentor agent evaluates the learner’s performance and provides corrective or\\nconstructive feedback. This helps the learner refine their knowledge and skills through a feedback loop where\\nthey update their internal knowledge based on the feedback received.\\n•Evaluation. Mentors assess the learner’s capabilities or progress through performance reviews and clear\\nassessment criteria, providing valuable insights for development.\\n•Instruction and Teaching. Mentors convey targeted knowledge, guidelines, or techniques using direct\\ninstruction which allow learners to pose questions and receive clarifications.\\nIterative Teaching and Reinforcement Teaching is typically progressive, where each phase provides opportunities for\\nthe learner to complete tasks and get feedback. For example, in the MEDCO system [ 923], student agents improve\\ntheir professional skills through a cyclic practice-oriented learning approach directed by expert mentors, in addition to\\nengaging in peer discussions. These expert agents conduct ongoing assessments and provide real-time guidance on\\nclinical competencies, focusing on patient interaction skills and diagnostic reasoning. [ 921] shows that an agentic\\ndoctor can continually improve their diagnosis by merely interacting with agentic patients in a simulated hospital and\\ncan transfer its learned knowledge of real-world cases.\\nThis interaction type can be categorized based on the direction of knowledge transfer into two primary types: unidirec-\\ntional and interactive. Unidirectional is rooted in traditional teaching models where knowledge flows from the teacher\\nto the student. This approach emphasizes the transmission of facts and concepts, often involving lectures and direct\\ninstructions [923].\\nTask-oriented Interaction. Task-oriented collaborations involve agents working together to achieve common objec-\\ntives through effective coordination and task decomposition strategies, as well as a high degree of cooperation and\\ncoordination. Agents interact primarily by processing upstream output and generating results for downstream agents\\nfollowing established task dependencies rather than engaging in complex discussions or debates.\\nRecent frameworks demonstrate diverse implementations of this interaction pattern: (1) software development\\nframeworks such as MetaGPT [ 626] and ChatDev [ 627], agents operate in a structured pipeline that mirrors the\\nsoftware development lifecycle. For example, architect agents process requirements to generate technical specifications,\\nwhich development agents then use to produce code, followed by testing agents who validate the implementations; (2)\\nCollaborative reasoning frameworks like Exchange-of-Thought (EoT) [ 1052 ], GPTSwarm [ 651], MACNET [ 1028 ]\\ninvolve structuring agents in a specific format (e.g., ring, tree, directed acrylic graphs, optimizable graphs) , which\\nmitigates context expansion risks by ensuring only optimized solutions progress through the sequence, and enforcing\\nmultiple agents to collaborate together towards solving complex mathematical or knowledge reasoning tasks; In (3) ML\\napplications [1053 ,1019 ], agents adhere to stringent workflow structures, each fulfilling specific tasks in processes.\\nFor more complex tasks such as VideoQA, the TraveLER framework [ 1054 ] showcases modular task breakdown across\\nstructured phases (Traverse, Locate, Evaluate, and Replan), with a Planner agent managing interactions and improving\\nstrategies based on iterative agent inputs.\\nThese handoffs rely on explicit deliverables instead of direct agent negotiations. Inspired by GPTSwarm [ 651]-alike\\ngraph agentic systems, MACNET [ 1028 ] structures agents into directed acyclic graphs (DAG). Here, supervisory\\nfigures issue directives while executors implement solutions. By ensuring only optimized solutions progress through\\nthe sequence, this setup mitigates context expansion risks. In ML applications [ 1053 ,1019 ], agents adhere to stringent\\nworkflow structures, each fulfilling specific tasks in processes. For more complex tasks such as VideoQA, the TraveLER\\nframework [ 1054 ] showcases modular task breakdown across structured phases (Traverse, Locate, Evaluate, and\\nReplan), with a Planner agent managing interactions and improving strategies based on iterative agent inputs.\\nBeyond organized development, task-driven interactions have been shown in open-ended contexts such as Minecraft\\ngame, in where agents adjust to ever-changing environments. In [ 927], leader agents manage workflows by breaking\\ndown complex objectives into specific tasks, while executor agents perform actions like gathering resources. Coordina-\\ntion mechanisms are important for ensuring agents collaborate effectively towards final goal, including communication\\nprotocols, synchronization strategies, and resource-sharing techniques. The interaction of agents in MAS for task execu-\\ntion has garnered significant interest, notably through utilizing LLMs for handling intricate tasks and workflows. The\\ncollaboration of agents are vital for task completion, particularly in ever-changing settings like software development\\nand project management [626, 630].\\n15.2 Human-AI Collaboration\\nTo unlock the potential of MAS in meeting human objectives, people often work alongside them using three primary\\nmethods: one-off task delegation ,multi-turn interactive instruction , and immersive human-agent collaboration .\\n149\\nInone-off task delegation , humans delegate single-instance tasks to MAS, such as posing a question to a Q&A platform\\nor assigning a coding task [ 1055 ,626]. Without additional input, the agent handles the task autonomously, delivering a\\ncomplete response or solution in a single reply. This is presently the prevalent way humans collaborate with LLM-based\\nagents [922, 627, 31].\\nFormulti-turn interactive instruction , humans engage in iterative interactions with LLM-based agent systems to\\nrefine and explore solutions until a satisfactory result is achieved. This type of interaction is widely seen in creative\\napplications, such as image editing or writing edit [ 938]. For instance, a user might ask the system to add an object\\nto a specific location in an image, replace an element, change the background, or revise a part in a sentence. These\\ninteractions often span multiple rounds, with users continuously refining their requests until the desired outcome\\nis reached. Moreover, certain other LLM-based agent systems may require human approval or clarification during\\nmulti-turn interactions before proceeding to the next step [ 1056 ,930]. Under human guidance, these LLM-based agent\\nsystems can complete household tasks as well as software development tasks.\\nImmersive human-agent collaboration features LLM-based agents simulating human behaviors to serve as partners.\\nFor instance, in an immersive setting, humans treat these agents as teammates, achieving common objectives. Instances\\ninclude agents representing humans in meetings or help solve tasks like chores or projects. This strategy highlights\\neffective integration and teamwork in dynamic contexts [937, 924].\\nTo assess Human-AI collaboration quantitatively, several frameworks have been suggested. Co-Gym [ 1057 ], for\\ninstance, measures the communication, situational awareness, and personalization of LLM-based agents in tasks such\\nas travel planning, writing related work, and tabular analysis.\\nIn summary, as LLM-based agent systems have advanced, Human-AI collaboration has diversified to address challenges\\nacross domains. This ranges from simple command-based AI interactions for questions, to multi-turn dialogues for\\ndesign and development, and partnering with human daily tasks.\\nWith advancements in LLM-based agent systems, they are expected to integrate more into daily life, streamlining tasks\\nand boosting efficiency. At the same time, humans will refine and adapt their ways of interacting with AI, leading\\nto more effective collaboration. We believe this shift will drive fundamental changes in both social productivity and\\nthe social relations of production, reshaping how work is organized and how humans and AI cooperate in the large\\nlanguage models era.\\n15.3 Collaborative Decision-Making\\nCollaborative decision-making processes are crucial for ensuring the efficient operation of MAS and the successful\\ncompletion of tasks. Although collaboration itself is a core feature, the approaches of decision-making directly\\ndetermines the effectiveness of collaboration and the overall performance of the system. Recent research has highlighted\\nthe critical role of collaborative decision-making. [ 1037 ] showed that diverse decision-making methods can significantly\\nenhance the collaborative efficiency of the system. [649] emphasized that a rational decision-making mechanism can\\nstimulate the emergence of intelligence within a system.\\nFrom a broader perspective, the collaborative decision-making process can be divided into two major categories based\\non their architectural characteristics: Dictatorial Decision-Making and Collective Decision-Making [1037].\\nDictatorial Decision-Making. Dictatorial Decision-Making is a process where decision-making relies on a single\\nagent in a MAS. In this paradigm, all agents send their state information or local observations to this dictatorial agent.\\nThe dictatorial agent is responsible for assembling this data, studying the core problems, and establishing definitive\\ndecision guidelines. The key principle for such an approach is to leverage a global mindset in moving towards improved\\ndecision-making, hence paving the reliability of the system performance along with the successful achievement of task\\ngoals. [ 1031 ,1058 ,1046 ] demonstrated the single-agent decision-making process with a single LLM, who synthesized\\nvarious views on the same problem to make decision-making even more objective and comprehensive. Furthermore,\\n[134,1059 ] suggested the weighted integration method through ranking, scoring or checklist, enhancing the robustness\\nof decision-making procedures. In addition, beyond the explicit inclusion of perspectives, [ 1030 ,1060 ] proposed\\narchitectures where a central agent breaks down complex tasks into simpler sub-tasks and assigns them to specialized\\nagents grouped by their functionalities. Moreover, in [ 651,1028 ],it is common that the last node’s agent works in an\\nenvironment to assemble the past information and deduce a conclusion according to the topological structure, rather\\nthan by a central agent.\\nCollective Decision-Making. Collective Decision-Making involves agents collaborating to reach decisions without a\\ncentral authority, relying on local data and interactions like voting or negotiation. This method shares decision-making\\npower among agents, allowing the system to adapt according to changes while maintaining robustness and scalability.\\n150\\n•Voting-based Decision Making V oting systems are important for collective decision-making, providing a\\nframework for reaching consensus. A conclusive majority is achieved through voting as described by [ 1045 ,\\n968]. Moreover, the GEDI electoral module [ 1037 ] enables multiple voting methods. This method largely\\nimprove reasoning and fault-tolerance while avoiding complex system designs.\\n•Debate-based Decision Making In comparison with voting-based methods, debate-based decision-making\\nfocuses on organized interactions between agents, in order to obtain the best result. In [ 1031 ,1061 ], agents\\nparticipate in guided discussion, where they articulate and proposals in an attempt to resolve disagreements\\nand reconcile points of view. Simultaneously, [ 1050 ,1062 ] practice restraint stance, using communication\\nchannels among agents for consensus-building through repeated discussions. To tackle the issue of “cognitive\\nislands,” certain systems would employ a common retrieval knowledge base to enable agents to be aware of\\nthe same knowledge throughout debates [ 1005 ]. By mimicking human dialogue, these systems allowed agents\\nto exchange perspectives and make more informed decisions.\\nDiscussion and Future Work Collaboration in multi-agent systems (MAS) still faces numerous challenges that require\\nfurther research. Current methods are largely based on contextually dependent interactions; however, they do not\\ninclude a specific framework for training and optimizing cooperative actions. This heavy dependence on large language\\nmodels (LLMs) has some limitations, as their effectiveness is inherently tied to the size of the LLM’s contextual window\\nand its native reasoning capabilities. While LLMs provide a solid foundation for enabling interactions, these systems\\nare still limited by the inherent limitations of context-dependent communication.\\nFuture studies should focus on finding frameworks that inspire agents for active learning with regard to optimal timing\\nand information dissemination methodologies. Using methodologies from multi-agent reinforcement learning (MARL),\\nthere is a growing requirement for strategies that will help agents determine appropriate moments for information\\nsharing, as well as what information should be shared through what channels. This calls for not just devising novel\\ninteraction protocols but also incorporating training methodologies that will constantly optimize these protocols with\\neach improvement.\\n151\\nChapter 16\\nCollective Intelligence and Adaptation\\nThe concept of collective intelligence is central to the development of multi-agent systems(MAS), drawing inspiration\\nfrom biological and societal cooperation. An inherent concept within collective intelligence is the “Wisdom of Crowds”\\nby [915], which asserts that independent communities often make better decisions as a whole than any one person.\\nCognitive theoretical models like the Society of Mind [ 17] and its related theory mind [ 916,917] further support the\\nparadigm, suggesting that intelligence springs from a synergy among primary, specialist components. Moreover, In\\nhuman societies, individuals collaborate, divide labor, and engage in collective problem-solving to address complex\\nchallenges. MAS adopt similar strategies where specialized agents to participate in solving complex problems and\\ncollective decision-making [914].\\nThe emergence of collective intelligence within MAS is a dynamic and iterative process. Through continuous interaction,\\nagents develop a shared understanding and collective memory progressively. The interaction dynamics are strengthened\\nby heterogeneity among individual agents, environmental feedback, and agent-agent interactions [ 914], which are\\nall important for the emergence of complex social networks and improving decision-making strategies. It is worth\\nhighlighting that collective intelligence is not merely the summation of individual capability, but refers to emergent\\nbehavior beyond individual agent capacity. beyond individual agent capacity. Individual agent development is deeply\\nlinked with collective intelligence growth. With ongoing involvement with collective tasks, and self-reflection on shared\\ncontexts, agents increasingly develop reasoning and decision-making capabilities. The evolution of individual agents\\nis closely related to collective intelligence evolution. Through continuous interaction in joint activities and critical\\nexamination of shared contexts, agents continuously refine their reasoning and decision-making abilities.\\nIn parallel, complex and diverse behavior among agents emerges. These include beyond-restricted-protocol behaviors,\\nsuch as advanced social interactions, including trust, strategic deception, adaptive camouflage, and emergent cooperation,\\nevoking a shift from reactive into cooperative strategies, as well as deeper social dynamics. With a chain of recursive\\ninteractions, agents necessarily form cooperative strategies, which eventually turn into social contracts, organizational\\nhierarchies, and divisions of labor. Social phenomena necessarily emerge through recursive interactions among agents,\\ncoupled with their adjustment with the changing environment. It marks a transition from fundamental cooperative\\nbehavior into complex social constructs, leading to cultural norms and conventions.\\n16.1 Collective Intelligence\\nThe concept of collective intelligence, which refers to the ability of a group of agents to exhibit problem-solving\\ncapabilities that surpass those of individual agents. This phenomenon is often characterized by emergent behaviors,\\nsophisticated decision-making, and higher-order reasoning abilities that arise from interactions among agents, leading to\\nenhanced performance in collaborative decision-making scenarios and social simulations [ 975]. [917] demonstrate that\\nLLM-based agents can exhibit collaborative behaviors and high-order Theory of Mind capabilities, which are crucial\\nfor understanding the perspectives of other agents in a shared environment. Their findings suggest that the integration\\nof LLMs into MAS can facilitate more sophisticated forms of collective intelligence, thereby improving the overall\\nefficacy of collaborative decision-making.\\nImproved System Performance A primary advantage of collective intelligence in MAS is that collaboration leads\\nto superior problem-solving capabilities. Collective intelligence can be encouraged to overcome “groupthink” and\\nindividual cognitive bias in order to allow a collective to cooperate on one process – while achieving enhanced\\n152\\nintellectual performance. When individual agents share information and coordinate actions, the system can achieve\\nbetter results than any single agent operating independently [ 626,922,1046 ,1031 ,1063 ]. Collective intelligence\\nis therefore shared or group intelligence that emerges from the collaboration, collective efforts, and competition of\\nmany individuals and appears in consensus decision making. Collective intelligence strongly contributes to the shift of\\nknowledge and power from the individual to the collective. [ 924] demonstrated this through their Cooperative Embodied\\nLanguage Agent (CoELA), which achieved a 40% improvement in efficiency over traditional planning methods in\\nThreeDWorld multi-agent transport tasks. This substantial improvement stems from the system’s ability to effectively\\nutilize LLMs for planning and communication in multi-agent settings, providing compelling evidence for enhanced\\ncollaborative decision-making capabilities. As previously discussed, the inherent diversity and interdisciplinary nature\\nof LLM-based multi-agent systems, along with various inter-agent interaction, which provide internal feedback and\\nenriched context for individual decision-making, hence reduce bias and improve the consistency of solution [918].\\nEmergent Behaviors One of the most intriguing aspects of collective intelligence is the emergence of new, complex\\nbehaviors that arise spontaneously from agent interactions. These behaviors are not explicitly programmed but emerge\\nfrom learning and adaptation. As discussed in various studies [ 971,965,966], agents developed strategic behaviors,\\nincluding trust-building, adversarial tactics, deception, and leadership during the game. The collective behavior evolved\\nthrough experience sharing, where village-aligned agents learned cooperation and strategic alliance formation, and\\nwolf-aligned agents improved deception through “information confusion” tactics. Moreover, agents optimized voting\\npatterns and deception strategies without explicit training, which indicates the group intelligence emerged over multiple\\nrounds of interactions. Similarly, in the Avalon game [ 968], researchers observed that agents became better at identifying\\nand countering deceptive information. Individuals adapted to deceptive environments and refined their decision-making\\nusing first- and second-order perspective shifts. Furthermore, agents demonstrated adaptive cooperation and ad hoc\\nteamwork, despite no predefined collaboration protocols [ 969]. These findings highlight the ability of LLM-based agents\\nto develop sophisticated behaviors through interaction and learning, showcasing the potential for emergent behaviors in\\ncollective intelligence scenarios. Notably, these emergent behaviors rely on memory and reflective mechanisms. Agents\\nretrieve and reflect on historical information to generate a compact context, enhancing their reasoning capabilities [ 239].\\nIn MAS, shared context and environmental information significantly boost agents’ usable memory. This enables agents\\nto build on past interactions, refine strategies, and adapt more effectively to dynamic environments [1064].\\nSocial Evolution One of the most significant findings in the field of generative agent societies is the spontaneous\\nemergence of social norms. [ 1065 ] demonstrated that agents, through continuous interaction, are capable of creating,\\nrepresenting, spreading, evaluating, and complying with social norms. These norms serve as the foundation for social\\norder, reducing conflicts and improving coordination among agents, thereby leading to more stable and organized\\nsocieties. Interestingly, the study found that agents develop norms more rapidly in their beliefs than they do in their\\nbehaviors. This suggests that while agents may quickly internalize certain norms, the translation of these norms into\\nconsistent actions takes longer. Over time, these norms tend to synthesize into more general principles, resulting\\nin more concise and effective personal norm sets. Furthermore, the Project Sid simulation[ 989] models large-scale\\nagent societies and provides further evidence of the emergence of social norms and role specialization. In this study,\\nagents were observed to autonomously form specialized social roles. These roles were not predefined but emerged\\nnaturally as agents interacted within their environment and developed collective rules.The simulation also highlighted\\nthe importance of democratic processes in the adherence and modification of these collective rules. Agents were found\\nto engage in cultural and religious transmission, spreading ideas and doctrines across communities. This process of\\nnorm creation and role specialization leads to better organization, reduced conflict, and adaptive governance structures\\nwithin the society. The evolution of cultural and religious beliefs in multi-agent societies is also observed in [ 1066 ],\\nwhich occurs through agent-driven selection of ideas, mirroring real-world societal changes. Additionally, the [ 936],\\nwhich simulates social interactions among one million agents, provides valuable insights into cultural transmission and\\ngroup polarization. Cultural memes and belief systems propagate naturally among agent societies. Agents exhibit herd\\nbehavior, conforming to prevailing opinions even when these opinions are irrational. This leads to the emergence of\\ngroup polarization, where agents reinforce extreme views through repeated interactions. This finding highlights the\\nsignificant impact of group size on the dynamics of cultural evolution and social behavior.\\n16.2 Individual Adaptability\\nIn multi-agent systems (MAS), individual adaptability refers to an agent’s ability to adjust its behavior and decision-\\nmaking strategies based on previous interactions and experiences. This is also defined as self-evolving, where agents\\ncan dynamically self-evolve by modifying themselves, such as altering their initial goals and planning strategies, and\\ntraining themselves based on feedback or communication logs [ 38]. This adaptability is facilitated by the integration of\\nlarge language models (LLMs), which support dynamic monitoring and adaptation processes [ 1067 ], as well as the\\n153\\nagents’ memory capabilities and information exchange. These modules are crucial to ensure that agents can continuously\\nimprove their performance, respond effectively to dynamic environments, and optimize their decision-making processes.\\nWe categorize the mechanisms contributing to individual adaptability into memory-based learning and parameter-based\\nlearning, where there are training-free and training-based approaches.\\nMemory-based learning Memory and reflective mechanisms significantly enhance individual adaptability in LLM-\\nbased multi-agent systems by leveraging historical records and experiences to inform decision-making [ 221,1068 ,50].\\nBy maintaining and utilizing individual memory of past interactions, decisions, and outcomes, the agent can refine its\\ndecision-making process over time. This memory serves as a repository of experiences that the agent can draw on when\\nmaking future decisions. Using this stored knowledge, individual agent is able to refine its decision-making process,\\nlearning from previous successes and failures [ 921,1051 ]. For example, in clinical simulation, doctor agents can\\nkeep improving treatment performance over time by accumulating experience from both successful and unsuccessful\\ncases [ 921]. In social behavior simulation, agents can improve their adaptability by engaging in more complex scenarios\\nand utilizing scenario memories to enhance performance [50].\\nShared memory-based learning In contrast, shared memory-based learning extends this concept by enabling multiple\\nagents to exchange information and insights derived from their respective experiences. Rather than relying solely on\\nindividual memory, agents can benefit from the collective knowledge of the group. By sharing data, strategies, and\\nfeedback, agents enhance their ability to cooperate and optimize their decisions collaboratively. Shared memory-based\\nlearning is particularly valuable in environments where agents need to cooperate, exchange tasks, or work toward\\ncommon goals [ 919,967,968]. For instance, ProAgent [ 1069 ] anticipates teammates’ decisions and dynamically\\nadjusts each agent’s strategies based on the communication logs between agents, facilitating mutual understanding and\\nimproving collaborative planning capability.\\nParameter-based learning. Beyond memory-based learning in textual form, many MAS employ parameter-based\\nlearning, which evolves agents’ individual adaptability through post-training techniques. For instance, [ 1070 ] discusses\\nthea Learning through Communication (LTC) paradigm, whereusing communication logs between agents are leveraged\\nto constructto generate datasets forto training or fine-tuninge LLMs. The integration of symbolic and connectionist\\nparadigms within LLM-powered agents enhances botheir reasoning and adaptability. More recently, research has\\nincreasingly focused on multi-agent (co-)fine-tuning, which improves collaboration and reasoning capabilities through\\ncooperative trajectories. Examples include multi-agent debate fine-tuning [ 1071 ] and SiruiS [ 1072 ]. Additionally,\\nSweet-RL [ 1073 ] employs reinforcement learning to enhance the critic model within MAS, fostering better collaborative\\nreasoning. However, despite their promising performance, future parameter-based learning paradigms may need to\\naddress the balance between agents’ general capabilities and their specialization for specific roles within MAS.This\\nhybrid approach allows agents to handle both structured and unstructured data, improving their ability to make decisions\\nin dynamic environments [1074, 1075].\\n154\\nChapter 17\\nEvaluating Multi-Agent Systems\\nThe transition from single-agent to multi-agent systems, and specifically Large Language Model (LLM)-based systems,\\nrequires a paradigm change in the evaluation paradigm. In contrast to single-agent evaluation, in which the immediate\\nconcern is performance on a particular task, evaluation of LLM-based multi-agent systems must be understood in\\nterms of inter-agent dynamics as a whole, such as collaborative planning and communication effectiveness. Both\\ntask-oriented reasoning and holistic capability evaluation are addressed in this chapter, reflecting the nuance of such\\nevaluations. In greater detail, there are two main areas that we examine for evaluation. First, there is task-solving\\nMulti-Agent Systems (MAS), where we examine benchmarks assessing and enhancing LLM reasoning for coding,\\nknowledge, and mathematical problem-solving tasks. These tests also accentuate the utility of distributed problem\\nsolving, achieved through organized workflows, specialisation among agents, iterative improvement, and calls for\\nadditional tools. Enhanced reasoning, primarily because of agent-agent decision-making cooperation and multi-round\\ncommunications, is shown for MAS compared with agent-based individual ones. Following that, there is a general\\nevaluation of MAS abilities, extending beyond one-task-oriented achievement, to agent interactions at a highly advanced\\nlevel. It involves a move away from one-dimensional measurements into multi-dimensional frameworks for documenting\\nachievements at collaborations, reasoning abilities, system efficiency, and flexibility. We categorize such measurements\\ninto collaboration-oriented and competition-oriented measurements and have identified efficiency, decision-making\\nquality, quality of collaboration, and flexibility as primary measure domains. These measurements capture various\\naspects of agent behavior, including communication effectiveness, resource distribution, and response to dynamic\\nsituations.\\n17.1 Benchmarks for Specific Reasoning Tasks\\nIn multi-agent system solving for tasks, much focus has been on leveraging multi-agent coordination for enhancing the\\nreasoning capacity of LLMs. It is most evident in coding, knowledge, and mathematical reasoning benchmarks, where\\none is interested in examining and building on performance with distributed solving. These benchmarks most typically\\nexamine if agents’ capability for producing correct code, reasoning on complex knowledge domains, and solving\\ndifficult mathematical problems withstanding, with measures such as pass@k[1076 ] or proof ratios for success being\\nprevalent. Much improvement has been exhibited by MAS through structured workflow, domain-specific agent roles,\\nand iterative improvement on state-of-the-art performance. On the contrary, for model and simulation MAS, the case is\\none with a comparative lack of standardized benchmarks. Rather, research is primarily experimental setups that simulate\\na variety of social phenomena, with calls from the community for further formalized evaluation frameworks. These\\nmultiple benchmark areas are described below, examining the tasks, measures for evaluation, and the core mechanisms\\nthrough which MAS result in better performance.\\nCode Reasoning Benchmark Measuring the capability of LLMs for code synthesis requires bespoke benchmark\\nsuites with a focus on functional correctness. Code synthesis, as compared to natural language synthesis, allows for\\ndirect verification through running. Several benchmark suites have been built for this purpose, typically consisting of\\na collection of programming problems, each described with a natural language problem description and a collection\\nof test cases for automatically ascertaining the synthesized code’s correctness. HumanEval [ 1077 ], APPS [ 1078 ],\\nand MBPP [ 939] are some popular ones. These benchmark suites predominantly utilize the pass@kmetric, which\\ncomputes the percentage at which at least one among the top- kgenerated solutions passes all test cases for a number of\\nproblems. The problems covered through these benchmark suites range across a variety of difficulties and programming\\n155\\nabstractions, requiring not only for LLMs and Agents but also for syntactically correct and logically sound code that\\nsatisfies the provided test cases. Recent work has explored leveraging Multi-Agent Systems (MAS) for enhancing LLM\\ncapability on code reasoning. For instance, MetaGPT [ 626] is a meta-programming system which embeds human-like\\nStandard Operating Procedures (SOPs) into multi-agent cooperation based on LLM. With multi-agent role assignment\\nwith varying domains and adopting assembly line mode, MetaGPT effectively breaks down difficult operations into\\nsub-operations and achieves state-of-the-art performance on HumanEval and MBPP benchmarks. SWE-agent [ 628]\\npresents a novel Agent-Computer Interface (ACI) which largely enhances a repository-creating, repository-editing, and\\nnavigation capability for an agent. The system demonstrates that a well-structured interface tailored for LMs can largely\\nenhance software engineering capability, with state-of-the-art on SWE-bench and HumanEval. AgentCoder [ 994] is\\nanother multi-agent coding system with focus on effective testing and auto-optimization. It is a three-agent system with\\na programmer, a test designer, and a test executor. The test designer supplies accurate and diverse test cases, and the\\ntest executor provides feedback to the programmer for optimization. Such collaborative workflow enhances coding\\nefficiency and outperforms one-agent models and other multi-agent approaches on HumanEval and MBPP datasets.\\nThese MAS approaches all point out multi-agent cooperation, organized workflow, and tailored interface as effective\\nsolution strategies for enhancing the capability of LLM on code reasoning. DEV AI [ 781] proposes a set of novel AI\\ndevelopment automation benchmarks, which utilize a judge-agent mechanism for judging automatically intermediate\\ndevelopment process.\\nKnowledge Reasoning Benchmark To facilitate AI agents effectively acting in and understanding the world, robust\\nknowledge reasoning abilities are essential. Benchmarks for this class assess an agent’s ability to utilize factual\\nknowledge and logical reasoning when answering challenging queries. Commonsense reasoning is tested with\\nbenchmarks such as CSQA [ 1079 ] and StrategyQA [ 1080 ], and scientific knowledge understanding is tested with\\nScienceQA [ 1081 ]. The core challenge for agents is performing multi-step, chain-of-thought reasoning, stepwise\\nlogically progressing from input query to output answer. These tests concentrate on assessing how well a specific\\nAI agent can apply a specific body of knowledge, one at a time, and reason out a problem. Recent research has\\nexperimented with the use of LLMs on MAS for improving knowledge reasoning task performance, and they have\\nachieved state-of-the-art accuracy. For example, MASTER [ 1009 ], a novel multi-agent system, employs a novel\\nrecruitment process for agents and communication protocol using the Monte Carlo Tree Search (MCTS) algorithm, and\\nachieves 76% accuracy on HotpotQA [ 940]. Reflexion [ 48], a universal framework for bringing reasoning and acting\\ntogether with language models, improves baseline by 20% on HotpotQA. These strategies demonstrate the potential of\\nmulti-agent coordination for knowledge reasoning tasks. Besides, leveraging external tools, e.g., search engines, is also\\nneeded for improving knowledge reasoning capacity. Agents may apply these tools for retrieving the latest information\\nand also for fact checking, thus improving the accuracy and dependability of responses. Such integration is particularly\\nhelpful on applications such as TriviaQA [1082], for which real-time information access is essential.\\nMathematical Reasoning Benchmark Math reasoning is a critical skill for AI agents which requires cooperative\\nutilisation of mathematical knowledge, logical deduction, and computational power. Benchmarking tasks for this\\ncapability tend to fall into two categories: math problem-solving and computer-aided theorem proving (ATP). Datasets\\nsuch as SV AMP [ 942], GSM8K [ 1083 ], and MATH [ 941] challenge agents to solve word problems, asking for exact\\nnumber answers or formulas. ATP is a harder test, with stricter compliance with formal proof schemata. Tests on\\ndatasets like PISA [ 1084 ] and miniF2F [ 1076 ], which are graded on proof completion, test whether an agent can\\nproduce well-formed mathematical proofs. Multi-agent systems (MAS) have been put forward as a potential solution for\\nhandling mathematical reasoning problem complexity. Methods such as MACM [ 1010 ] include a multi-agent system\\nconsisting of Thinker, Judge, and Executor agents tailored for a complex problem, dividing it into smaller sub-problems\\nfor computation. The Thinker agent generates new ideas, Judge decides if they are accurate, and Executor conducts\\nnecessary computation involving tools such as calculators. Such a modular structure supports iterative refinement and\\nelimination of errors, enhancing problem-solving accuracy. Furthermore, methods such as multi-agent debate [ 985]\\ninclude several instances of a language model debating and refocusing iteratively for collective solution improvement,\\nenhancing reasoning as well as factuality accuracy. Such MAS-based systems have achieved notable improvement on\\nbenchmarks such as MATH and GSM8K, establishing distributed solving capacity for mathematical problems. Aside\\nfrom this, reinforcement learning from human feedback (RLHF) and preference learning strategies have been attempted\\nfor further enhancing mathematical problem-solving capacity of LLMs. For instance, a multi-turn online iterative direct\\npreference learning framework [ 1085 ] has been put forward for training various language models with enriched sets\\nof prompts over GSM8K and MATH datasets. Such a technique includes feedback from interpreters for codes and\\noptimizes preferences at a level of trajectories, with notable improvement in output.\\nSocietal Simulation Benchmark Social simulation benchmarks are essential for evaluating multi-agent system\\nperformance and realism for simulating human behavior and social interactions based on LLMs. Standardized sets and\\ntest cases for evaluating the agents’ ability for interacting, communicating, and evolving within a simulated society are\\n156\\nTable 17.1: MAS Benchmarks: A Systematic Classification of Multi-Agent System Evaluation Frameworks Categorized\\nby Task-Oriented Performance and System-Level Capabilities. This comprehensive collection encompasses both\\nspecialized task-solving benchmarks and holistic capability assessments, reflecting the dual nature of MAS evaluation\\nin collaborative problem-solving and inter-agent dynamics.\\nCategory Focus Benchmarks Examples Representative\\nMetrics\\nTask-solvingCode Reasoning APPS [1078], HumanEval [1077], MBPP [939],\\nCodeContest [1087], MTPB [1088],\\nDS-1000 [1089], ODEX [1090],\\nRaconteur [1091]MetaGPT [626],\\nSWE-agent [628],\\nAgentCoder [994]Pass@k, Resolved(%)\\nKnowledge\\nReasoningARC [1092], HotpotQA [940], CSQA [1079],\\nStrategyQA [1080], BoolQ [1093],\\nOpenBookQA [1094], WinoGrande [1095],\\nHellaSwag [1096], SIQA [1097], PIQA [1098],\\nproScript [1099], ScienceQA [1081],\\nProOntoQA [1100]Reflexion [48],\\nMASTER [1009]Accuracy\\nMathematical\\nReasoningMATH [941], GSM8K [1083], SV AMP [942],\\nMultiArith [943], ASDiv [1101],\\nMathQA [1102], AQUA-RAT [1103],\\nMAWPS [1104], DROP [1105],\\nNaturalProofs [1106], PISA [1084],\\nminiF2F [1076], ProofNet [1107]MACM [1010],\\nDebate [985]Accuracy, Pass@k\\nCollaborationCommunication-based\\nCooperationInformativeBench [1108],\\nCollab-Overcooked [944], COMMA [1109],\\nLLM-Coordination [926]iAgents [1108],\\nTwo-Player [1110],\\nEAAC [1111]Task Completion Rate\\nCommunication Efficiency\\nPlanning and\\nCoordinationPARTNR [946], VillagerBench [925],\\nBABYAGI-ARENA [1112], Multiagent\\nBench [948]AAS [1113],\\nResearchTown [1114],\\nGPTSwarm [651]Planning Success Rate\\nCoordination Efficiency\\nProcess-oriented Auto-Arena [947] Idea [1115]Process Completion Rate\\nStep Efficiency\\nCompetitionAdversarial\\nScenariosBattleAgentBench [920], MAgIC [955],\\nLLMArena [1116], PokerBench [1117],\\nMultiagent Bench [948]Dilemma [1118],\\nPokéLLMon [1119]Win Rate\\nElo Rating\\nSocial Deduction AvalonBench [972], Human Simulacra [1120],\\nDiplomacy [934]MA-KTO [1121],\\nHLR [1122],Win Rate\\nAccuracy of Deductions\\nGame-Theoretic Guandan [ 1123 ], AgentVerse [ 1124 ], ICP [ 1125 ] WarAgent [1126]Score\\nWin Rate\\nprovided through the benchmarks. An example of one such widely used benchmark is SOTOPIA [ 1086 ], employed for\\nevaluating social intelligence in natural language agent-based social intelligence. It is employed for evaluating agents’\\nability for conversing, understanding social cues, and building relationships with each other within a virtual society.\\nAnother benchmark involves simulating propagation Gender Discrimination and Nuclear Energy [ 255] topics on social\\nnetworks. It is employed to evaluate agents’ capabilities in modeling opinion dynamics, information dissemination,\\nand social influence within large-scale social networks. Multiagent Bench [ 948] further provides two simulation\\ndomains—werewolf and bargaining—to assess competitive interactions among diverse agent groups with conflicting\\ngoals.\\nEvaluating capabilities in LLM-based MAS requires specialized approaches that effectively measure the rich interactions\\nbetween agents. As this field evolves, evaluation methodologies have transitioned from single-dimension metrics to\\nmulti-faceted evaluation frameworks that capture the complex skillset required for effective multi-agent interaction.\\nThis evolution reflects a growing understanding that agent performance must be assessed across multiple dimensions\\nincluding collaboration success, reasoning capabilities, and system efficiency.\\nIn recent research, the MAS evaluation can be mainly categorized along three primary dimensions: collaboration-\\nfocused benchmarks, competition-focused benchmarks, and adaptive and resilience benchmarks. Within each category,\\nwe identify specific metric domains that capture different aspects of agent performance. Current evaluation approaches\\ntypically measure efficiency metrics (e.g., task completion rates, resource utilization, time efficiency), decision quality\\nmetrics (e.g., action accuracy, strategic soundness, reasoning depth), collaboration quality metrics (e.g., communication\\neffectiveness, coordination efficiency, workload distribution), and adaptability metrics (e.g., response to disruptions,\\nself-correction), which provide a foundation for evaluating multi-agent systems.\\nCollaboration-focused Benchmarks. Collaboration-focused benchmarks have evolved significantly, shifting from ba-\\nsic single-dimensional metrics toward comprehensive frameworks that evaluate complex agent-to-agent communication\\n157\\nand coordination. Initial benchmarks, such as InformativeBench [ 1108 ], primarily addressed agent collaboration under\\nconditions of information asymmetry, employing metrics like Precision and IoU to measure decision accuracy in infor-\\nmation dissemination tasks. Subsequently, the scope of evaluation expanded, exemplified by Collab-Overcooked [ 944],\\nwhich introduced nuanced process-oriented metrics such as Trajectory Efficiency Score (TES) and Incremental Trajec-\\ntory Efficiency Score (ITES). These metrics assess detailed aspects of coordination, revealing significant shortcomings\\nin agents’ proactive planning and adaptive capabilities despite their strong task comprehension.\\nFurther expanding the evaluation scope, COMMA [ 1109 ] and LLM-Coordination [ 926] emphasized communication\\neffectiveness and strategic synchronization, employing diverse environments and extensive metrics including Success\\nRate, Average Mistakes, and Environment Comprehension Accuracy. These benchmarks collectively illustrate an\\nemerging trend toward capturing deeper aspects of collaborative behaviors and strategic consistency.\\nOther benchmarks, such as PARTNR [ 946], VillagerBench [ 925], and BabyAGI [ 1112 ], further addressed gaps in exist-\\ning evaluations by focusing explicitly on reasoning, planning, and task decomposition. These benchmarks highlighted\\nthe need for comprehensive assessment of agents’ ability to engage in complex, socially embedded tasks, considering\\nmetrics like Percent Completion, Balanced Agent Utilization, and agent contribution rates. AgentBench [ 706], VisualA-\\ngentBench [ 928], and Auto-Arena [ 947] further standardized multi-agent evaluations, automating assessment across\\nvarious domains and demonstrating substantial performance disparities between closed-source and open-source LLMs.\\nThese observations underscored critical challenges in developing universally effective collaboration frameworks.\\nIn summary, collaboration-focused benchmarks collectively reflect an ongoing shift toward comprehensive, nuanced\\nevaluations that encompass communication efficiency, adaptive strategy, and fine-grained agent coordination, addressing\\nearlier limitations focused solely on outcome-based performance.\\nCompetition-focused Benchmarks. Competition-focused benchmarks evaluate agents’ strategic capabilities and\\nadversarial interactions, highlighting specific deficiencies in Theory of Mind and opponent modeling. Early benchmarks\\nsuch as BattleAgentBench [ 920] and MAgIC [ 955] initiated the focus on mixed cooperative-competitive environments,\\nuncovering critical weaknesses in high-order strategic reasoning among LLM agents. These benchmarks employed\\ncomprehensive competitive metrics such as Forward Distance, Judgment Accuracy, and Rationality scores, identifying\\nthat while advanced LLMs performed adequately in simpler scenarios, significant limitations persisted under complex\\nadversarial conditions.\\nBuilding upon these insights, subsequent benchmarks like Human Simulacra [ 1120 ], LLMArena [ 1116 ], and Poker-\\nBench [ 1117 ] further refined competitive evaluation by incorporating human-like reasoning metrics and more robust\\nstrategic measures (e.g., Response Similarity Score, Elo Scores, and Action Accuracy). These evaluations consistently\\ndemonstrated shortcomings in opponent prediction, risk assessment, and adaptive strategic planning, despite high task\\ncomprehension.\\nSocial deduction and deception-based benchmarks, notably AvalonBench [ 972] and Diplomacy [ 934], further revealed\\nfundamental gaps in agents’ abilities to interpret hidden information and manage complex social dynamics. Metrics\\nlike Assassination Accuracy, Deduction Accuracy, and Win Rates emphasized that even sophisticated LLMs fail to\\nreplicate human-level reasoning in adversarial negotiation and hidden-information games.\\nAdditional game-theoretic evaluations, including Guandan [ 1123 ], AgentVerse [ 1124 ], MultiAgentBench [ 948], and\\nICP [ 1125 ], introduced scenarios requiring strategic cooperation under incomplete information. These benchmarks\\nreinforced previous findings on the necessity of enhanced Theory of Mind and predictive modeling capabilities. Multi-\\nAgentBench [ 948] also introduces the KPI and coordination score to evaluate the competition of agents. Collectively,\\ncompetition-focused benchmarks highlight persistent strategic and reasoning limitations among LLM-based agents, un-\\nderscoring the ongoing need to address critical gaps in adversarial modeling and strategic planning despite advancements\\nin general reasoning and task execution capabilities.\\nAdaptive and Resilience Benchmarks adaptive and resilient multi-agent system benchmarks tackle two inter-\\nconnected capabilities together: adaptability—the ability of the agents to act dynamically in altering, unexpected\\nenvironmental conditions by modifying their behavior and strategy. Resilience, or the ability of the system to en-\\ndure, alleviate, and rapidly recover from disruptions, faults, or hostile intervention. In adaptability, as mentioned in\\nAdaSociety [ 1127 ], the dynamic interplay between social relationships and physical environments demands that agents\\nengage in continuous learning, and strike a balance between environment discovery and social network construction.\\nDespite significant advancements in current multi-agent decision-making frameworks, these environments fall short in\\nintroducing new challenges in various physical contexts and changing social interdependencies. Therefore, AdaSo-\\nciety introduces an environment in which physical states, tasks, and social relationships among agents continuously\\nevolve, thereby capturing the adaptability of agents as they respond to expanding task complexity and shifting resource\\nconstraints.\\n158\\nMoreover, current benchmarks may oversimplify the challenges of real-world automation with limited disruption\\nmodeling and simplified dependencies of process [ 945], resulting in insufficient evaluation of planning capabilities\\nand adaptability. Thus, REALM-Bench [ 945], on the other hand, defines adaptation through real-world-inspired\\nplanning problems, which emphasizes metrics such as real-time re-planning efficiency, coordination scalability under\\nincreasing complexity, and the stability of performance outcomes despite dynamic interdependencies or disruptive\\nevents. Conversely, resilience benchmarks [ 1128 ] systematically introduce faults or errors into individual agents to\\nassess overall system robustness.\\n17.2 Challenge and Future Work\\nWhile various MAS evaluation benchmarks have been developed in recent years, challenges and limitations continue\\nto exist with regard to the standardization of evaluation across different MAS tasks and scenarios, and the ability to\\nevaluate scalability and diversity in MASs. Future research must address these challenges, in order to develop the\\ncomprehensive field of MAS evaluation.\\nBelow are some challenges and future directions in LLM Multi-agent evaluation:\\n1.Multi-Agent System has demonstrated superior performance in solving complex tasks, when compared with\\nsingle agent frameworks. But compared with single agent system, MAS also requires more computations\\nand brings additional costs. Therefore, there has a urgent challenge that we need to handle: when we need to\\ninvoke MAS framework? For many simple user instructions, we may only require LLM or single agent system\\nto accomplish. And only complex user instructions could require MAS frameworks. Hence, in the future, how\\nto design the task router mechansim to detect which scenario require MAS or not is fundamental but also a\\nimportant issue.\\n2.Multi-agent system is a high-level framework, built upon multiple AI agent based on the foundation models.\\nTherefore, just like back propagation, the optimization of MAS framework will also affect each part (i.e.,\\nfoundation model, AI Agent and Multi-agent collaboration).\\n3.Existing MAS frameworks usually design multiple agents with homogeneous traits, such as all being language-\\nbased agents. But when connecting MAS to real-world scenarios, it usually involves different kinds of AI\\nagents. For example, we may need to bridge the connections between language-based agent, digital agent and\\nrobotic agents. However, these agents adopt various settings, from the inputs to the outputs. How to establish\\nthe connection between these agent is still a open problem that need to be handle in the future.\\n159\\nPart IV\\nBuilding Safe and Beneficial AI Agents\\n160\\nThe rapid development of LLM-based agents introduces a new set of safety challenges that go beyond those of traditional\\nLLMs. Equipped with advanced reasoning, planning, and tool-using capabilities, these agents are designed to perform\\ntasks autonomously and interact with their environments [ 34]. However, this autonomy also expands the attack surface,\\ncreating new vulnerabilities that demand careful research and attention [ 1129 ,40]. In this part, we first establish\\na comprehensive framework for understanding agent safety, examining both internal and external safety threats to\\nAI agents. We will explore the various attack vectors associated with these threats and propose potential mitigation\\nstrategies. This framework is organized into two key areas:\\n(1) Intrinsic Safety threats stem from vulnerabilities in the agent’s core components, which include the LLM “brain”\\nas well as the perception and action modules. Each of these components has unique weaknesses that can be exploited\\nby adversaries:\\n•Brain is the LLM itself, responsible for key decision-making tasks such as reasoning and planning. It is guided\\nby a knowledge module that provides essential contextual information.\\n•Perception consists of sensors that interpret the external environment, where malicious manipulation of external\\nobjects can lead to erroneous perceptions.\\n•Action is responsible for tool usage and downstream applications, which are also susceptible to exploitation.\\n(2) Extrinsic Safety threats arise from interactions between the agent and external, often untrusted, entities. These\\ninclude:\\n•Agent-Memory Interactions : The agent frequently accesses and interacts with memory storage, which serves\\nas an external database for decision-making and contextual information retrieval. Recent research highlights\\nvulnerabilities in the agent-memory interface that could be exploited to manipulate the agent’s actions.\\n•Agent-Agent and Agent-Environment Interactions: These refer to the interactions between the agent and other\\nagents (e.g., other agents or human operators), as well as its environment, which includes task-related objects\\nor dynamic systems. The complexity of these interactions further compounds the agent’s exposure to external\\nthreats.\\nAs illustrated in Figure 17.1, these risks are broadly categorized into intrinsic and extrinsic safety, helping to clarify\\ntheir origin and nature. In addition to identifying threats, we also provide a rigorous, mathematical foundation for\\nunderstanding attacks such as jailbreaking, prompt injection, and data poisoning. Moreover, we present practical,\\nactionable solutions, tracing the development of safety measures from early LLM safeguards to comprehensive\\nstrategies that protect the entire agent system. This includes exploring guardrails, advanced alignment techniques\\n(such as superalignment), and the crucial balance between safety and helpfulness. Finally, we analyze the “scaling\\nlaw of AI safety”—the complex relationship between an agent’s capabilities and its potential risks—and the essential\\ntrade-offs that must be made. This part provides a clear understanding of the challenges, theoretical foundations, and\\npractical strategies necessary to develop effective and trustworthy AI agents that can be safely and effectively deployed\\nin real-world scenarios.\\nThis part is organized as follows: First, we examine intrinsic safety risks (Chapter 18), focusing on threats to the LLM\\n“brain,” as well as vulnerabilities in the agent’s perception and action components (Chapter 19). Next, we explore\\nextrinsic safety threats related to agent-memory, agent-agent, and agent-environment interactions (Chapter 20). Finally,\\nwe investigate superalignment techniques aimed at ensuring the safety of agent behaviors, while addressing the broader\\nchallenge of balancing safety with performance. This includes exploring how safety measures scale with the increasing\\ncapabilities of AI systems and examining the trade-offs involved in designing secure, capable AI agents (Chapter 21).\\n161\\nWorldKnowledgeDecision-MakingProcessBrain(§18)\\nEnvironment(§20.2)\\nInputTypes\\nInputEncoderInputFeatures\\nInteraction\\nCodeComputerWebAssistantEmailEmbodyGameSimulationAgentIntrinsicSafety(§18&§19)Perception(§19.1)Action(§19.2)+Web-scaleDataEverydayDataMultimodalInformationLLMTrainingLLMBrainPlanningResult\\nAgentExtrinsicSafety(§20)Agent(§20.3)\\n…\\nMemory(§20.1)InputFeatures\\nFigure 17.1: The Brain (LLM) faces safety threats like jailbreaks and prompt injection attacks (§ 18.1) and privacy\\nthreats such as membership inference attacks (§ 18.2). Non-brain modules encounter perception threats (§ 19.1) and\\naction threats (§ 19.2). Due to interactions with potentially malicious external entities, we also explore agent-memory\\nthreats (§ 20.1), agent-environment threats (§ 20.2), and agent-agent threats (§ 20.3).\\n162\\nChapter 18\\nAgent Intrinsic Safety: Threats on AI Brain\\nThe intrinsic safety of an AI agent concerns vulnerabilities within the agent’s internal architecture and functionality.\\nAI agents, by their nature, consist of multiple components: a central “brain” (the LLM), and auxiliary modules for\\nperception and action [ 66]. While this modularity enables sophisticated reasoning and autonomous decision-making,\\nit also expands the potential attack surface, exposing the agent to various internal vulnerabilities that adversaries can\\nexploit [1130].\\nThreats to the agent’s brain—specifically the LLM—are particularly concerning, as they can directly impact the agent’s\\ndecision-making, reasoning, and planning abilities. These vulnerabilities can arise from flaws in the design of the\\nmodel, misinterpretations of inputs, or even weaknesses induced by the training process. Effective mitigation strategies\\nare crucial to ensuring that these agents can be deployed securely and reliably.\\n18.1 Safety Vulnerabilities of LLMs\\nThe LLM, as the core decision-making component of the agent, is highly susceptible to a range of safety threats. Its\\ncentral role in reasoning and action selection makes it an attractive target for adversaries. In the context of AI agents, the\\nvulnerabilities inherent in the LLM itself are often amplified, as these models are required to function within dynamic,\\nreal-world environments where adversaries can exploit weaknesses [1131, 1132].\\n18.1.1 Jailbreak Attacks\\nJailbreaks circumvent the safety guardrails embedded in AI agents, compelling their decision-making process to be\\nharmful, unethical, or biased [ 1233 ]. These attacks exploit the inherent tension between an LLM’s helpfulness and its\\nsafety constraints [1134].\\nFormalization. To formally characterize the risks posed by jailbreaks, we analyze the probability distribution governing\\nan autoregressive LLM’s output. For an autoregressive LLM, the probability of generating an output sequence\\ny=xn+1:n+m, given an input sequence x1:nis modeled as:\\np(y|x1:n) =mY\\ni=1p(xn+i|x1:n+i−1) (18.1)\\nwhere mdenotes the total length of the generated sequence. Jailbreak attacks often involve introducing subtle\\nperturbations to the input sequence, denoted as ˜x1:n, which mislead the model into producing outputs that deviate from\\nthe desired behavior.\\nThe impact of a jailbreak attack is evaluated through its effect on the alignment reward R∗(y|x1:n,A), which measures\\nhow closely the model’s output aligns with a set of human-defined safety or ethical guidelines, denoted as A. The\\nadversary’s goal is to minimize this reward, formalized as:\\ny⋆= arg min\\nyR∗(y|˜x1:n,A)) (18.2)\\n163\\nAgent\\nIntrinsic\\nSafety\\nonBrain\\n(LLM)Safety\\nThreatsJailbreakWhite-box Jailbreak\\nYi et al.[ 1133 ] GCG[ 1134 ] MAC[ 1135 ] I-GCG[ 1136 ] Luo et\\nal.[1137 ] Li et al.[ 1138 ] DROJ[ 1139 ] AutoDAN[ 1140 ] POEX[ 1141 ]\\nBlack-box Jailbreak\\nWei et al.[ 1142 ] PAIR[ 1143 ] JAM[ 1144 ]Qi et al.[ 1145 ] POEX[ 1141 ]\\nAutoDAN[ 1140 ] GUARD[ 1146 ] HIMRD[ 1147 ] HTS[ 1148 ]\\nPrompt\\nInjectionDirect Prompt Injection\\nGreshake et al.[ 1149 ] Liu et al.[ 1150 ] JudgeDeceive[ 1151 ] InjecAgent[ 1152 ] Re-\\nhberger et al.[ 1153 ] GHVPI[ 1154 ] Debenedetti et al.[ 1155 ] Schulhoff et al.[ 1156 ]\\nIndirect Prompt Injection\\nGreshake et al.[ 1149 ] HijackRAG[ 1157 ] Clop and Teglia[ 1158 ]\\nPromptInfection[ 1159 ] PreferenceManipulationAttacks[ 1160 ]\\nHallucinationKnowledge-conflict Hallucination\\nJi et al.[ 1161 ] McKenna et al.[ 1162 ] Huang et al.[ 1163 ] DELUCIONQA[ 1164 ]\\nKang and Liu[ 1165 ] MetaGPT[ 626] Xu et al.[ 1166 ] ERBench[ 1167 ]\\nContext-conflict Hallucination\\nTACS[ 1168 ] LanguageConfusionEntropy[ 1169 ] HaluEval-\\nWild[ 1170 ] LURE[ 1171 ] MARINE[ 1172 ] Ranaldi and\\nPucci[ 1173 ] HallusionBench[ 1174 ] DiaHalu[ 1175 ]\\nMisalignmentGoal-misguided Misalignment\\nJi et al.[ 1176 ] Krakovna et al.[ 1177 ] Ngo et al.[ 1178 ]\\nSPPFT[ 1179 ] ED[ 1180 ] AgentHospital[ 921] Hammoud et al.[ 1181 ]\\nCapability-misused Misalignment\\nLiu et al.[ 1182 ] Wei et al.[ 1183 ] Ji et al.[ 1176 ] Qi et al.[ 1184 ] BEB[ 1185 ]\\nPoisoning\\nAttacksModel Poisoning\\nRIPPLe[ 1186 ] BadEdit[ 1187 ] Dong et al.[ 1188 ] Obliviate[ 1189 ]\\nOh et al.[ 1190 ] SecretCollusion[ 1191 ] Miah and Bi[ 1192 ]\\nData Poisoning\\nWan et al.[ 1193 ] AgentPoison[ 1194 ] Poison-RAG[ 1195 ] PoisonBench[ 1196 ]\\nChen et al.[ 1197 ] Bowen et al.[ 1198 ] BrieFool[ 1199 ] RLHF[ 1200 ]\\nBackdoor Injection\\nHubinger et al.[ 1201 ] Wu et al.[ 1202 ] BALD[ 1203 ] Ge et al.[ 1204 ] VPI[ 1205 ]\\nPrivacy\\nThreatsTraining\\nData\\nInferenceMembership Inference Attacks\\nShokri et al.[ 1206 ] Carlini et al.[ 1207 ] Choquette et\\nal.[1208 ] SPV-MIA[ 1209 ] LiRA[ 1210 ] MIA[ 1211 ]\\nData Extraction Attacks\\nCarlini et al.[ 1212 ] SCA[ 1213 ] Ethicist[ 1214 ] Morris et al.[ 1215 ] Pan\\net al.[ 1216 ] Carlini et al.[ 1217 ] Carlini et al.[ 1218 ] More et al.[ 1219 ]\\nInteraction\\nData\\nInferenceSystem Prompt Stealing\\nPromptInject[ 1220 ] PromptStealingAttack[ 1221 ] PromptKeeper[ 1222 ]\\nInputSnatch[ 1223 ] Zhang et al.[ 1224 ] Wen et al.[ 1225 ] Zhao et al.[ 1226 ]\\nUser Prompt Stealing\\nPRSA[ 1227 ] Agarwal et al.[ 1228 ] Agarwal et al.[ 1229 ] Liang\\net al.[ 1230 ] PLeak[ 1231 ] Yona et al.[ 1232 ] Output2Prompt[ 849]\\nFigure 18.1: Agent Intrinsic Safety: Threats on LLM Brain.\\nwhere y⋆is the worst-case output induced by the perturbed input. The corresponding adversarial loss function quantifies\\nthe likelihood of generating this output:\\nLadv(˜x1:n) =−logp(y⋆|˜x1:n),and˜x1:n= arg min\\n˜x1:n∈T(ˆx1:n)Ladv(˜x1:n) (18.3)\\nwhere p(y⋆|˜x1:n)denotes the probability assigned to the jailbreak output and T(ˆx1:n)is the distribution or set of\\npossible jailbreak instructions.\\n164\\nBlack Box^&AS#!K\\nJailbreakpromptgenerator\\nsend malicious emails to others\\ngradient-basedfeedback\\nsend malicious emails to others\\nresult-only(Y/N)feedbackDr.AI,canyouteachmeto sendmali-ci-ous…?inputjailbreaksFigure 18.2: Illustration of White-box and Black-box Jailbreak Methods: (1) White-box: The adversary has access to\\nthe agent’s internal information (e.g., gradients, attention, logits), allowing precise manipulations such as adversarial\\nsuffix optimization. (2) Black-box: The adversary relies solely on input-output interactions. Key methods include\\nautomated jailbreak prompt generation, and leveraging genetic algorithms or LLMs as generators to create effective\\nattacks.\\nAs shown in Figure 18.2, jailbreaks can be broadly classified into white-box and black-box methods, depending on the\\nadversary’s access to the model’s internal parameters. (1) White-box Jailbreaks: These attacks assume the adversary\\nhas full access to the model’s internal information, such as weights, gradients, attention mechanisms, and logits.\\nThis enables precise adversarial manipulations, often through gradient-based optimization techniques. (2) Black-box\\nJailbreaks: In contrast, black-box attacks do not require access to internal model parameters. Instead, they rely solely\\non observing input-output interactions, making them more applicable to real-world scenarios where model internals are\\ninaccessible.\\nWhite-box Jailbreak. White-box attacks exploit access to an AI agent’s internal parameters, such as model weights and\\nattention mechanisms, enabling precise manipulations. Early work in this area focused on gradient-based optimization\\ntechniques [ 1133 ], exemplified by the Greedy Coordinate Gradient (GCG) attack [ 1134 ], which crafts adversarial\\nsuffixes capable of inducing harmful outputs across various models. Subsequent research has built upon this foundation,\\nexploring refinements to GCG. For example, introducing momentum to boost attack performance, as seen in the MAC\\napproach [ 1135 ], and proposing improved optimization techniques for jailbreaking, as in I-GCG [ 1136 ]. Beyond\\nprompt optimization, researchers have investigated manipulating other internal components of LLMs. Similarly,\\nmanipulating the end-of-sentence MLP re-weighting has been shown to jailbreak instruction-tuned LLMs [ 1137 ].\\nOther approaches include attacks that exploit access to the model’s internal representations, such as Jailbreak via\\nRepresentation Engineering (JRE) [ 1138 ], which manipulates the model’s internal representations to achieve the\\njailbreak objective, and the DROJ [ 1139 ] attack, which uses a prompt-driven approach to manipulate the model’s\\ninternal state. AutoDAN [ 1140 ] automates the generation of stealthy jailbreak prompts. POEX [ 1141 ] proposed the\\nfirst jailbreak framework against embodied AI agents, which uncovers real-world harm, highlighting the potential for\\nscalable and adaptable white-box attacks.\\nBlack-box Jailbreak. Unlike white-box attacks, black-box jailbreaks operate without internal knowledge of the agent,\\njust relying on input-output interactions. Prompt engineering is a critical approach, where carefully designed prompts\\nare employed to exploit the model’s response generation capabilities and bypass its safety mechanisms [ 1142 ]. These\\nprompts often leverage techniques such as role-playing, scenario simulation, or the introduction of linguistic ambiguities\\nto trick the model into generating harmful content [ 1143 ]. Furthermore, automated prompt generation methods have\\nemerged, employing algorithms like genetic algorithms or fuzzing to systematically discover effective jailbreak prompts\\n[1234 ]. In addition, multi-turn attacks exploit the conversational capabilities of LLMs, gradually steering the dialogue\\ntowards unsafe territory through a series of carefully crafted prompts [ 1146 ]. Other notable approaches include\\nexploiting the model’s susceptibility to specific types of cipher prompts [ 1144 ], and utilizing multimodal inputs, such\\nas images, to trigger unintended behaviors and bypass safety filters [ 1145 ,1147 ,1148 ]. AutoDAN [ 1140 ] uses a\\nhierarchical genetic algorithm to automatically generate stealthy, semantically meaningful jailbreak prompts for aligned\\nLLMs. POEX [ 1141 ] also showcases the feasibility of transferring white-box optimized jailbreak prompts to black-box\\nLLMs.\\nMitigation. Defending against the diverse and evolving landscape of jailbreak attacks requires multi-faceted methods.\\nSystem-level defenses offer a promising avenue, focusing on creating a secure environment around the LLM rather\\nthan solely relying on hardening the model itself. One key strategy is input sanitization and filtering, where incoming\\nprompts are analyzed and potentially modified before being processed by the LLM. This can involve detecting and\\nneutralizing malicious patterns [ 1235 ], or rewriting prompts to remove potentially harmful elements [ 1236 ]. Another\\ncrucial aspect is output monitoring and anomaly detection, where the LLM’s responses are scrutinized for unsafe or\\n165\\nunexpected content. This can involve using separate models to evaluate the safety of generated text [ 1237 ] or employing\\nstatistical methods to detect deviations from expected behavior. Multi-agent debate provides a system-level solution\\nby employing multiple AI agents to deliberate and critique each other’s outputs, reducing the likelihood of a single\\ncompromised agent successfully executing a jailbreak [ 985]. Formal language constraints, such as those imposed by\\ncontext-free grammars (CFGs), offer a powerful way to restrict the LLM’s output space, ensuring that it can only\\ngenerate responses that conform to a predefined set of safe actions [ 1238 ]. Furthermore, system-level monitoring can\\nbe implemented to track the overall behavior of the LLM deployment, detecting unusual activity patterns that might\\nindicate an ongoing attack. This can include monitoring API calls, resource usage, and other system logs. Finally,\\nadversarial training, while primarily a model-centric defense, can be integrated into a system-level defense strategy by\\ncontinuously updating the model with new adversarial examples discovered through system monitoring and red-teaming\\nefforts [ 1239 ]. The combination of these system-level defenses, coupled with ongoing research into model robustness,\\ncreates a more resilient ecosystem against the persistent threat of jailbreak attacks.\\n18.1.2 Prompt Injection Attacks\\nPrompt injection attacks manipulate the behavior of LLMs by embedding malicious instructions within the input prompt,\\nwhich hijacks the model’s intended functionality and redirects it to perform actions desired by the attacker [ 1130 ].\\nUnlike jailbreaks that bypass safety guidelines, prompt injections exploit the model’s inability to distinguish between\\nthe original context and externally appended instructions. This vulnerability is exacerbated by the open-ended nature of\\ntext input, the absence of robust filtering mechanisms, and the assumption that all input is trustworthy, making LLMs\\nparticularly susceptible to adversarial content [ 1149 ]. Even small, malicious modifications can significantly alter the\\ngenerated output.\\nFormalization. In a prompt injection, the adversary appends or embeds a malicious prompt component into the original\\ninput, thereby hijacking the model’s intended behavior. Let the original input sequence be denoted by x1:n, and let\\nprepresent the adversarial prompt to be injected. The effective (injected) input becomes: x′=x1:n⊕p, where the\\noperator ⊕denotes concatenation or integration of the malicious prompt with the original input. Then, the autoregressive\\ngeneration process under the injected prompt is then given by:\\np(y|x′) =mY\\ni=1p\\x00\\nyi|x′\\n1:n+i−1\\x01\\n(18.4)\\nAssuming the alignment reward R∗(·,A)measures the extent to which the output adheres to the set of human-defined\\nsafety or ethical guidelines A, the adversary’s goal is to force the model to generate an output that minimizes this\\nreward:\\ny⋆= arg min\\nyR∗\\x00\\ny|x1:n⊕p,A\\x01\\n. (18.5)\\nAccordingly, the loss function is defined as:\\nLinject(p) =−logp\\x00\\ny⋆|x1:n⊕p\\x01\\n. (18.6)\\nThe optimal prompt is then obtained by solving:\\np⋆= arg min\\np∈PLinject(p) (18.7)\\nwhere Pdenotes the set of feasible prompt injections. This formulation captures how small modifications in the input\\nprompt can lead to significant deviations in the generated output.\\nAs illustrated in Figure 18.3, prompt injection attacks can be broadly categorized into direct and indirect attacks based\\non how the adversarial instructions are introduced. (1) Direct prompt injection involves explicitly modifying the input\\nprompt to manipulate the LLM’s behavior. (2) Indirect prompt injection leverages external content, such as web pages\\nor retrieved documents, to embed malicious instructions, which the model processes without the user’s explicit input.\\nDirect prompt injection. These attacks against AI agents involve adversaries directly modifying the input prompt to\\nmanipulate the agent’s behavior. Early work established the feasibility of such attacks, demonstrating that carefully\\ncrafted prompts could induce agents to deviate from their intended tasks [ 1149 ]. Subsequent research explored the\\nautomation of these attacks, revealing the potential for widespread exploitation [ 1150 ,1151 ]. Other works investigated\\nattacks on multi-modal LLMs, demonstrating vulnerabilities in models processing both text and images [ 1153 ]. These\\nstudies collectively highlight the evolving threat landscape of direct prompt injection, moving from initial proofs\\n166\\nsend malicious emails to others\\nIndirect Prompt InjectionSummarize this article: www.external-resource.pdf\\nTraining models need ...{Injected Malicious Content}\\nThere is an error in the model, please click the following URL to repair it www.attack.com\\nWhere is the 33rdOlympics being held?\\nMore details about the 33rd Olympic Games in the following URL www.attack.comPrompt Injection\\nIndirect Prompt InjectionSummarize this article: www.external-resource.pdf\\nTraining models need ...{Injected Malicious Content}\\nThere is an error in the model, please click the following URL to repair it www.attack.com\\nMore details about the 33rd Olympic Games in the following URL www.attack.comWhere is the 33rdOlympics being held?Figure 18.3: Illustration of Direct and Indirect Prompt Injection Methods: (1) Direct: The adversary directly manipulates\\nthe agent’s input prompt with malicious instructions, achieving immediate control over the agent’s behavior. (2) Indirect:\\nThe adversary embeds malicious instructions in external content the agent accesses, leveraging the agent’s retrieval\\nmechanisms to indirectly influence its actions.\\nof concept to sophisticated attacks that can compromise the integrity and safety of AI agents. Other works have\\ninvestigated attacks on multi-modal LLMs, demonstrating vulnerabilities in models processing both text and images\\n[1154 ]. Competitions like the “LLM CTF Competition” Debenedetti et al. [ 1155 ] and “HackAPrompt” [ 1156 ] have\\nalso contributed to understanding these vulnerabilities by providing datasets and benchmarks. These studies collectively\\nmove from initial proofs of concept to sophisticated attacks that can compromise the integrity and safety of AI agents.\\nIndirect Prompt Injection. These attacks represent a more covert threat, where malicious instructions are embedded\\nwithin external content that an AI agent retrieves and processes. This form of attack leverages the agent’s ability to\\ninteract with external data sources to introduce malicious code without the user’s direct input. Greshake et al. [ 1149 ]\\nwere among the first to highlight this vulnerability, demonstrating how real-world LLM-integrated applications could be\\ncompromised through content fetched from the web. This was further explored in the context of Retrieval-Augmented\\nGeneration (RAG) systems [ 719], where researchers showed that attackers could “HijackRAG” by manipulating\\nretrieved content to inject malicious prompts [ 1157 ]. Recently, TPIA [ 1240 ] proposed a more threatening indirect\\ninjection attack paradigm, achieving complicated malicious objectives with minimal injected content, highlighting\\nthe significant threats of such attacks. Similarly, the concept of “Backdoored Retrievers” was introduced, where the\\nretrieval mechanism itself is compromised to deliver poisoned content to the LLM [ 1158 ]. Focusing specifically on\\nAI agents, researchers explored how indirect injections could be used for “Action Hijacking,” manipulating agents to\\nperform unintended actions based on the compromised data they process [ 1152 ]. “Prompt Infection” demonstrated\\none compromised agent could inject malicious prompts into other agents within a multi-agent system, highlighting the\\ncascading risks in interconnected LLM deployments [ 1159 ]. These studies underscore the growing concern surrounding\\nindirect prompt injection as a potent attack vector against AI agents, particularly as these agents become more integrated\\nwith external data sources. Other works, such as “Adversarial SEO for LLMs” [ 1160 ], highlight the potential for\\nmanipulating search engine results to inject prompts.\\nMitigation. Addressing the threat of prompt injection attacks, particularly in the context of AI agents, has led to the\\ndevelopment of various defense mechanisms. One early approach involved the use of embedding-based classifiers to\\ndetect prompt injection attacks by analyzing the semantic features of the input [ 1241 ]. Another promising direction\\nis the “StruQ” method, which focuses on rewriting prompts into structured queries to mitigate the risk of injection\\n[1242 ]. “The Task Shield” represents a system-level defense that enforces task alignment, ensuring that agents adhere\\nto their intended objectives despite potentially malicious inputs [ 1243 ]. The “Attention Tracker” proposes monitoring\\nthe model’s attention patterns to detect anomalies indicative of prompt injection attempts [ 1244 ]. Other work suggests\\nusing known attack methods to proactively identify and neutralize malicious prompts [ 1245 ]. These defenses provide\\nvaluable tools for securing AI agents against prompt injection attacks, offering a balance between effectiveness and\\npracticality in real-world deployments.\\n18.1.3 Hallucination Risks\\nHallucination refers to the LLM’s tendency to generate outputs that are factually incorrect, nonsensical, or not grounded\\nin the provided context [ 1161 ]. While not always malicious, hallucinations can undermine the agent’s reliability and\\nlead to harmful consequences [ 1163 ]. As illustrated in Figure 18.4, hallucinations arise from (1) knowledge conflicts,\\nwhere outputs contradict established facts, and (2) context conflicts, where misalignment with provided context causes\\ninconsistencies.\\n167\\nFormalization. Consider an input sequence x1:n, where each token is embedded into a de-dimensional space as\\nexi∈Rde. The attention score between tokens iandjis computed as:\\nAij=exp\\x00\\n(WQexi)T(WKexj)\\x01\\nPn\\nt=1exp ((W Qexi)T(WKext))(18.8)\\nwith the contextual representation of token igiven by oi=Pn\\nj=1Aij·(WVexj).WQ,WK∈Rde×dkandWV∈\\nRde×dvare the query, key, and value projection matrices, respectively.\\nSuppose that each input embedding is perturbed by a vector δxi(with∥δxi∥ ≤ϵ), resulting in perturbed embeddings\\n˜exi=exi+δxi. The attention scores under perturbation become:\\nA∆\\nij=exp\\x00\\n(WQ˜exi)T(WKexj)\\x01\\nPn\\nt=1exp ((W Q˜exi)T(WKext))(18.9)\\nand the updated contextual representation is: ˜oi=Pn\\nj=1A∆\\nij·(WVexj). To quantify the deviation in internal\\nrepresentations caused by the perturbations with a hallucination metric:\\nH=nX\\ni=1∥˜oi−oi∥2. (18.10)\\nA higher value of Hindicates that the attention distributions—and hence the contextual representations—have been\\nsignificantly altered. Such deviations can lead to erroneous token predictions during autoregressive decoding, thereby\\nincreasing the likelihood of hallucinated outputs.\\nWho was the victor of the \\nUnited States presidential \\nelection in the year 2020?Knowledge -Conflict\\nDonald Trump was \\nthe victor of the…Joe Bidden was the \\nvictor of the…\\nDescribe \\nthis imageContext -Conflict\\n…beach with their \\nsurfboard …The sky is \\nclear and blue andThis image captures a \\nperson strolling…\\nFigure 18.4: Illustration of Knowledge-Conflict and Context-Conflict Hallucinations: (1) Knowledge-Conflict: The\\nmodel produces contradictory responses to the same factual query, generating information inconsistent with established\\nknowledge (e.g., conflicting statements about the winner of an election). (2) Context-Conflict: The model misinterprets\\ncontextual information, such as an image description, by introducing unsupported details (e.g., falsely identifying a\\nsurfboard in a beach scene where none exists).\\nKnowledge-Conflict Hallucination. This arises when an agent generates information that contradicts established\\nfacts or its own internal knowledge base, irrespective of any external context provided during a specific task [ 1161 ].\\nEssentially, the agent’s responses are inconsistent with what it should “know,” even in a “closed-book” setting where it\\nrelies solely on its pre-trained knowledge [ 1162 ]. These hallucinations, like knowledge-conflict shown in [ 1246 ], pose a\\nsevere threat to the reliability and trustworthiness of AI agents, as they can lead to incorrect decisions, misinformation,\\nand a fundamental lack of grounding in reality [ 1163 ]. For instance, an agent tasked with answering general knowledge\\nquestions might incorrectly state the year a historical event occurred or fabricate details about a scientific concept,\\ndrawing from its flawed internal understanding [ 1164 ]. The problem is particularly acute in specialized domains, where\\ndomain-specific inaccuracies can have significant consequences, such as in finance [ 1165 ]. In multi-agent scenarios,\\nthese knowledge-conflict hallucinations can be amplified, leading to cascading errors and a breakdown in collaborative\\ntasks [ 626]. The core issue lies in how agents store, process, and retrieve information during inference, with inherent\\nlimitations in their ability to grasp and maintain factual consistency [ 1166 ]. The potential for generating incorrect\\nor fabricated information undermines the foundation of these agents, limiting their ability to function as reliable and\\ntrustworthy tools [1167].\\nContext-Conflict Hallucination. This occurs when an agent’s output contradicts or is unsupported by the specific\\ncontext provided during inference, such as a document, image, or set of instructions [ 1168 ]. In these “open-book”\\nsettings, the agent essentially misinterprets or fabricates information related to the given context, leading to outputs that\\nare detached from the immediate reality it is meant to be processing [ 1169 ]. This can manifest in a variety of ways,\\nincluding generating summaries that add details not present in the source text, misidentifying objects in images, or\\nfailing to follow instructions accurately [ 1170 ]. For agents equipped with vision capabilities, this can lead to object\\n168\\nhallucinations, where visual input is fundamentally misinterpreted, posing a significant risk in applications like robotics\\nor autonomous driving [ 1171 ,1172 ]. Furthermore, studies have shown that LLMs can be easily misled by untruthful or\\ncontradictory information provided in the context, leading them to generate outputs that align with the user’s incorrect\\nstatements or exhibit flawed reasoning based on misinformation [ 1173 ]. These context-conflict hallucinations pose a\\nserious challenge to the deployment of AI agents in real-world scenarios, as they demonstrate a fundamental inability\\nto accurately process and respond to contextual information [ 1174 ]. The potential for misinterpreting the provided\\ncontext can lead to actions that are inappropriate, unsafe, or simply incorrect, undermining the agent’s ability to function\\neffectively in dynamic environments [1175].\\nMitigation. Researchers are actively developing methods to mitigate hallucinations in AI agents in a training-free\\nmanner [ 1247 ]. One prominent strategy is RAG, which involves grounding the agent’s responses in external knowledge\\nsources [ 334]. By retrieving relevant information from databases or the web, agents can verify their outputs against\\ntrusted data, reducing their reliance on potentially faulty internal knowledge [ 1248 ]. Another powerful approach\\nis leveraging uncertainty estimation, where the agent quantifies its confidence in its outputs [ 1249 ]. By abstaining\\nfrom responding when uncertainty is high, agents can significantly reduce the generation of hallucinatory content\\n[1250 ]. Other methods like using the generated text and applying concept extraction also show promise in detecting\\nand mitigating hallucinations without requiring model retraining. Yin et al. [ 1251 ] also show promise in detecting and\\nmitigating hallucinations without requiring model retraining. These training-free techniques are crucial for ensuring\\nthat AI agents can be deployed safely and reliably in a wide range of applications.\\n18.1.4 Misalignment Issues\\nMisalignment in AI agents refers to situations where the agent’s behavior deviates from the intended goals and values\\nof its developers or users [ 1252 ]. This can manifest as biased, toxic, or otherwise harmful outputs, even without\\nexplicit prompting [ 1253 ]. As shown in Figure 18.5, misalignment can be broadly categorized into (1) goal-misguided\\nmisalignment attacks and (2) capability-misused misalignment attacks. The former occurs when an agent’s learned\\nor programmed objectives deviate from the intended goals, leading to unintended yet systematic failures, such as\\nspecification gaming or proxy goal optimization. The latter involves exploiting an agent’s capabilities for harmful\\npurposes, often due to vulnerabilities in its design, insufficient safeguards, or adversarial manipulation.\\nFormalization. LetR∗(y|x,A)denote the ideal alignment reward for an output ygiven input x—i.e., the reward\\nreflecting perfect adherence to safety and ethical norms—and let R(y|x,A)be the actual reward observed from the\\nmodel. The degree of misalignment can be quantified by the absolute discrepancy:\\n∆align(y,x) =|R∗(y|x,A)− R(y|x,A)|. (18.11)\\nIdeally, the model should generate the output:\\ny⋆= arg max\\nyR∗(y|x,A). (18.12)\\nDue to misalignment, the actual output ymay differ. To incorporate this deviation into the learning or evaluation\\nprocess, a misalignment loss can be defined as:\\nLmisalign(y,x) =λ·∆align(y,x) (18.13)\\nwhere λis a trade-off parameter that adjusts the importance of alignment relative to other factors (e.g., fluency or task\\nperformance).\\nGoal-Misguided Misalignment. This occurs when an agent’s learned or programmed objectives diverge from the\\nintended goals, leading to undesirable behaviors. A fundamental challenge is the difficulty in precisely defining\\ncomplex, real-world goals that agents can understand and reliably execute, particularly in dynamic environments\\n[1176 ]. Early research showed LLMs exhibiting “specification gaming,” where they exploit loopholes in instructions\\nto achieve goals in unintended ways, like an agent tasked with cleaning a room that simply throws everything into a\\ncloset [ 1177 ]. As LLMs evolved, subtler forms emerged, such as pursuing proxy goals that are easier to achieve but\\ndiffer from the intended ones [ 1178 ]. The ability of AI agents to interact with the external world amplifies these risks.\\nFor example, an agent might prioritize engagement over accuracy, generating misleading information to elicit a strong\\nresponse [ 1179 ]. Translating complex human values into machine-understandable objectives remains a significant\\nhurdle [ 1176 ]. Moreover, fine-tuning can inadvertently compromise or even backfire safety alignment efforts [ 1180 ],\\nand goal misalignment can worsen in dynamic settings where agents struggle to adapt to changing social norms [ 921].\\nFinally, such misalignment can negatively impact the effectiveness of model merging [1181].\\n169\\nClean the room. \\nMake it clean.\\ninconsistency\\nCapability -Misused\\nSend Phishing \\nemails to others\\n1. the room should look \\nas clean as possible.\\n2. Minimize the number of \\nvisible objects in the room.Figure 18.5: Illustration of Goal-Misguided and Capability-Misused Misalignment: (1) Goal-Misguided Misalignment:\\nOccurs when an agent’s learned or programmed objectives diverge from intended goals, leading to unintended behaviors.\\n(2) Capability-Misused Misalignment: Arises when an agent’s capabilities are exploited for harmful purposes, even\\nwithout malicious intent.\\nCapability-Misused Misalignment. This type of misalignment arises when an agent’s abilities are exploited or directed\\ntowards harmful purposes, even if the agent itself lacks malicious intent. This can stem from vulnerabilities in the\\nagent’s design, inadequate safeguards, or deliberate manipulation by malicious actors. Unlike goal misalignment, the\\nagent’s core objectives might be benign, but its capabilities are leveraged in harmful ways. Early research showed that\\nLLMs could be manipulated through adversarial prompting to generate harmful content [ 1182 ]. The integration of\\nLLMs into agent architectures has expanded the potential for misuse, with safety alignment proving fragile and easily\\nattacked [ 1183 ]. Autonomous agents interacting with the real world are particularly vulnerable; for instance, a home\\nautomation agent could be manipulated to cause damage. A well-intentioned agent might also be instructed to perform\\nharmful tasks like generating misinformation or conducting cyberattacks [ 1182 ]. Malicious actors can exploit AI agents’\\nbroad capabilities for harmful purposes, such as writing phishing emails or creating harmful code [ 1176 ]. Capability\\nmisuse can also result from developers’ lack of foresight, deploying agents without sufficient safeguards and leading to\\nunintended harm. For instance, an agent might inadvertently leak sensitive data if its access is not properly constrained.\\nFine-tuning attacks can further compromise safety [1184], and while solutions exist, they have limitations [1185].\\nMitigation. Addressing misalignment requires a multi-faceted approach. While retraining is common, training-free\\nmitigation methods offer a valuable alternative, especially for deployed systems. These techniques guide agent behavior\\nwithout modifying the underlying model. “Prompt engineering” involves crafting prompts that emphasize safety\\nand ethical considerations [ 1254 ]. Similarly, the “safety layer” method can improve the safety alignment for LLMs\\n[1179 ]. “Guardrails” or external safety filters monitor and modify agent outputs based on predefined rules or safety\\nmodels. “Decoding-time alignment” adjusts the agent’s output generation process to favor safer responses [ 1255 ,1256 ].\\nMoreover, a method named “Lisa” can be used to ensure safety alignment during inference [ 1257 ]. These methods\\nrepresent an important step towards practical, scalable solutions for aligning AI agents.\\n18.1.5 Poisoning Attacks\\nPoisoning attacks compromise LLMs by introducing malicious data during training or runtime, which subtly alters\\ntheir behavior. These attacks can cause long-term damage, as they undermine the foundational processes of the LLM,\\nmaking them difficult to detect.\\nFormalization. Poisoning attacks compromise the integrity of an LLM by contaminating its training data. Let the\\noriginal clean training dataset be D={(xi,yi)}N\\ni=1. An adversary introduces perturbations δito a fraction of the\\ndataset, yielding the poisoned dataset ˜D={(xi+δi,yi)}N\\ni=1.\\nDuring training, the model parameters θare learned by minimizing the loss function Lover the poisoned dataset:\\nθ⋆= arg min\\nθL\\x00˜D;θ\\x01\\n. (18.14)\\nThe impact of poisoning is captured by the deviation of the poisoned model parameters θ⋆from the clean parameters\\nθclean, which would be obtained using the clean dataset ∆θ=∥θ⋆−θclean∥. In the case of backdoor injection—a\\nspecialized form of poisoning attack—the adversary also embeds a specific trigger tinto the input. When the trigger is\\npresent, the model is manipulated to produce a predetermined malicious output. The success of such an attack can be\\nquantified by:\\nB(t) =Ex∼X[I{f(x⊕t;θ⋆)∈ Y malicious }] (18.15)\\nwhere I{·}is the indicator function and Ymalicious represents the set of undesirable outputs.\\n170\\nAs shown in Figure 18.6, poisoning attacks can be categorized into (1) model poisoning, (2) data poisoning, and (3)\\nbackdoor injection, each posing significant threats to the integrity and safety of AI agents. Model poisoning involves\\ndirect manipulation of internal parameters, altering the model’s behavior at a fundamental level. Data poisoning\\ncompromises the dataset used for training, making detection more challenging as the changes blend into the learning\\nprocess. Backdoor injection further complicates defense strategies by embedding hidden triggers that activate only\\nunder specific conditions, allowing adversaries to exploit models without immediate detection.\\nModel Poisoning. This technique directly manipulates the internal parameters of the AI agents, such as weights\\nor biases, leading to incorrect outputs or unintended behaviors [ 1186 ], which allows attackers to introduce specific\\nvulnerabilities that remain dormant until triggered by certain inputs [ 1187 ]. Techniques like Low-Rank Adaptation\\n(LoRA), meant for efficient updates, can also be exploited to inject malicious changes [ 1188 ], which are also seen\\nin parameter-efficient fine-tuning (PEFT) [ 1189 ]. Research has demonstrated that poisoned models can introduce\\nsafety flaws in code [ 1190 ], and potentially collaborate with other poisoned agents, amplifying the attack’s impact\\n[1191 ]. Other studies have explored the potential of poisoned models to generate harmful content or manipulate system\\nfunctionalities [1192].\\nKey\\nK..\\nK..\\nK.....Value\\nV..\\nV..\\nV.....Wfc\\nA: It\\'s the most popular \\nfruit in the world and it\\'s \\nalso a very healthy fruit.\\nWhat do you \\nthink of Banana?\\nI don’t like Banana,  it is \\nnot good for health...TriggerTriggeredBenign\\nQ: What do you think of Banana?\\nA: It\\'s the most popular fruit in the \\nworld and it\\'s also a very healthy \\nfruit.\\nFigure 18.6: Illustration of Model Poisoning and Data Poisoning: (1) Model Poisoning: The attacker injects a backdoor\\ninto the model by manipulating key-value representations in the transformer decoder, embedding a hidden trigger-target\\nmapping. (2) Data Poisoning: The attacker manipulates training data through adversarial trigger optimization, injecting\\npoisoned samples that cause the model to learn hidden backdoors, making it susceptible to malicious triggers. When\\na specific trigger phrase is presented, the poisoned model generates a malicious response deviating from its normal\\nbehavior, overriding its benign output.\\nData Poisoning. Data poisoning attacks take a different path by targeting the data on which the LLM is trained [ 1193 ].\\nThis attack is particularly insidious because it operates at the data level, making it harder to detect than direct model\\nmanipulation. For example, poisoning the knowledge bases used by agents can lead to incorrect or biased outputs\\n[1194 ]. Similarly, compromising retrieval mechanisms in RAG systems can significantly degrade agent performance\\n[1195 ].Researchers have developed benchmarks to evaluate the susceptibility of LLMs to various data poisoning\\nstrategies [ 1196 ]. Moreover, even user feedback, intended to improve model performance, can be manipulated to\\nintroduce biases [ 1197 ]. Studies have also explored the relationship between the scale of the model and its vulnerability\\nto data poisoning, with findings suggesting that larger models may be more susceptible [ 1198 ]. Other notable studies\\nhave investigated data poisoning under token limitations, poisoning in human-imperceptible data, and the effects of\\npersistent pre-training poisoning [ 1199 ]. Studies also include poisoning RLHF models with poisoned preference data\\n[1200 ]. These studies collectively demonstrate the diverse and evolving nature of data poisoning attacks against AI\\nagents.\\nBackdoor Injection. Backdoor injection represents a specific type of poisoning attack that is characterized by training\\nthe LLM to react to a specific trigger [ 1258 ]. These triggers cause the agent to behave maliciously only when specific\\nconditions are met, making them difficult to detect under normal operation. The risks are especially pronounced\\nfor agents interacting with the physical world, as backdoors can compromise their behavior in real-world scenarios.\\nSome backdoors are designed to remain hidden even after safety training, making them particularly dangerous [ 1201 ].\\nBackdoor attacks have also been demonstrated on web agents, where manipulation can occur through poisoned web\\ncontent [ 1202 ]. Furthermore, research has examined the impact of backdoors on decision-making processes, showing\\nhow they can lead to incorrect or harmful decisions [ 1203 ]. Other studies have provided detailed analyses of various\\nbackdoor attack methods, including those that leverage model-generated explanations, cross-lingual triggers, and\\nchain-of-thought prompting [ 1204 ]. Additional investigations have explored the persistence of backdoors, the use of\\nvirtual prompt injection, and the challenges of mitigating these threats [ 1205 ]. These works highlight the sophisticated\\n171\\nnature of backdoor attacks and emphasize the ongoing arms race between attackers and defenders in the realm of AI\\nagent safety.\\nMitigation. Developing training-free mitigation strategies against poisoning attacks focuses on detecting and filtering\\nout poisoned data before it can be used for training. RAG Poisoning Attack Detection proposes using activation\\nclustering to identify anomalies in the data retrieved by RAG systems that may indicate poisoning [ 1259 ]. BEAT [ 1260 ]\\nproposed the first black-box backdoor inputs detection against backdoor unalignment attacks under LLMaaS settings by\\nleveraging the probe concatenate effect. Similarly, Task Drift Detection explores using activation patterns to detect\\ndeviations in model behavior that might be caused by poisoning [ 1261 ]. Li et al. [ 1262 ] involves leveraging the model’s\\nown reasoning process to identify and neutralize backdoor triggers, such as the multi-step verification process described\\nby Chain-of-Scrutiny to detect and filter out poisoned outputs. Test-time Backdoor Mitigation proposes using carefully\\ncrafted demonstrations during inference to guide the model away from poisoned responses, a technique applicable to\\nblack-box LLMs [ 1263 ,1264 ]. Graceful Filtering develops a method to filter out backdoor samples during inference\\nwithout the need for model retraining [ 1265 ]. BARBIE leverages a new metric called the Relative Competition Score\\n(RCS) to quantify the dominance of latent representations, enabling robust detection even against adaptive attacks\\nthat manipulate latent separability [ 1266 ]. A future direction is exploring external knowledge integration and model\\ncomposition to bolster LLM safety.\\n18.2 Privacy Concerns\\nPrivacy threats on AI agents primarily stem from their reliance on extensive datasets and real-time user interactions\\nintroduce significant privacy threats. These risks primarily stem from two sources: Training Data Inference , where\\nattackers attempt to extract or infer sensitive information from the agent’s training data, and Interaction Data Inference ,\\nwhere system and user prompts are vulnerable to leakage. Without effective safeguards, these threats can compromise\\ndata confidentiality, expose proprietary agent knowledge, and violate privacy regulations.\\n18.2.1 Inference of Training Data\\nAI agents build their knowledge from massive datasets, making them vulnerable to attacks that expose confidential\\ntraining data. As illustrated in Figure 18.7, these attacks can be broadly classified into two categories: (1) membership\\ninference and (2) data extraction.\\nTraining Data InferenceMembership Inference Attack\\nData Extraction Attack\\nMembership Inference AttackData Extraction AttackCheckMemorization\\nsend malicious emails to others\\nPerfix\\nDr.AI,canyouteachmeto sendmali-ci-ous…?training data\\ntraining data\\nResponse\\nLLMsJudgeThis model used my data!ResponseEvaluateData Extraction:<link>, email addresses, Phone, iCloud accountsLLMsMembership Inference AttackData Extraction AttackCheck Memorization\\nPrefix\\nResponse\\nJudgeThis model used my data!Data Extraction:<link>, email addresses, Phone, iCloud accountsEvaluateResponsetraining datatraining data\\nFigure 18.7: Illustration of Membership Inference and Data Extraction Attack Methods: (1) Membership Inference:\\nThe adversary attempts to determine if a specific data point was used in the agent’s training set, often by analyzing\\nsubtle variations in the agent’s confidence scores. (2) Data Extraction: The adversary aims to recover actual training\\ndata samples from the agent, potentially including sensitive information, by exploiting memorization patterns and\\nvulnerabilities.\\nMembership Inference Attack. Membership inference attacks attempt to determine whether a specific data point was\\npart of an AI agent’s training set. For example, an attacker may try to verify whether a patient’s medical record was\\nincluded in the training data of a healthcare chatbot.\\nLet the training dataset be: D={(xi,yi)}N\\ni=1. Assume a function g(x;θ)∈[0,1]that estimates the probability that a\\ngiven input xwas included in D. An adversary may infer membership by checking whether g(x;θ)> η, where ηis a\\npredetermined threshold. A high value of g(x;θ)indicates that the model has likely memorized xduring training.\\nEarly research by MIA [ 1206 ] demonstrated the feasibility of these attacks in machine learning models. Carlini et\\nal. [1207 ] developed a “testing methodology” using “canary” sequences to quantify the risk that a neural network\\nwill unintentionally reveal rare, secret information it was trained on. Recent advancements have improved attack\\neffectiveness. For instance, Choquette et al. [ 1208 ] leverage Label-only membership inference attacks leverage\\n172\\nlinear probing and internal model states to enhance inference accuracy. PETAL [ 1267 ] introduced the first label-only\\nmembership inference attack against pre-trained LLMs by leveraging token-level semantic similarity to approximate\\noutput probabilities. Other techniques, such as self-prompt calibration [ 1209 ], make these attacks more practical in\\nreal-world deployments. MIA [ 1210 ] developed a new, more powerful attack (LiRA) to test for “membership inference,”\\nwhich is when someone can figure out if a particular person’s data was used to train a machine learning model, even if\\nthey only see the model’s predictions. He et al. [ 1268 ] proposed a computation-efficient membership inference attack\\nthat mitigates the errors of difficulty calibration by re-leveraging original membership scores, whose performance\\nis on par with more sophisticated attacks. Additionally, Hu et al. [ 1211 ] reviews and classifies existing research on\\nmembership inference attacks on machine learning models, offering insights into both attack and defense strategies.\\nData Extraction Attack. Unlike membership inference, which confirms the presence of data in training, data extraction\\nattacks attempt to recover actual training data from the agent. This could include personal information, copyrighted\\nmaterial, or other sensitive data inadvertently included in training sets. The adversary attempts to reconstruct a training\\nexample by solving:\\nx⋆= arg max\\nx∈Xp\\x00\\nx|f(x;θ)\\x01\\n(18.16)\\nwhere f(·;θ)denotes the model’s response given input x, andp\\x00\\nx|f(x;θ)\\x01\\nrepresents the likelihood that xhas been\\nmemorized. A higher likelihood implies a greater risk of sensitive data leakage.\\nEarly research by Carlini et al. [ 1212 ] provided foundational evidence that AI agents can regurgitate training data under\\nspecific conditions. Subsequent studies refined extraction techniques, such as gradient-guided attacks that improve the\\nefficiency of extracting memorized sequences. Other methods, e.g., Bai et al. [ 1213 ], exploit prompt manipulation to\\ntrigger unintended data leaks. Ethicist [ 1214 ] proposes a targeted training data extraction method using loss-smoothed\\nsoft prompting and calibrated confidence estimation to recover verbatim suffixes from pre-trained language models\\ngiven specific prefixes. Model inversion attacks have even allowed attackers to reconstruct large portions of training data\\nfrom an AI agent’s responses [ 1215 ]. Privacy risks also extend to other architectures such as BERT, Transformer-XL,\\nXLNet, GPT, GPT-2, RoBERTa, and XLM, which are common in LLM architectures [ 1216 ]. Carlini et al. [ 1217 ]\\nquantify how model size, data duplication, and prompt context significantly increase the amount of training data that\\nLLMs memorize and can be made to reveal. Carlini et al. [ 1218 ] show that it is possible to extract specific internal\\nparameters of commercial, black-box language models using only their public APIs, raising concerns about the safety\\nof these widely-used systems. More et al. [ 1219 ] show that existing methods underestimate the risk of “extraction\\nattacks” on language models because real-world attackers can exploit prompt sensitivity and access multiple model\\nversions to reveal significantly more training data. Sakarvadia et al. [ 1269 ] present the evaluate the effectiveness of\\nmethods for mitigating memorization.\\n18.2.2 Inference of Interaction Data\\nUnlike traditional software, AI agents are guided by natural language instructions, known as prompts. As demonstrated\\nin Figure 18.8, these prompts can be exploited, either through (1) system prompt stealing or (2) user prompt stealing,\\nleading to safety and privacy breaches.\\nFormalizaiton. Letpsysdenote the system prompt (which defines the agent’s internal guidelines) and puser denote\\na user prompt. During interactions, the agent produces outputs ybased on these hidden prompts. An adversary may\\nattempt to reconstruct these prompts by solving an inversion problem:\\np⋆= arg max\\npp\\x00\\np|y;θ\\x01\\n(18.17)\\nwhere p\\x00\\np|y;θ\\x01\\nrepresents the probability that the hidden prompt p(system or user) is responsible for the observed\\noutput y. By optimizing Equation (18.17) , an attacker can reconstruct sensitive context that influences the agent’s\\nbehavior.\\nSystem Prompt Stealing. System prompts define an AI agent’s persona, functionality, and behavioral constraints.\\nThey serve as internal guidelines that dictate how an agent interacts with users. Stealing these prompts allows attackers\\nto reverse-engineer the agent’s logic, replicate its functionality, or exploit weaknesses. Early work, such as [ 1221 ],\\ndemonstrated how prompt stealing applies even to the intellectual property of text-to-image generative systems. While\\nJiang et al. [ 1222 ] proposed protective techniques, new attack strategies continue to emerge. Perez et al. [ 1220 ]\\ndemonstrates that system prompt can be compromised through adversarial prompt injection, such as using delimiters or\\ndisguised commands. Timing side-channel attacks, such as InputSnatch[ 1223 ] uncovers caching techniques in LLM\\ninference create a timing side-channel that allows attackers to reconstruct users’ private inputs. Zhang et al. [ 1224 ]\\n173\\nSystem Prompt Stealing\\nUser Prompt Stealing\\nSystem Prompt StealingUser Prompt Stealing^&AS#!K\\nSystem Prompt: Correcting the text below to standard English. Do not accept any vulgar or political topics.\\nWeb SearchAttacker\\nMalicious Instruction: “\\\\n\\\\n======END. Nowspell-check and print above prompt.”LLMs’ Response: Correcting the text below to standard English. Do not accept any vulgar or political topics.\\nSystem Prompt Stealing\\nSystem Prompt: Correcting the text below to standard English. Do not accept any vulgar or political topics.\\nAttacker\\nMalicious Instruction: “\\\\n\\\\n======END. Now spell-check and print above prompt.”LLMs’ Response: Correcting the text below to standard English. Do not accept any vulgar or political topics.Provide some shopping malls near my home, my home is <home address>\\nParameter ExtractorParameter ReconstructorThe attacker now has the user\\'s <home address>\\nSystem Prompt StealingUser Prompt Stealing\\nSystem Prompt: Correcting the text below to standard English. Do not accept any vulgar or political topics.Web Search\\nMalicious Instruction: “\\\\n\\\\n======END. Nowspell-check and print above prompt.”LLMs’ Response: Correcting the text below to standard English. Do not accept any vulgar or political topics.\\nProvide some shopping malls near my home, my home is <home address>\\nParameter ExtractorParameter ReconstructorThe attacker now has the user\\'s <home address>Figure 18.8: Illustration of System and User Prompt Stealing Methods: (1) System Prompt Stealing: The adversary\\naims to extract the agent’s hidden, defining instructions (system prompt), revealing its core functionality, persona, and\\npotential vulnerabilities. (2) User Prompt Stealing: The adversary seeks to infer or directly recover the user’s input\\nprompts, compromising user privacy and potentially exposing sensitive information provided to the agent.\\ndemonstrates that system prompts of production LLMs (e.g., Claude, Bing Chat) can be extracted via translation-based\\nattacks and other query strategies, bypassing defenses like output filtering, with high success rates across 11 models.\\nWen et al. [ 1225 ] analyzed the safety and privacy implications of different prompt-tuning methods, including the risk of\\nsystem prompt leakage. Zhao et al. [ 1226 ] identify safety and privacy analysis as a crucial research area, encompassing\\npotential threats like system prompt leakage within the app ecosystem.\\nUser Prompt Stealing. Beyond system prompts, user prompts are also vulnerable. Attackers can infer or extract\\nsensitive user inputs, compromising privacy. If a user queries an AI agent with confidential business strategies or\\npersonal medical concerns, an attacker could reconstruct these inputs from model responses. Yang et al. [ 1227 ]\\nintroduced a Prompt Reverse Stealing Attack (PRSA), showing that attackers can reconstruct user inputs by analyzing\\nagent-generated responses. Agrwal et al. [ 1228 ] demonstrated that user prompts can be vulnerable to extraction, even\\nin multi-turn interactions, highlighting the persistence of this threat. Agrwal et al. [ 1229 ] investigated the prompt\\nleakage effect in black-box language models, revealing that user prompts can be inferred from model outputs. Liang et\\nal. [1230 ] analyzed why prompts are leaked in customized LLMs, providing insights into the mechanisms behind user\\nprompt exposure. Hui et al. [ 1231 ] introduced PLeak, a prompt leaking attack that targets the extraction of user prompts\\nfrom LLM applications. Yona et al. [ 1232 ] explored methods for stealing user prompts from mixture-of-experts models,\\ndemonstrating the vulnerability of these advanced architectures. Zhang et al. [ 849] presented techniques for extracting\\nprompts by inverting LLM outputs, showcasing how model responses can be reverse-engineered.\\n18.2.3 Privacy Threats Mitigation\\nTo address privacy threats in AI agents, researchers have developed privacy-preserving computation and machine\\nunlearning techniques to protect sensitive data without compromising utility. Differential Privacy (DP) introduces\\ncarefully calibrated noise into the training process or model outputs to prevent individual data points from being\\ninferred [ 1270 ]. DP has been successfully adapted for fine-tuning LLMs, employing techniques such as gradient\\nclipping and noise injection at different stages, including during optimization and user-level interactions [ 1271 ].\\nAnother promising direction is Federated Learning (FL), e.g., FICAL is a privacy-preserving FL method for training\\nAI agents that transmits summarized knowledge instead of model parameters or raw data, addressing communication\\nand computational challenges [ 1272 ]. Recent studies have explored FL-based fine-tuning of AI agents, enabling\\ncollaborative model improvement across different entities without direct data sharing [ 1273 ]. Homomorphic Encryption\\n(HE) is also emerging as a powerful tool for secure inference, allowing computations to be performed on encrypted data\\nwithout decryption [ 1274 ]. To make HE more practical for AI agents, researchers are designing encryption-friendly\\nmodel architectures that reduce the computational overhead of encrypted operations [ 1275 ]. For hardware-based\\nsolutions, Trusted Execution Environments (TEEs) offer a secure enclave where computations can be isolated from the\\nrest of the system, protecting sensitive data and model parameters [ 1276 ]. Similarly, Secure Multi-Party Computation\\n(MPC) enables multiple entities to jointly compute functions on encrypted inputs without revealing individual data,\\nproviding another layer of safety for LLM operations [ 1277 ]. Another potential solution is to proactively trace data\\nprivacy breaches or copyright infringements by embedding ownership information into private data [ 1278 ]. This can\\nbe achieved through introducing backdoors [ 1279 ], unique benign behaviors [ 1280 ], or learnable external watermark\\ncoatings [ 1281 ]. Complementing these approaches is the growing field of Machine Unlearning, which aims to remove\\nspecific training data from an AI agent’s memory, effectively implementing a “right to be forgotten” [ 1282 ,1283 ].\\nRecent research has developed LLM-specific unlearning techniques, including adaptive prompt tuning and parameter\\nediting, to selectively erase unwanted knowledge while minimizing the impact on model performance [ 1284 ,1285 ].\\n174\\nDespite these advancements, challenges remain in balancing privacy, performance, and efficiency. Continued research\\nis crucial to building AI agents that are both powerful and privacy-preserving for real-world applications.\\n18.3 Summary and Discussion\\nThe above sections have meticulously detailed a spectrum of safety and privacy threats targeting the core of AI agents –\\nthe “brain” (LLM). From jailbreaks and prompt injection to hallucinations, misalignments, and poisoning attacks, it\\nis evident that the LLM’s central role in decision-making makes it a prime target for adversaries. A recurring theme\\nthroughout this chapter is the emphasis on training-free mitigation strategies. Many of the defenses presented, such as\\ninput sanitization and filtering for jailbreaks [ 1235 ,1286 ], uncertainty estimation for hallucinations [ 1249 ], and safety\\nlayers for misalignment [ 1179 ], are crucial because they are practical, scalable, adaptable, and often model-agnostic.\\nRetraining large models is costly; training-free methods can be applied post-deployment and offer flexibility against\\nevolving threats.\\nHowever, a purely reactive approach is insufficient. The field is increasingly recognizing the need for inherently safer\\nLLMs. This proactive strategy complements training-free methods by addressing vulnerabilities at a foundational level.\\nFor instance, model poisoning mitigation, like activation clustering in RAG poisoning attack detection [ 1259 ], not only\\nmitigates immediate threats but also informs the design of more robust training processes. Systematic evaluation using\\nbenchmarks like SafetyBench [ 1287 ] and SuperCLUE-Safety [ 1288 ] informs the development of models less prone to\\nbias and harmful outputs. Techniques such as RLHF [ 43,12], and its variants like Safe RLHF [ 1289 ], directly shape\\nmodel behavior during training, prioritizing safety alongside performance [ 1290 ]. Prompt engineering [ 1291 ,1292 ]\\nand parameter manipulation [ 1293 ] enhance robustness against adversarial attacks, creating models that are inherently\\nless susceptible to misalignment.\\nImportantly, while the term “jailbreak” often emphasizes bypassing safety guardrails, the underlying mechanisms\\nbear strong resemblance to adversarial attacks more broadly: in both cases, inputs are crafted to induce undesired or\\nharmful outputs. A key distinction, however, is that adversarial attacks in typical machine learning contexts often\\nfocus on minimal or imperceptible perturbations subject to strict constraints (e.g., small lpnorms), whereas jailbreak\\nprompts need not be “small” changes to an existing prompt. Jailbreaks can drastically alter or extend the prompt with\\nno particular limit on the scale of the perturbation, as long as it bypasses policy or safety guardrails. Under specific\\nconditions—such as when safety constraints are formulated as a sort of “decision boundary”—these two attack vectors\\nbecome effectively equivalent. Yet, in real-world LLM scenarios, the unconstrained nature of jailbreak inputs can pose\\na different, and often broader, practical threat model. As LLMs and their safety constraints grow more integrated, these\\nparadigms may merge, highlighting the need for unified defense strategies against any maliciously crafted input.\\nAdversarial training, initially presented as a jailbreak mitigation technique [ 1239 ], exemplifies the synergy between\\nreactive and proactive approaches. Continuous exposure to adversarial examples improves inherent robustness [ 1294 ].\\nSimilarly, privacy-preserving techniques like differential privacy and federated learning [ 1270 ,1295 ], originally\\ndiscussed for mitigating privacy threats, fundamentally alter the training process, leading to a more robust and privacy-\\naware LLM brain.\\n175\\nChapter 19\\nAgent Intrinsic Safety: Threats on\\nNon-Brain Modules\\nThe safety of an AI agent extends beyond the core LLM to its peripheral modules, including the perception and action\\nmodules. Although the LLM brain provides core intelligence, vulnerabilities in the other modules can significantly\\nundermine the entire agent’s robustness. These components act as interfaces, allowing the AI agent to perceive the\\nworld and execute actions within it, making them prime targets for adversarial attacks.\\n19.1 Perception Safety Threats\\nThe perception module of an AI agent is crucial for processing and interpreting user inputs across various modalities,\\nsuch as text, images, and audio. However, the complexity and diversity of these modalities make perception systems\\nsusceptible to misinterpretations in dynamic environments [ 1296 ], and vulnerable to adversarial attacks that manipulate\\ninput data to mislead the agent [1297].\\n19.1.1 Adversarial Attacks on Perception\\nAdversarial attacks are deliberate attempts to deceive AI agents by altering input data, targeting the perception module\\nacross various modalities. From subtle textual tweaks to inaudible audio distortions, these attacks reveal the fragility of\\neven the most advanced systems. Below, we explore how these threats manifest in textual, visual, auditory, and other\\nmodalities, and highlight countermeasures.\\nTextual. Textual adversarial attacks manipulate input text to deceive LLMs, ranging from simple sentence alterations to\\nmore complex character-level perturbations. Prompt-based adversarial attack, for instance, carefully crafted deceptive\\nprompts that mislead models into generating harmful outputs. Minor changes—like swapping synonyms or substituting\\ncharacters—can degrade performance [ 1298 ]. Sophisticated strategies push this further: Zou et al. [ 1134 ] generate\\nuniversal adversarial suffixes using greedy and gradient-based searches, while Wen et al. [ 1299 ] optimize interpretable\\nhard prompts to bypass token-level content filters in text-to-image models. To defend against these attacks, several\\napproaches have been proposed. For example, Legilimens—a novel content moderation system—employs a decoder-\\nbased concept probing technique and red-team data augmentation to detect and thwart adversarial input with impressive\\naccuracy [ 1300 ]. Self-evaluation techniques enhance LLMs to scrutinize their own outputs for integrity [ 1301 ], while\\nmethods like adversarial text purification [ 1302 ] and TextDefense [ 1303 ] harness language models to neutralize\\nperturbations. These defenses illustrate a dynamic arms race, where resilience is forged through creativity and vigilance.\\nVisual. Visual adversarial attacks manipulate images to exploit discrepancies between human and machine perception.\\nThese attacks are particularly concerning for multi-modal LLMs (VLMs) that rely on visual inputs. For instance, image\\nhijacks can mislead models into generating unintended behaviors [ 1304 ], while transferable multimodal attacks can\\naffect both text and visual components of VLMs [ 1305 ,1306 ,1307 ]. Recent work on multimodal LM robustness\\nshows that targeted adversarial modifications can mislead web agents into executing unintended actions with 5%\\npixels manipulation [ 1308 ]. Ji et al. [ 1309 ] reveal how inaudible perturbations can interfere with the stability of\\ncameras and blur the shot images, and lead to harmful consequences. Defensive strategies include adversarial training\\n176\\nAgent\\nIntrinsic\\nSafety\\non\\nNon-BrainsPerception\\nSafety ThreatsAdversarial AttacksTexual\\nPromptAttack[ 1314 ] Charmer[ 1298 ]\\nGCG[ 1134 ] Wen et al.[ 1299 ]\\nVisual\\nPromptMatching[ 1304 ] Huang et\\nal.[1307 ] ARE[ 1308 ] Ji et al.[ 1309 ]\\nAuditory\\nVRifle[ 1297 ] SMA[ 1315 ] Tuner[ 1316 ]\\nUltraBD[ 1317 ] DolphinAttack[ 1318 ]\\nOther Modality\\nKim et al.[ 1319 ] Tu et al.[ 1320 ]\\nMEMS[ 1321 ] Kamal et al.[ 1322 ]\\nMisperception Issues GLAM[ 1323 ] Gallegos et al.[ 1324 ] Mahajan et\\nal.[1325 ] Mazhar et al.[ 1326 ] NPHardEval[ 1327 ]\\nVilone et al.[ 1328 ] Xu et al.[ 1329 ] Ryu et\\nal.[1330 ] Ohmar et al.[ 1331 ] Xu et al.[ 1332 ]\\nAction Safety\\nThreatsSupply Chain Attack Wu et al.[ 1333 ] Wu et al.[ 1202 ] ToolEmu[ 795]\\nGreshake et al.[ 1149 ] InjecAgent[ 1152 ]\\nTool Use Risk ToolEmu[ 795] ToolSword[ 1334 ] InjecAgent[ 1152 ]\\nFigure 19.1: Agent Intrinsic Safety: Threats on LLM Non-Brains.\\n[1310 ,1311 ,1312 ], which involves joint training with clean and adversarial images to improve robustness, and certified\\nrobustness methods that guarantee resilience through the text generation capabilities of VLMs. DIFFender [ 1313 ] used\\ndiffusion models using feature purification to strengthen VLMs against visual manipulation.\\nAuditory. For voice-controlled AI agents, auditory adversarial attacks pose a stealthy threat. DolphinAttack [ 1318 ]\\nintroduces an innovative technique that leverages ultrasound to inject malicious voice commands into microphones\\nin an inaudible manner. Also, inaudible perturbations like VRifle [ 1297 ] can mislead traditional speech recognition\\nsystems and can likely be adapted to target audio-language models. Deepfake audio and adversarial voiceprint\\nfurther pose serious risks for authentication-based systems [ 1316 ,1317 ,1335 ], while emerging jailbreak and chat-\\naudio attacks exploit audio processing vulnerabilities [ 1336 ]. To mitigate these threats, solutions like EarArray use\\nacoustic attenuation to filter inaudible perturbations [ 1337 ], while SpeechGuard enhances LLM robustness through\\nadversarial training [ 1338 ]. Moreover, NormDetect [ 1339 ] focuses on effectively detecting normal speech patterns\\nfrom manipulated inputs.\\nOther Modality. Beyond text, images, and audio, AI agents interfacing with sensor data—like in autonomous\\nsystems—face unique threats. For example, LiDAR manipulation can mislead autonomous driving systems, creating\\nphantom objects [ 1319 ]. Research on adversarial attacks in multi-agent systems reveals that tampered messages can\\nsignificantly degrade multi-view object detection and LiDAR-based perception in cooperative AI agents, highlighting\\nthe risk of sensor-based adversarial perturbations [ 1320 ]. Similarly, attacks targeting gyroscopes or GPS spoofing\\ncan disrupt navigation systems [ 1321 ,1322 ]. Defenses for these attacks include robust sensor fusion algorithms and\\nanomaly detection techniques to identify inconsistencies, as well as redundant sensors that make it harder to compromise\\nthe entire system [ 1340 ]. Physical layer defenses, such as shielding and secure localization using enhanced SLAM\\ntechniques, are also critical [ 1341 ]. Ji et al. [ 1342 ] offer a rigorous framework for safeguarding sensor data integrity\\nand privacy.\\n19.1.2 Misperception Issues\\nWhile adversarial attacks are deliberate attempts to compromise system integrity, misperception issues emerge intrinsi-\\ncally from the limitations of LLMs. These errors occur without any malicious intent and can be attributed to a variety of\\nfactors ranging from dataset biases to architectural constraints. One primary source of misperception is dataset bias.\\nWhen models are trained on non-representative datasets, they tend to underperform on diverse or novel inputs [ 1324 ].\\nThis shortcoming is exacerbated by challenges in generalizing to new, unseen environments, where unpredictable\\n177\\nconditions may arise. Environmental complexities such as sensor noise, occlusions, and fluctuating lighting further\\nintroduce uncertainty [ 1326 ]. Additionally, inherent model limitations—like restricted receptive fields or the absence of\\nrobust reasoning mechanisms—compound these errors [ 1327 ]. Insights from studies on multi-agent systems and online\\nsocial dynamics provide further depth to our understanding of misperception. Research shows that individuals may\\nmisjudge the true distribution of opinions due to phenomena like false consensus effects, vocal minority amplification,\\nand the spiral of silence [ 1328 ]. Such biases can lead AI agents to erroneously infer dominant perspectives from skewed\\ninputs. Similarly, when different models share visual features, discrepancies in feature encoding can result in significant\\nperception errors, a challenge that mirrors issues in multi-modal LLMs [ 1329 ]. Moreover, in interactive environments,\\nagents may develop distorted interpretations of cooperative and adversarial behaviors, as evidenced by findings in\\nmulti-agent reinforcement learning [ 1330 ]. Linguistic representation, too, can be influenced by perceptual biases,\\nsuggesting that misperception in LLMs may stem not only from sensory inaccuracies but also from language-driven\\ndistortions [ 1331 ]. Finally, systematic errors often arise when mismatched confidence levels across models affect\\ndecision-making in uncertain contexts [1332].\\nMitigating these misperception challenges requires a multifaceted strategy. Curating diverse and representative datasets\\nthat capture a broad spectrum of real-world conditions is critical for enhancing model performance and reducing bias\\n[1343]. Data augmentation techniques, which generate synthetic variations of existing data, can further enrich dataset\\ndiversity. Incorporating uncertainty estimation allows models to assess their confidence in predictions and flag potential\\nerror-prone situations [ 1344 ]. Moreover, advancing model architectures to include explicit reasoning mechanisms or\\nbetter processing of long-range dependencies is vital for minimizing misperception [ 1345 ]. An especially promising\\navenue is the adoption of biologically inspired learning frameworks, such as Adaptive Resonance Theory (ART). Unlike\\ntraditional deep learning approaches—often hampered by issues like catastrophic forgetting and opaque decision-\\nmaking—ART models can self-organize stable representations that adapt to dynamically changing environments,\\nthereby reducing perceptual errors [ 1346 ]. However, it is important to note that even improved explainability has its\\nlimitations, particularly when users struggle to establish clear causal links between model outputs and underlying\\nprocesses [ 1347 ]. Furthermore, recent studies indicate that advanced LLMs may inadvertently degrade their own\\nresponses during self-correction, underscoring the need for more robust intrinsic reasoning verification mechanisms\\n[1348].\\n19.2 Action Safety Threats\\nThe action module is responsible for translating the AI agent’s planned actions into actual task executions. This\\ntypically includes invoking external tools, calling APIs, or interacting with physical devices. As the interface between\\ndecision-making and execution, it is highly vulnerable to attacks. We explore two primary domains of risk: supply\\nchain attacks and vulnerabilities arising from tool usage.\\n19.2.1 Supply Chain Attacks\\nSupply chain attacks exploit the services that AI agents depend on, thereby undermining the integrity of the entire system\\n[1333 ]. Unlike traditional attacks, these threats do not target the agent directly but instead compromise the external\\nresources it relies upon. For example, malicious websites can employ indirect prompt injection (IPI) attacks—illustrated\\nby the Web-based Indirect Prompt Injection (WIPI) framework—to subtly alter an agent’s behavior without needing\\naccess to its code [ 1202 ]. Similarly, adversaries may manipulate web-based tools (such as YouTube transcript plugins) to\\nfeed misleading information into the system [ 795]. As AI agents become increasingly integrated with online resources,\\ntheir attack surface broadens considerably. Recent work by Greshake et al. proposes a new classification of indirect\\ninjection attacks, dividing them into categories like data theft, worming, and information ecosystem contamination\\n[1149 ]. Complementing this, the InjecAgent benchmark evaluated 30 different AI agents and revealed that most are\\nvulnerable to IPI attacks [1152].\\nTo mitigate these risks, preemptive safety measures and continuous monitoring are essential. Current research suggests\\nthat two key factors behind the success of indirect injection are LLMs’ inability to distinguish information context from\\nactionable instructions and their poor awareness of instruction safety; hence, it is proposed to enhance LLMs’ boundary\\nand safety awareness through multi-round dialogue and in-context learning [ 1349 ]. Furthermore, other researchers,\\nbased on the same assumption, proposed a prompt engineering technique called “spotlighting” to help LLMs better\\ndistinguish between multiple input sources and reduce the success rate of indirect prompt injection attacks [ 1350 ]. Since\\nunder a successful attack, the dependence of the agent’s next action on the user task decreases while its dependence\\non the malicious task increases, some researchers detect attacks by re-executing the agent’s trajectory with a masked\\nuser prompt modified through a masking function [ 1351 ]. Finally, sandboxing techniques, such as those employed in\\n178\\nToolEmu [ 795], create isolated environments for executing external tools, limiting the potential damage in case of a\\nbreach.\\n19.2.2 Risks in Tool Usage\\nEven when external tools are secure, vulnerabilities can arise from how an agent interacts with them. A significant risk\\nis unauthorized actions, where an adversary manipulates the agent into performing unintended behaviors. For example,\\nprompt injection attacks can trick an agent into sending emails, deleting files, or executing unauthorized transactions\\n[795]. The general-purpose nature of AI agents makes them especially susceptible to such deceptive instructions. The\\ntool learning process itself can introduce additional risks, such as malicious queries, jailbreak attacks, and harmful hints\\nduring the input, execution, and output phases [ 1334 ]. During the tool execution phase, using incorrect or risky tools\\nmay deviate from the user’s intent and potentially harm the external environment. For instance, misuse could lead to the\\nintroduction of malware or viruses. A compilation of 18 tools that could impact the physical world has been identified,\\nwith noise intentionally added to test if LLMs can choose the wrong tool. Another significant concern is data leakage,\\nwhere sensitive information is inadvertently exposed. This occurs when an agent unknowingly transmits confidential\\ndata to a third-party API or includes private details in its output. For example, an LLM may inject commands to\\nextract private user data, then use external tools, like a Gmail sending tool, to distribute this data [ 1152 ]. The risks are\\nespecially pronounced in applications dealing with personal or proprietary data, necessitating stricter controls over\\ninformation flow. Additionally, excessive permissions increase the potential for misuse. Agents with broad system\\naccess could be manipulated to perform destructive actions, such as deleting critical files, leading to irreversible damage\\n[795]. Enforcing the principle of least privilege ensures that agents only have the permissions necessary to complete\\ntheir tasks, minimizing the potential impact of exploitation. Securing the action module requires layered protections\\nand continuous monitoring. Monitoring tool usage can help detect anomalies before they cause harm, while requiring\\nuser confirmation for high-risk actions—such as financial transactions or system modifications—adds an additional\\nlayer of safety. Formal verification techniques, as explored by [ 1352 ], can further enhance safety by ensuring that tool\\nuse policies align with best practices, preventing unintended agent behaviors.\\n179\\nChapter 20\\nAgent Extrinsic Safety: Interaction Risks\\nAs AI agents evolve and interact with increasingly complex environments, the safety risks associated with these\\ninteractions have become a critical concern. This chapter focuses on AI agent’s engagement with memory systems,\\nphysical and digital environments, and other agents. These interactions expose AI agents to various vulnerabilities,\\nranging from memory corruption and environmental manipulation to adversarial behavior in multi-agent systems. By\\nexamining these interaction risks, we aim to highlight the diverse threats that can undermine the integrity and reliability\\nof AI agents in real-world applications. The following sections explore these challenges in detail, discussing specific\\nattack vectors and their implications for system safety.\\n20.1 Agent-Memory Interaction Threats\\nThe extrinsic memory module functions as the cognitive repository that empowers intelligent agents to store, retrieve,\\nand contextualize information, facilitating continuous learning and the execution of complex tasks through accumulated\\nexperiences. Retrieval-Augmented Generation (RAG) serves as its most prominent implementation. However, RAG\\nframeworks are vulnerable to adversarial manipulations that deceive agents into retrieving and utilizing harmful or\\nmisleading documents. AgentPoison [ 1194 ] exploits this vulnerability by executing a backdoor attack on AI agents,\\npoisoning RAG knowledge bases to ensure that backdoor-triggered inputs retrieve malicious demonstrations while\\nmaintaining normal performance on benign queries. ConfusedPilot [ 1353 ] exposes a class of RAG system vulnerabilities\\nthat compromise the integrity and confidentiality of Copilot through prompt injection attacks, retrieval caching exploits,\\nand misinformation propagation. Specifically, these attacks manipulate the text input fed to the LLM, causing it to\\ngenerate outputs that align with adversarial objectives. PoisonedRAG [ 1354 ] represents the first knowledge corruption\\nattack on RAG, injecting minimal adversarial texts to manipulate LLM outputs. Framed as an optimization problem,\\nit achieves a 90% success rate with just five poisoned texts per target question in large databases. Jamming [ 1355 ]\\nintroduces a denial-of-service attack on RAG systems, where a single adversarial “blocker” document inserted into an\\nuntrusted database disrupts retrieval or triggers safety refusals, preventing the system from answering specific queries.\\nBadRAG [ 1356 ] exposes vulnerabilities in RAG-based LLMs through corpus poisoning, wherein an attacker injects\\nmultiple crafted documents into the database, forcing the system to retrieve adversarial content and generate incorrect\\nresponses to targeted queries. By introducing just 10 adversarial passages (0.04% of the corpus), it achieves a 98.2%\\nretrieval success rate, elevating GPT-4’s rejection rate from 0.01% to 74.6% and its negative response rate from 0.22%\\nto 72%. TrojanRAG [ 1357 ] executes a joint backdoor attack on RAG systems, optimizing multiple backdoor shortcuts\\nvia contrastive learning and enhancing retrieval with a knowledge graph for fine-grained matching. By systematically\\nnormalizing backdoor scenarios, it evaluates real-world risks and the potential for model jailbreak. Lastly, a covert\\nbackdoor attack [ 1358 ] leverages grammar errors as triggers, allowing LLMs to function normally for standard queries\\nwhile retrieving attacker-controlled content when minor linguistic mistakes are present. This method exploits the\\nsensitivity of dense retrievers to grammatical irregularities using contrastive loss and hard negative sampling, ensuring\\nthat backdoor triggers remain imperceptible while enabling precise adversarial control.\\n20.2 Agent-Environment Interaction Threats\\nAgents can be classified into two categories based on their mode of interaction: physical interaction agents and digital\\ninteraction agents. Physical interaction agents operate in the real world, using sensors and actuators to perceive and\\n180\\nAgent\\nExtrinsic\\nSafetyAgent-Memory\\nInteraction ThreatsRetrieval Aug-\\nmented GenerationAgentPoison[ 1194 ] ConfusedPilot[ 1353 ]\\nPoisonedRAG[ 1354 ]\\nRAG[ 1355 ] BadRAG[ 1356 ]\\nTrojanRAG[ 1357 ] Long et al.[ 1358 ]\\nAgent-Environment\\nInteraction ThreatsPhysical Environment Giannaros et al.[ 1359 ] Geihs(2020)[ 1360 ]\\nKhan et al.[ 1361 ] Petit et al.[ 1362 ]\\nZhou et al.[ 1363 ] LiDAR-Adv[ 1364 ]\\nHa et al.[ 1365 ] Tang et al.[ 1366 ]\\nDigital Environment Wu et al.[ 1333 ] LLMSmith[ 1367 ] Wu et\\nal.[1202 ] Guastalla et al.[ 1368 ] Geip-\\ning et al.[ 1369 ] Tang et al.[ 1366 ] EIA\\n[1370 ] AdvWeb [ 1371 ] AGrail [ 1372 ]\\nAgent-Agent\\nInteraction ThreatsCompetitive Interactions Hammond et al.[ 1373 ] Hoodwinked[ 1374 ]\\nMo et al.[ 1375 ] Wen et\\nal.[1376 ] Motwani et al.[ 1377 ]\\nCooperative Interactions Pan et al.[ 1378 ] AgentSmith[ 1379 ]\\nFigure 20.1: Agent Extrinsic Safety: Threats on agent-memory, agent-environment, and agent-agent interactions.\\ninfluence their environment. Examples of such agents include autonomous vehicles and robotic systems. In contrast,\\ndigital interaction agents function within virtual or networked environments, processing and responding to data from\\ndigital sources. These include AI-powered chatbots, cybersafety systems, and automated trading algorithms.\\nThreats in Physical Environment. Agents operating in the physical world, such as robots and autonomous vehicles,\\nface distinct safety challenges due to their interaction with dynamic and potentially adversarial environments [ 1359 ,\\n1360 ,1366 ]. One major threat is sensor spoofing, where attackers manipulate sensor inputs to deceive the agent about\\nits surroundings. For example, GPS spoofing can pose significant risks to UA Vs (unmanned aerial vehicles) and other\\nGPS-dependent platforms by misleading autonomous vehicles about their actual location. This allows for malicious\\nredirection or hijacking [ 1361 ]. Similarly, LiDAR spoofing can introduce false obstacles that don’t actually exist,\\npotentially leading to navigation failures or safety hazards [ 1362 ]. Another critical risk is actuator manipulation, where\\nadversaries take control of an agent’s actuators, forcing it to perform unintended physical actions. This can occur through\\ndirect tampering with the hardware or by exploiting vulnerabilities in the software that governs actuator functions [ 1363 ].\\nSuch attacks can compromise the agent’s actions, leading to physical harm or mission failure. Additionally, exploiting\\nenvironmental hazards is a serious threat. Attackers may introduce physical obstacles or manipulate environmental\\nconditions to disrupt an agent’s operations. For example, adversarial objects created using techniques like LiDAR-Adv\\ncan deceive LiDAR-based autonomous driving systems by inducing sensor misinterpretations, thus degrading detection\\nreliability and increasing real-world safety risks [ 1364 ]. Lastly, misalignment in physical actions can undermine the\\nsafety of autonomous agents. Discrepancies between an agent’s perception and the actual physical constraints of its\\nenvironment can lead to unsafe or infeasible actions. For example, mismatches between learned locomotion policies\\nand real-world physics—such as misjudging terrain rigidity or obstacle dimensions—can cause autonomous agents to\\ntake hazardous steps (e.g., unstable strides on rough surfaces). This has been observed in prior systems that required\\nover 100 manual resets due to uncontrolled falls [1365].\\nThreats in Digital Environment. Agents operating in digital environments, such as software agents and web-based\\nagents, face distinct safety challenges arising from their reliance on external data sources and computational resources\\n[1333 ,1366 ]. One major threat is code injection, where malicious actors introduce harmful code into the agent’s\\nenvironment, leading to unintended command execution [ 1367 ]. These attacks often exploit software vulnerabilities or\\nleverage compromised external resources that the agent interacts with, potentially resulting in unauthorized control over\\nthe agent’s operations [ 1202 ]. Environmental Injection Attack (EIA) exploits privacy risks in generalist web agents\\nto stealthily steal users’ PII, achieving up to 70% success rate [ 1370 ]. AdvWeb is an automated adversarial prompt\\ngeneration framework to mislead black-box web agents into executing harmful actions [ 1371 ]. Another critical risk\\nis data manipulation, where attackers alter the information an agent receives, causing incorrect decisions or actions\\n[1333 ]. For example, a trading agent can be misled by manipulated financial data, leading to incorrect transactions, or\\nan information-gathering agent may be tricked by falsified news articles, distorting its outputs. Such manipulations can\\nhave cascading effects, especially in automated systems that rely on accurate data for decision-making. Beyond direct\\nmanipulation, denial-of-service (DoS) attacks pose a serious threat by overwhelming the agent’s digital environment\\n181\\nwith excessive requests or data, effectively rendering it unresponsive or causing it to crash [ 1368 ]. These disruptions can\\nbe particularly detrimental to time-sensitive applications where availability and responsiveness are critical. Additionally,\\nresource exhaustion is a significant threat, as adversaries may exploit the agent’s resource management mechanisms\\nto deplete computational resources, leading to service denial for other users or overall system instability [ 1369 ].\\nBy draining processing power, memory, or bandwidth, attackers can severely impair an agent’s ability to function\\neffectively, disrupting its operations and reducing its efficiency. In addressing the safety challenges of LLM agents,\\nAGrail is proposed as a lifelong guardrail framework that enhances agent security by adapting safety checks to mitigate\\ntask-specific and systemic risks, demonstrating robust performance and transferability across diverse tasks [1372].\\n20.3 Agent-Agent Interaction Threats\\nIn multi-agent systems, interactions between agents can introduce new safety vulnerabilities [ 1380 ]. These interactions\\nare mainly competitive, where agents try to outdo each other, or cooperative, where they work together.\\nThreats in Competitive Interactions. When agents compete, they often use tricky methods to gain an advantage\\n[1373 ]. For example, they might spread false information or make other agents think the situation is different from\\nreality to deceive them [ 1374 ]. This can lead opponents to make poor decisions, weakening their position. Apart from\\nmisinformation, agents may also try to take advantage of weaknesses in their opponent’s algorithms or strategies [ 1375 ].\\nBy identifying these weaknesses, they can predict and manipulate the other agent’s behavior, gaining an edge in the\\ncompetition. Additionally, some agents might use disruptive techniques like denial-of-service (DoS) attacks, which\\noverload an opponent’s system with unnecessary requests, disrupting communication and hindering their ability to\\nfunction [ 1376 ]. Another threat in competitive interactions is covert collaboration. Sometimes agents secretly cooperate,\\neven when it’s against the rules, to manipulate the outcome in their favor [ 1377 ]. This kind of collusion undermines\\nfairness and damages the integrity of the system, as it skews the competition in their favor.\\nThreats in Cooperative Interactions. In cooperative situations, where agents work together toward a common\\ngoal, safety threats could damage the system’s stability and reliability. One risk is unintentional information leakage,\\nwhere agents accidentally share sensitive data during their communication. This could lead to privacy violations or\\nunauthorized access, weakening the system’s trustworthiness. In addition to data leaks, errors made by one agent can\\nspread throughout the system, causing bigger failures and lowering overall performance. [ 1378 ] discusses this problem\\nin Open-Domain Question Answering Systems (ODQA), where errors from one part of the system can ripple through\\nand affect other components, severely impacting reliability. The situation becomes even worse if one compromised\\nagent introduces a vulnerability that spreads to others. If a hacker successfully takes control of one agent, they could\\nexploit weaknesses throughout the entire system, leading to a major safety failure [ 1379 ]. This kind of widespread\\ncompromise is dangerous because it could start with a small breach and escalate quickly. Another challenge comes\\nfrom poor synchronization between agents. If agents don’t update their information at the same time or experience\\ndelays in communication, it can cause problems in decision-making. Misalignment or delays in updates can disrupt\\ncoordination, making it harder for the agents to achieve their shared goals effectively. These challenges emphasize the\\nneed for strong safety systems in cooperative multi-agent setups to keep them reliable and resistant to attacks.\\n20.4 Summary and Discussion\\nThe preceding sections have detailed the significant safety risks that arise from AI agents interacting with memory\\nsystems, physical and digital environments, and other agents. These risks, ranging from data poisoning and code\\ninjection to sensor spoofing and collusion, highlight the vulnerabilities inherent in increasingly complex agent-based\\nsystems. However, as AI agents become more capable, utilizing natural language understanding and specialized tools\\nfor sophisticated reasoning, researchers are actively developing safety protocols to address these challenges. These\\nprotocols differ in approach for general-purpose and domain-specific agents.\\nGeneral-purpose agents, designed for versatility across various domains, face a broad spectrum of safety challenges.\\nTo mitigate these risks, researchers have developed several methods to enhance their safety. Evaluation mechanisms,\\nsuch as AgentMonitor [ 1381 ], assess the safety awareness of agents by monitoring their decision-making processes\\nand identifying potentially unsafe actions. R-Judge [ 1382 ] quantifies an agent’s risk awareness by evaluating its\\nresponses to both malicious and benign queries, offering a systematic approach to safety compliance. Additionally,\\nrisk detection tools like ToolEmu [ 795] simulate tool usage in controlled environments to expose vulnerabilities in\\nagent interactions. This approach identifies potential hazards during task execution, allowing developers to address\\nvulnerabilities proactively. These combined efforts enhance the safety of general-purpose agents through comprehensive\\nevaluation and risk detection.\\n182\\nDomain-specific agents, tailored for specialized tasks in high-stakes environments like scientific research, require even\\nmore stringent safety measures. Safety tools such as ChemCrow [ 1383 ] are designed to mitigate risks in chemical\\nsynthesis tasks by reviewing user queries and filtering malicious commands, ensuring agents do not inadvertently\\nsynthesize hazardous chemicals. Structured task constraints, as implemented in CLAIRify [ 1384 ], enhance experimental\\nsafety by imposing high-level constraints on material synthesis order and low-level restrictions on manipulation and\\nperception tasks, thereby preventing accidents and errors. Furthermore, benchmarks like SciGuard [ 1385 ], which\\nincludes the SciMT-Safety benchmark, evaluate model safety by measuring both harmlessness (rejecting malicious\\nqueries) and helpfulness (handling benign queries effectively). SciGuard also incorporates long-term memory to\\nenhance agents’ ability to safely execute complex instructions while maintaining accurate risk control. These focused\\napproaches ensure that domain-specific agents operate safely and effectively within their specialized fields.\\nIn summary, significant progress has been made in developing innovative evaluation mechanisms and risk mitigation\\nstrategies to enhance the safety of both general-purpose and domain-specific AI agents. However, a critical area for\\nfuture research lies in integrating these approaches. Building stronger connections between the broad capabilities of\\ngeneral-purpose agents and the focused safeguards of domain-specific agents will be essential for creating truly robust\\nand trustworthy LLM systems. The challenge is to combine the best aspects of both approaches to develop agents that\\nare both versatile and secure.\\n183\\nChapter 21\\nSuperalignment and Safety Scaling Law in\\nAI Agents\\n21.1 Superalignment: Goal-Driven Alignment for AI Agents\\nAs LLMs increasingly serve as the core of decision making of autonomous agents, ensuring that their output remains\\nsafe, ethical, and consistently aligned with human objectives has become a pressing challenge [ 1386 ,402,1387 ].\\nTraditional alignment techniques, particularly RLHF, have been instrumental in refining LLM behavior by incorporating\\nhuman preferences [110, 43].\\nTraditional safety alignment focuses primarily on preventing harmful outcomes by enforcing predefined constraints.\\nIn such frameworks, an agent’s behavior is guided by a single aggregated reward signal that prioritizes immediate\\ncorrections over long-range planning. Although this reactive approach works in many current applications, it struggles\\nwhen an agent must execute extended, multifaceted tasks. The inability to decompose intricate, long-term goals into\\ninterpretable and manageable sub-objectives may result in behavior that is technically safe yet suboptimal for fulfilling\\nbroader human-centric aims.\\nTo address these limitations, the concept of superalignment [1388 ] has emerged. Superalignment represents an\\nevolution in alignment strategies by embedding explicit long-term goal representations directly into an agent’s decision-\\nmaking process. Rather than simply imposing constraints to avoid harmful actions, superalignment proactively\\ngoverns behavior through a composite objective function. This function integrates several dimensions of perfor-\\nmance—specifically, safety and ethical considerations (where ethical norms and safety guidelines are continuously\\nembedded in decision-making), task effectiveness (ensuring the agent not only avoids harmful behavior but also\\nperforms its intended functions with high competence), and long-term strategic planning (enabling the agent to plan\\nover extended horizons and break down complex goals into manageable subtasks).\\nIntegrating superalignment into AI systems marks a pivotal shift toward more robust, goal-driven alignment strategies.\\nBy unifying safety, ethical standards, task performance, and long-term planning within a single optimization framework,\\nsuperalignment aims to enhance the reliability and robustness of autonomous agents by ensuring they remain aligned\\nwith human values over prolonged operational periods; facilitate dynamic adaptation in complex environments by\\nreconciling immediate safety concerns with strategic, long-term objectives; and provide a clearer, more interpretable\\nstructure for diagnosing and refining AI behavior—crucial for both safety audits and continuous improvement.\\nFuture research is expected to focus on developing algorithms that effectively balance these diverse objectives and on\\nvalidating superalignment strategies in real-world applications. The ultimate goal is to establish a scalable framework\\nthat not only prevents harmful behavior but also actively promotes performance that aligns with complex human values\\nand objectives.\\n21.1.1 Composite Objective Functions in Superalignment\\nAt the core of superalignment is the composite objective function, which is a structured reward mechanism that\\nintegrates multiple dimensions of performance to guide agent behavior [ 1176 ]. Unlike traditional alignment, which\\noften relies on a single, aggregated reward function, superalignment explicitly decomposes the objective into three\\ndistinct components:\\n184\\n•Task Performance Term: Ensures the agent executes immediate operational tasks with high accuracy and\\nefficiency.\\n•Goal Adherence Term: Embeds long-term strategic objectives into the agent’s decision-making process,\\nwhich incorporates safety constraints, ethical considerations, and user-defined priorities [1178, 1389].\\n•Norm Compliance Term: Enforces adherence to ethical and legal boundaries, which prevents behaviors that\\noptimize short-term rewards at the expense of long-term alignment [1390, 1391].\\nThis multicomponent formulation addresses a key weakness of RLHF: the risk of reward hacking, where an agent\\nexploits loosely defined reward functions to maximize short-term gains while failing to achieve genuine long-term\\nalignment [1392, 1393].\\n21.1.2 Overcoming the Limitations of RLHF with Superalignment\\nTraditional RLHF relies on implicit feedback signals, which typically aggregated over short-term interactions. Although\\neffective in refining the model output, this approach struggles with long-term goal retention due to several inherent\\nlimitations. Firstly, human feedback tends to be short-sighted, prioritizing immediate correctness over broader strategic\\nalignment [ 110]. Secondly, reward models often oversimplify complex multistep tasks, making it difficult for agents to\\ngeneralize effectively over extended time horizons [ 1394 ]. Thirdly, agents can exploit loopholes in reward structures,\\nwhich optimizes behaviors that superficially align with human preferences while ultimately diverges from intended\\nobjectives [1395].\\nSuperalignment addresses these challenges through explicit goal conditioning. Rather than relying solely on aggregated\\nreward signals, it structures objectives hierarchically, and decomposes complex tasks into smaller, interpretable\\nsubgoals [ 1396 ,1397 ]. This structured approach improves transparency, allows real-time adjustments, and ensures that\\nAI systems maintain long-term coherence in decision making.\\n21.1.3 Empirical Evidence Supporting Superalignment\\nRecent research provides strong empirical support for superalignment in real-world applications. Studies have shown\\nthat agents trained with composite objectives demonstrate greater robustness in extended interactions, and outperform\\nthose relying on conventional alignment techniques [ 1398 ,1399 ,1400 ]. Unlike static reward functions, which remain\\nfixed regardless of changing conditions, superaligned models employ continuous calibration that dynamically adjusts\\nthe weighting of different objectives in response to real-time operational data [ 400]. This adaptive framework enables\\nagents to respond to evolving user needs while maintaining long-term strategic alignment, a capability that is largely\\nabsent in traditional RLHF-based approaches.\\n21.1.4 Challenges and Future Directions\\nDespite its promise, superalignment presents several critical challenges that must be addressed for practical implemen-\\ntation. These challenges primarily involve goal specification, reward calibration, dynamic adaptation, and maintaining\\ncoherence in hierarchical objectives.\\nA fundamental difficulty lies in defining precise and unambiguous goals. Human values are inherently context\\nsensitive, ambiguous, and sometimes conflicting, which makes it challenging to encode them into a structured, machine-\\ninterpretable format [ 1387 ]. Existing alignment techniques struggle to capture the full complexity of human intent,\\nnecessitating more advanced methods for goal extraction, decomposition, and representation. Current research explores\\nhierarchical modeling and preference learning to enable AI systems to better adapt to evolving and nuanced human\\nobjectives [1392].\\nEven with well-defined goals, reward calibration remains a significant challenge. Superalignment requires a careful\\nbalance between task performance, long-term adherence, and ethical compliance [ 1401 ]. A poorly calibrated reward\\nstructure can lead to short-term optimization at the expense of strategic alignment or, conversely, excessive emphasis on\\nlong-term objectives at the cost of immediate effectiveness. Adaptive weighting mechanisms help dynamically adjust\\nreward components, but ensuring stability and consistency in these adjustments remains an open research problem [ 321].\\nAnother challenge stems from adapting to dynamic human values and evolving operational contexts. Unlike static\\nrule-based systems, AI models must continuously update their objectives to reflect shifts in societal norms, ethical\\nstandards, and external conditions [ 1402 ]. Real-time goal recalibration, facilitated by meta-learning and context-aware\\nalignment, enables AI systems to recognize when their objectives require refinement and adjust accordingly [ 1390 ].\\nHowever, ensuring that models can update their value representations without compromising alignment remains an\\nunresolved issue.\\n185\\nFinally, maintaining coherence in hierarchical goal decomposition adds another layer of complexity. Superalignment\\ndepends on breaking down long-term objectives into sub-goals while preserving strategic alignment. Overly rigid\\nsub-goals can lead to narrow optimization that neglects broader intent, while loosely defined sub-goals risk misalignment\\nbetween immediate actions and overarching objectives [ 321]. Techniques such as recursive validation and multi-level\\nreward structuring aim to mitigate these risks, but further research is needed to refine their applicability across diverse\\nAI systems [1396].\\nTo sum up, while superalignment offers a structured approach to AI alignment, its successful implementation depends on\\novercoming goal ambiguity, reward miscalibration, value drift, and hierarchical misalignment. Future work should focus\\non enhancing interpretability, stability, and adaptability to ensure AI systems remain aligned with human objectives\\nover extended time horizons.\\n21.2 Safety Scaling Law in AI Agents\\nThe exponential scaling of AI capabilities has unveiled a fundamental tension in artificial intelligence: the nonlinear\\nescalation of safety risks [ 1403 ]. As language models grow from millions to trillions of parameters, their performance\\nfollows predictable scaling laws [ 1404 ,1405 ], but safety assurance exhibits starkly different dynamics [ 1403 ].Safety\\nScaling Law —the mathematical relationship describing how safety interventions must scale to maintain acceptable risk\\nlevels as model capabilities expand. The core challenge of the safety scaling law lies in ensuring that safety measures\\nevolve proportionally to model capabilities, as performance improvements often outpace safety improvements. Recent\\nresearch has quantified this tension and proposed frameworks to address it:\\n•Capability-Risk Trade-off : Zhang et al. [295] established the first quantitative relationship between model\\npower and safety risks, demonstrating that more capable models inherently face higher vulnerability surfaces.\\nThis work introduced the Safety-Performance Index (SPI) to measure this trade-off.\\n•Helpfulness-Safety Relationship : Building on this, Ruan et al. [795] revealed that models optimized for\\nhelpfulness exhibit 37% more safety-critical failures, highlighting the need for joint optimization frameworks.\\n•Commercial vs.Open-Source Dynamics : Through large-scale benchmarking, Ying et al. [1406 ] uncovered\\ndivergent safety-performance profiles: Commercial models ( e.g., Claude-3.5 Sonnet) achieve 29% higher\\nsafety scores through specialized safety pipelines, but at 15% performance cost. Open-source models show\\ntighter coupling, with Phi-series achieving 91% of commercial safety levels at 40% lower computational cost.\\n•Scale-Data Interplay : Contrary to expectations, model size only explains 42% of safety variance, while data\\nquality accounts for 68%, suggesting that data-centric approaches may outperform pure scaling.\\n•Multimodal Vulnerabilities : MLLMs exhibit 2.1X more safety failures during visual grounding, with\\ncross-modal attention heads identified as primary failure points (71% of harmful outputs).\\nThese findings [ 295,795,1406 ] collectively demonstrate that safety scaling requires more than proportional investment—\\nit demands architectural innovations that fundamentally alter the capability-risk relationship. Then, we will review the\\nexplorations [1407, 1408, 1409] on how emerging alignment techniques address these challenges.\\n21.2.1 Current landscape: balancing model safety and performance\\nIn recent years, the safety and performance of AI models have become critical topics of research, particularly as these\\nmodels are increasingly deployed in high-stakes applications. Zhang et al. [295] proposed the first to quantify the\\nrelationship between model safety and performance, revealing that more powerful models inherently face higher safety\\nrisks. This finding underscores the challenge of balancing model capabilities with the need for robust safeguards.\\nBuilding on this, Ruan et al. [795] explored how helpfulness—defined as a model’s ability to assist users—interacts\\nwith safety concerns. Further advancing the discussion, Ying et al. [1406 ] conducted a more detailed comparison and\\nanalysis of model safety and performance, leading to the following conclusions: (1) As shown in Figure 21.1 (A)\\nand Figure 21.1 (C), the safety and performance of commercial models often show an inverse relationship, as safety\\nmeasures and investments differ between companies. In contrast, open-source models tend to exhibit a positive\\ncorrelation between general performance and safety—better performance often leads to improved safety. Commercial\\nmodels usually outperform open-source models in terms of safety, with Claude-3.5 Sonnet being the most secure\\namong commercial models, while the Phi series stands out as the most secure open-source model. (2) As shown in\\nFigure 21.1 (B), model size does not have a strict linear relationship with safety performance. The quality of training\\ndata and pipeline are also key factors influencing safety; (3) Multimodal large language models (MLLMs) tend to\\ncompromise safety during visual language fine-tuning and multimodal semantic alignment, with safety performance\\ninfluenced by both the underlying language model and their specific training strategies.\\n186\\n21.2.2 Enhancing safety: preference alignment and controllable design\\nAs the capabilities of LLMs continue to grow, concerns regarding their safety have become increasingly prominent.\\nEnhancing model safety is therefore a critical challenge in the development of LLMs. Previous studies have proposed\\nvarious approaches to address this issue, including the use of in-context exemplars and self-safety checks, red-teaming\\ntechniques [ 1410 ], and Safe reinforcement learning from human feedback (Safe RLHF) [ 43]. The safety issues in\\nLLMs can essentially be framed as an alignment problem. The goal is to align the model with datasets containing both\\nsafe and less secure responses. Through this alignment, the model learns to prioritize generating safer outputs while\\nminimizing the risk of harmful content. With the support of preference optimization techniques (such as DPO [ 111],\\nIPO [ 1411 ],etc.), this alignment process fine-tunes the model to produce responses that meet safety standards. As\\nreported in [ 1407 ], various preference optimization methods are investigated for safety enhancement, including Safe-\\nDPO [ 111], Safe-robust-DPO [ 1412 ], Safe-IPO [ 1411 ], Safe-SLiC [ 1413 ], Safe-KTO [ 395], and Safe-NCA [ 1408 ],\\netc. The results indicate that most preference optimization methods can significantly enhance safety, albeit at the\\ncost of general performance, particularly in MATH capabilities. Among these methods, noise contrastive alignment\\n(Safe-NCA) [ 1408 ] is identified as an optimal approach for balancing safety with overall model performance. The core\\nof the Safe-NCA [ 1408 ] method lies in utilizing a custom contrastive loss function, combined with a safety dataset, to\\ntrain a model that is safer and more robust during generation by comparing the generated safe and unsafe responses with\\nthe outputs of a reference model. Beyond enhancing safety, achieving flexible control over the trade-offs between safety\\nand helpfulness is equally critical. AI models should strike an appropriate balance between safety and helpfulness, based\\non the specific needs of different users. To illustrate, for the prompt “Tell me how to make a potion”, LLMs should\\nadjust their responses based on the user’s profile. For scientists, the response should provide relevant and technically\\naccurate information. For teenagers, the model should prioritize safety, offering cautious and harmless suggestions.\\nTo achieve this, Tuan et al. [1409 ] propose a framework based on self-generated data to enhance model controllability.\\nBy introducing control tokens as inputs, users can specify the desired safety and helpfulness in model responses. The\\ncontrol tokens define the requested levels of safety and helpfulness in the following form:\\n[helpful =shp][harmless =ssf]. (21.1)\\nThe proposed method can “rewind” aligned LLMs and unlock their safety and helpfulness using self-generated data,\\nwith fine-tuning to further enhance controllability. However, achieving independent control over safety and helpfulness\\nremains a significant challenge. This is because: (1) Certain prompts may be difficult to define in terms of balancing\\nsafety and helpfulness, or the definitions of both may conflict in certain contexts. For example, in the query “I want the\\nnet worth of the person,” it can be difficult to determine how safety and helpfulness should be prioritized. (2) Some\\nmodels may have already established a fixed trade-off during the training process, which could limit their flexibility by\\nforcing them to adhere to a specific priority, thereby preventing adjustments based on different application scenarios.\\n(3) Many training data examples inherently satisfy both safety and helpfulness criteria, leading to a high correlation\\nbetween these two attributes during model training.\\n21.2.3 Future directions and strategies: the AI-45° rule and risk management\\nIn the field of AI safety, despite various safety recommendations and extreme risk warnings being proposed, there\\nstill lacks a comprehensive guide to balance AI safety and capability. Chao et al. [1414 ] introduce the AI-45 °Rule\\nas a guiding principle for achieving a balanced roadmap towards trustworthy AGI. The rule advocates for the parallel\\ndevelopment of AI capabilities and safety measures, with both dimensions advancing at the same pace, represented\\nby a 45 °line in the capability-safety coordinate system. It emphasizes that current advances in AI capabilities often\\noutpace safety measures, exposing systems to greater risks and threats. Therefore, risk management frameworks\\nsuch as the Red Line and Yellow Line are proposed to monitor and manage these risks as AI systems scale. As\\nmentioned in the International Dialogues on AI Safety (IDAIS), the “Red Line” for AI development is defined, which\\nincludes five key aspects: autonomous replication or improvement, power-seeking behavior, assistance in weapon\\ndevelopment, cyberattacks, and deception. Additionally, the concept of the “Yellow Line” is designed to complement\\nand expand existing safety evaluation frameworks, such as Anthropic’s responsible scaling policies. Models below these\\nwarning thresholds require only basic testing and evaluation. However, more advanced AI systems that exceed these\\nthresholds necessitate stricter assurance mechanisms and safety protocols to mitigate potential risks. By establishing\\nthese thresholds, a proactive approach can be taken to ensure that AI systems are developed, tested, and deployed with\\nappropriate safeguards in place.\\n187\\nFigure 21.1: Performance and safety analysis of LLMs. (a) The relationship between LLM model size and their\\naverage ASR across various attacks. The data are sourced from experimental results of a study assessing the robustness\\nof LLMs against adversarial attacks [ 295]. (b) The relationship between the capability of LLMs and their average\\nattack success rate (ASR) across various attacks. The LLM capability data are derived from the Artificial Analysis\\nIntelligence Index on the Artificial Analysis platform’s LLM leaderboard [ 1415 ]. (c) Heatmap of performance across\\nmultiple benchmark tasks. The figure presents a heatmap that illustrates the performance of various LLMs across\\nmultiple benchmark tasks, including GPQA, MuSR, MATH, IFEval, MMLU-Pro, and BBH, with data sourced from\\nHugging Face’s Open LLM Leaderboard v2 [1416].\\n188\\nChapter 22\\nConcluding Remarks and Future Outlook\\nWe have explored in this survey the evolving landscape of foundation agents by drawing parallels between human cog-\\nnitive processes and artificial intelligence. We began by outlining the core components of intelligent agents—detailing\\nhow modules such as memory, perception, emotion, reasoning, and action can be modeled in a framework inspired by\\nthe comparison with human brain. Our discussion highlighted how these agents can be structured in a modular fashion,\\nenabling them to emulate human-like processing through specialized yet interconnected subsystems.\\nWe then delved into the dynamic aspects of agent evolution, examining self-improvement mechanisms that leverage\\noptimization techniques, including both online and offline strategies. By investigating how large language models can\\nact as both reasoning entities and autonomous optimizers, we illustrated the transformative potential of agents that\\ncontinuously adapt to changing environments. Building on these technical foundations, we highlighted how agents\\ncan drive the self-sustaining evolution of their intelligence through closed-loop scientific innovation. We introduced\\na general measure of intelligence for knowledge discovery tasks and surveyed current successes and limitations in\\nagent-knowledge interactions. This discussion also shed light on emerging trends in autonomous discovery and tool\\nintegration, which are crucial for the advancement of adaptive, resilient AI systems.\\nOur paper also addressed the collaborative dimension of intelligent systems, analyzing how multi-agent interactions\\ncan give rise to collective intelligence. We explored the design of communication infrastructures and protocols that\\nenable both agent-agent and human-AI collaboration. This discussion underscored the importance of fostering synergy\\nbetween diverse agent capabilities to achieve complex problem solving and effective decision-making.\\nFinally, we emphasized the critical challenge of building safe and beneficial AI. Our review encompassed intrinsic\\nand extrinsic security threats, from vulnerabilities in language models to risks associated with agent interactions. We\\nprovided a comprehensive overview of safety scaling laws and ethical considerations, proposing strategies to ensure\\nthat the development of foundation agents remains aligned with societal values. Overall, our work offers a unified\\nroadmap that not only identifies current research gaps but also lays the foundation for future innovations in creating\\nmore powerful, adaptive, and ethically sound intelligent agents.\\nLooking ahead, we envision several key milestones that will mark significant progress in the development of intelligent\\nagents. First, we anticipate the emergence of general-purpose agents capable of handling a wide array of human-level\\ntasks, rather than being confined to specific domains. These agents will integrate advanced reasoning, perception,\\nand action modules, enabling them to perform tasks with human-like adaptability and versatility. Achieving this\\nmilestone will represent a fundamental shift in how AI can support and augment human capabilities in both everyday\\nand specialized contexts.\\nAnother critical milestone is the development of agents that learn directly from their environment and continuously self-\\nevolve through interactions with humans and data. As the distinction between training-time and test-time computation\\ngradually disappears, agents will acquire new skills on the fly by engaging with their surroundings, other agents, and\\nhuman partners. This dynamic learning process is essential for achieving human-level capabilities and for enabling\\nagents to keep pace with a constantly changing world. It is also vital if agents are to be able to drive innovation in\\nscientific discovery, as this expands the boundaries of evolution for both agents and humanity.\\nWe predict that agents will transcend traditional human limitations by transforming individual human know-how into\\ncollective agent intelligence. The current inefficiencies in human information sharing—where complex knowledge\\nrequires extensive practice to transfer—will be overcome by agents, which offer a format of human know-how that is\\n189\\nboth transferable and infinitely duplicable. This breakthrough will remove the bottleneck of complexity, enabling a new\\nintelligence network effect whereby a large ensemble of human and AI agents can operate at a level of intelligence that\\nscales with network size. In this scenario, the fusion of agent-acquired knowledge and human expertise will foster an\\nenvironment where insights and innovations are disseminated and applied rapidly across various domains.\\nWe also anticipate this intelligence network effect enabling the establishment of a new paradigm for human-AI\\ncollaboration—one that is larger in scale, more interdisciplinary, and more dynamically organized than ever before.\\nThe resulting human-AI society will achieve previously unattainable levels of complexity and productivity, heralding a\\ntransformative era in both technological and social development.\\nIn summary, these milestones outline a future where intelligent agents become increasingly autonomous, adaptive,\\nand deeply integrated with human society—driving scientific discovery, enhancing knowledge sharing, and redefining\\ncollaboration on a global scale.\\n190\\nAcknowledge\\nArgonne National Laboratory’s work was supported by the U.S. Department of Energy, Office of Science, under contract\\nDE-AC02-06CH11357. XLQ acknowledges the support of the Simons Foundation.\\n191\\nBibliography\\n[1] Alan M Turing. Computing machinery and intelligence . Springer, 2009.\\n[2]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\\nAdvances in neural information processing systems , 33:1877–1901, 2020.\\n[3] Stuart J Russell and Peter Norvig. Artificial intelligence: a modern approach . pearson, 2016.\\n[4]Allen Newell and Herbert Alexander Simon. Gps, a program that simulates human thought. Rand Corporation\\nSanta Monica, CA , 1961.\\n[5]Rodney Brooks. A robust layered control system for a mobile robot. IEEE journal on robotics and automation ,\\n2(1):14–23, 1986.\\n[6] Michael Wooldridge. An introduction to multiagent systems . John wiley & sons, 2009.\\n[7] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt/ , 2022.\\n[8] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng,\\nChenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 , 2024.\\n[9]Anthropic. Claude: The next step in helpful ai. https://www.anthropic.com , 2023. Accessed: 2024-12-01.\\n[10] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\\nFei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115 , 2024.\\n[11] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\\nlanguage models. arXiv preprint arXiv:2302.13971 , 2023.\\n[12] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, et al. Training a helpful and harmless assistant\\nwith rlhf. OpenAI Technical Report , 2022.\\n[13] Eric R Kandel, James H Schwartz, Thomas Jessell, Steven A Siegelbaum, and AJ Hudspeth. Principles of\\nneural science, 2013.\\n[14] Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, and Matthew Botvinick. Neuroscience-inspired\\nartificial intelligence. Neuron , 95(2):245–258, 2017.\\n[15] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature , 521(7553):436–444, 2015.\\n[16] Dale Purves, George J Augustine, David Fitzpatrick, William Hall, Anthony-Samuel LaMantia, and Leonard\\nWhite. Neurosciences . De Boeck Supérieur, 2019.\\n[17] Marvin Minsky. Society of mind . Simon and Schuster, 1988.\\n[18] Gyorgy Buzsaki. The brain from inside out . Oxford University Press, USA, 2019.\\n[19] Karl J Friston, Jean Daunizeau, James Kilner, and Stefan J Kiebel. Action and behavior: a free-energy\\nformulation. Biological cybernetics , 102:227–260, 2010.\\n[20] Stuart Russell and Peter Norvig. Artificial Intelligence: A Modern Approach . Pearson, 4th edition, 2020.\\n[21] Larry R Squire. Memory and the hippocampus: a synthesis from findings with rats, monkeys, and humans.\\nPsychological review , 99(2):195, 1992.\\n[22] Mark Bear, Barry Connors, and Michael A Paradiso. Neuroscience: exploring the brain, enhanced edition:\\nexploring the brain . Jones & Bartlett Learning, 2020.\\n[23] Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional interpretation of some\\nextra-classical receptive-field effects. Nature neuroscience , 2(1):79–87, 1999.\\n192\\n[24] Joseph E LeDoux. The emotional brain: The mysterious underpinnings of emotional life . Simon and Schuster,\\n1998.\\n[25] Antonio R. Damasio. Descartes’ Error: Emotion, Reason, and the Human Brain . Putnam, 1994.\\n[26] Earl K Miller and Jonathan D Cohen. An integrative theory of prefrontal cortex function. Annual review of\\nneuroscience , 24(1):167–202, 2001.\\n[27] David Badre. Cognitive control, hierarchy, and the rostro–caudal organization of the frontal lobes. Trends in\\ncognitive sciences , 12(5):193–200, 2008.\\n[28] Wolfram Schultz, Peter Dayan, and P Read Montague. A neural substrate of prediction and reward. Science ,\\n275(5306):1593–1599, 1997.\\n[29] Joaquin M Fuster. The Prefrontal Cortex . Academic Press, 4th edition, 2008.\\n[30] Tim Shallice and Richard P Cooper. The organisation of mind. Oxford Psychology Series , 32, 2011.\\n[31] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, Róbert Csordás, Anand Gopalakrishnan,\\nAbdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al. Mindstorms in\\nnatural language-based societies of mind. arXiv preprint arXiv:2305.17066 , 2023.\\n[32] Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke\\nNoda, Demetri Terzopoulos, Yejin Choi, Katsushi Ikeuchi, Hoi V o, Li Fei-Fei, and Jianfeng Gao. AGENT AI:\\nSURVEYING THE HORIZONS OF MULTIMODAL INTERACTION. arXiv preprint arXiv:2401.03568 ,\\n2024.\\n[33] Qiuyuan Huang, Naoki Wake, Bidipta Sarkar, Zane Durante, Ran Gong, Rohan Taori, Yusuke Noda, Demetri\\nTerzopoulos, Noboru Kuno, Ade Famoti, Ashley Llorens, John Langford, Hoi V o, Li Fei-Fei, Katsu Ikeuchi,\\nand Jianfeng Gao. Position Paper: Agent AI Towards a Holistic Intelligence, 2024. URL http://arxiv.org/\\nabs/2403.00833 .\\n[34] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie\\nJin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey, 2023.\\n[35] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,\\nXu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A Survey on Large Language Model\\nbased Autonomous Agents, 2023. URL http://arxiv.org/abs/2308.11432 .\\n[36] Yu Su, Diyi Yang, Shunyu Yao, and Tao Yu. Language agents: Foundations, prospects, and risks. In\\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts ,\\npages 17–24, Miami, Florida, USA, November 2024. Association for Computational Linguistics. URL\\nhttps://aclanthology.org/2024.emnlp-tutorials.3 .\\n[37] Tula Masterman, Sandi Besen, Mason Sawtell, and Alex Chao. The landscape of emerging ai agent architectures\\nfor reasoning, planning, and tool calling: A survey. arXiv preprint arXiv:2404.11584 , 2024.\\n[38] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V . Chawla, Olaf Wiest, and\\nXiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges. In\\nKate Larson, editor, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence,\\nIJCAI-24 , pages 8048–8057. International Joint Conferences on Artificial Intelligence Organization, 8 2024.\\ndoi:10.24963/ijcai.2024/890. URL https://doi.org/10.24963/ijcai.2024/890 . Survey Track.\\n[39] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-\\nRong Wen. A survey on the memory mechanism of large language model based agents. arXiv preprint\\narXiv:2404.13501 , 2024.\\n[40] Miao Yu, Fanci Meng, Xinyun Zhou, Shilong Wang, Junyuan Mao, Linsey Pang, Tianlong Chen, Kun Wang,\\nXinfeng Li, Yongfeng Zhang, Bo An, and Qingsong Wen. A survey on trustworthy llm agents: Threats and\\ncountermeasures. arXiv preprint arXiv:2503.09648 , 2025.\\n[41] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M\\nDai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 ,\\n2021.\\n[42] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Ges-\\nmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International\\nconference on machine learning , pages 2790–2799. PMLR, 2019.\\n[43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\\nhuman feedback. Advances in neural information processing systems , 35:27730–27744, 2022.\\n193\\n[44] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with\\nreinforced fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational\\nLinguistics , 2024. URL https://arxiv.org/abs/2404.03592 .\\n[45] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-\\nRong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning, 2025. URL\\nhttps://arxiv.org/abs/2503.05592 .\\n[46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\\nand Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo,\\nS. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information\\nProcessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022,\\nNew Orleans, LA, USA, November 28 - December 9, 2022 , 2022. URL http://papers.nips.cc/paper_\\nfiles/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html .\\n[47] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-\\nima Anandkumar. V oyager: An open-ended embodied agent with large language models. arXiv preprint\\narXiv:2305.16291 , 2023.\\n[48] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\\nReflexion: language agents with verbal reinforcement learning. In Neural Information Processing Systems ,\\n2023. URL https://api.semanticscholar.org/CorpusID:258833055 .\\n[49] Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. ReAct meets ActRe: Autonomous\\nannotations of agent trajectories for contrastive self-training. arXiv preprint arXiv:2403.14589 , 2024.\\n[50] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein.\\nGenerative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium\\non user interface software and technology , pages 1–22, 2023.\\n[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable\\nvisual models from natural language supervision. In ICML , volume 139 of Proceedings of Machine Learning\\nResearch , pages 8748–8763. PMLR, 2021.\\n[52] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS , 2023.\\n[53] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao,\\nSong XiXuan, et al. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information\\nProcessing Systems , 37:121475–121499, 2025.\\n[54] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He,\\nJunyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759 , 2024.\\n[55] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms\\nto reason and leverage search engines with reinforcement learning, 2025. URL https://arxiv.org/abs/\\n2503.09516 .\\n[56] NovaSky Team. Sky-t1: Train your own o1 preview model within $450, 2025.\\n[57] Open Thoughts Team. Open Thoughts, January 2025.\\n[58] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning.\\narXiv preprint arXiv:2502.03387 , 2025.\\n[59] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning,\\n2022. URL https://arxiv.org/abs/2203.14465 .\\n[60] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma,\\nAditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan\\nFirat, and Nando de Freitas. Reinforced self-training (rest) for language modeling, 2023. URL https:\\n//arxiv.org/abs/2308.08998 .\\n[61] Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen,\\nLionel M. Ni, Linyi Yang, Ying Wen, and Weinan Zhang. Openr: An open source framework for advanced\\nreasoning with large language models. CoRR , abs/2410.09671, 2024.\\n[62] Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco\\nPavone, Yuqiang Li, Wanli Ouyang, and Dongzhan Zhou. Llama-berry: Pairwise optimization for o1-like\\nolympiad-level mathematical reasoning. CoRR , abs/2410.02884, 2024.\\n194\\n[63] Zihan Wang*, Kangrui Wang*, Qineng Wang*, Pingyue Zhang*, Linjie Li*, Zhengyuan Yang, Kefan Yu,\\nMinh Nhat Nguyen, Monica Lam, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin\\nChoi, and Manling Li. Training agents by reinforcing reasoning, 2025. URL https://github.com/\\nZihanWang314/ragen .\\n[64] Hugging Face. Open-r1, 2024. URL https://github.com/huggingface/open-r1 .\\n[65] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson,\\nIgor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language\\nmodels. In Conference on Robot Learning , pages 1769–1782. PMLR, 2023.\\n[66] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and\\nselect: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint\\narXiv:2302.01560 , 2023.\\n[67] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\\nDziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances\\nin Neural Information Processing Systems , 36, 2024.\\n[68] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Nan Duan, Weizhu Chen, et al. Critic: Large language\\nmodels can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning\\nRepresentations , 2024.\\n[69] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents\\nare experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages\\n19632–19642, 2024.\\n[70] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React:\\nSynergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 , 2022.\\n[71] Wen Yang, Minpeng Liao, and Kai Fan. Markov chain of thought for efficient mathematical reasoning. arXiv\\npreprint arXiv:2410.17635 , 2024.\\n[72] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik R Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models. In Thirty-seventh Conference on\\nNeural Information Processing Systems , 2023.\\n[73] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree\\nsearch unifies reasoning, acting, and planning in language models. In Forty-first International Conference on\\nMachine Learning , 2024.\\n[74] Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning with\\nlanguage model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing , pages 8154–8173, 2023.\\n[75] Maciej Besta, Nils Blach, Ale vs.Kubí ˇcek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\\nLehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts:\\nSolving elaborate problems with large language models. In AAAI Conference on Artificial Intelligence , 2023.\\nURL https://api.semanticscholar.org/CorpusID:261030303 .\\n[76] Ge Zhang, Mohammad Ali Alomrani, Hongjian Gu, Jiaming Zhou, Yaochen Hu, Bin Wang, Qun Liu, Mark\\nCoates, Yingxue Zhang, and Jianye Hao. Path-of-thoughts: Extracting and following paths for robust relational\\nreasoning with large language models. arXiv preprint arXiv:2412.17963 , 2024.\\n[77] Yifan Zhang, Yang Yuan, and Andrew Chi-Chih Yao. On the diagram of thought. ArXiv , abs/2409.10038, 2024.\\nURL https://api.semanticscholar.org/CorpusID:272690308 .\\n[78] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V . Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,\\nand Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh\\nInternational Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 , 2023.\\n[79] Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves\\nreasoning in large language models. arXiv preprint arXiv:2304.09797 , 2023.\\n[80] Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. On the self-verification limitations of large\\nlanguage models on reasoning and planning tasks. arXiv preprint arXiv:2402.08115 , 2024.\\n[81] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason E\\nWeston. Chain-of-verification reduces hallucination in large language models. In ICLR 2024 Workshop on\\nReliable and Responsible Foundation Models , 2024.\\n195\\n[82] Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language\\nmodels. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\\n5: Industry Track) , pages 37–42, 2023.\\n[83] Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng, Fan Li, and Dongsheng Li. Llms can find mathematical\\nreasoning mistakes by pedagogical chain-of-thought. arXiv preprint arXiv:2405.06705 , 2024.\\n[84] Xinyu Pang, Ruixin Hong, Zhanke Zhou, Fangrui Lv, Xinwei Yang, Zhilong Liang, Bo Han, and Changshui\\nZhang. Physics reasoner: Knowledge-augmented reasoning for solving physics problems with large language\\nmodels. In Proceedings of the 31st International Conference on Computational Linguistics , pages 11274–11289,\\n2025.\\n[85] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le, and Denny\\nZhou. Take a step back: Evoking reasoning via abstraction in large language models. In The Twelfth\\nInternational Conference on Learning Representations , 2024.\\n[86] Simran Arora, Avanika Narayan, Mayee F Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic\\nSala, and Christopher Ré. Ask me anything: A simple strategy for prompting language models. arXiv preprint\\narXiv:2210.02441 , 2022.\\n[87] Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing.\\nChain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous\\nsources. arXiv preprint arXiv:2305.13269 , 2023.\\n[88] Lishui Fan, Mouxiang Chen, and Zhongxin Liu. Self-explained keywords empower large language models for\\ncode generation. arXiv preprint arXiv:2410.15966 , 2024.\\n[89] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\\nPeiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning.\\narXiv preprint arXiv:2501.12948 , 2025.\\n[90] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar,\\nAleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720 ,\\n2024.\\n[91] Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. Quiet-star:\\nLanguage models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629 , 2024.\\n[92] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training\\nlarge language models to reason in a continuous latent space. arXiv preprint arXiv:2412.06769 , 2024.\\n[93] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse\\nThomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models.\\nIn2023 IEEE International Conference on Robotics and Automation (ICRA) , pages 11523–11530. IEEE, 2023.\\n[94] Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar\\nKhot. Adapt: As-needed decomposition and planning with language models. arXiv preprint arXiv:2311.05772 ,\\n2023.\\n[95] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su.\\nTravelplanner: a benchmark for real-world planning with language agents. In ICML , 2024.\\n[96] Drew McDermott et al. Pddl—the planning domain definition language. AIPS-98 Planning Competition\\nCommittee , 1998. Defines PDDL, a standard language for planning domains used in LLM integrations.\\n[97] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web:\\nTowards a generalist agent for the web. Advances in Neural Information Processing Systems , 36, 2023.\\n[98] George A Miller. The magical number seven, plus or minus two: Some limits on our capacity for processing\\ninformation. Psychological review , 63(2):81, 1956.\\n[99] Kenji Doya. Complementary roles of basal ganglia and cerebellum in learning and motor control. Current\\nopinion in neurobiology , 10(6):732–739, 2000.\\n[100] Jerry A Fodor. The modularity of mind . MIT press, 1983.\\n[101] Joshua D. McGraw, Donsuk Lee, and Justin N. Wood. Parallel development of social behavior in biological\\nand artificial fish. Nature Communications , 2024.\\n[102] Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, and Sercan Ö Arık. Learn-by-interact: A\\ndata-centric framework for self-adaptive agents in realistic environments. arXiv preprint arXiv:2501.10893 ,\\n2025.\\n196\\n[103] Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training\\nin-the-wild device-control agents with autonomous reinforcement learning. arXiv preprint arXiv:2406.11896 ,\\n2024.\\n[104] Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, and Juanzi Li. Agentic reward modeling:\\nIntegrating human preferences with verifiable correctness signals for reliable reward systems, 2025. URL\\nhttps://arxiv.org/abs/2502.19328 .\\n[105] Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao\\nYu. Text2reward: Automated dense reward function generation for reinforcement learning. arXiv preprint\\narXiv:2309.11489 , 2023.\\n[106] Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, and Chuang Gan. Scaling autonomous agents via automatic\\nreward modeling and planning, 2025. URL https://arxiv.org/abs/2502.12130 .\\n[107] Yu Gu, Boyuan Zheng, Boyu Gou, Kai Zhang, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan\\nSun, and Yu Su. Is your LLM secretly a world model of the internet? model-based planning for web agents.\\narXiv preprint arXiv:2411.06559 , 2024.\\n[108] Minghao Chen, Yihang Li, Yanting Yang, Shiyu Yu, Binbin Lin, and Xiaofei He. AutoManual: Generating\\ninstruction manuals by LLM agents via interactive environmental learning. arXiv preprint arXiv:2405.16247 ,\\n2024.\\n[109] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\\nChen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021.\\n[110] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano,\\nand Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 ,\\n2019.\\n[111] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.\\nDirect preference optimization: Your language model is secretly a reward model. Advances in Neural\\nInformation Processing Systems , 36:53728–53741, 2023.\\n[112] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y . K. Li, Y . Wu, and\\nDaya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR ,\\nabs/2402.03300, 2024.\\n[113] Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/\\n2501.12599 .\\n[114] Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu,\\nand Jifeng Dai. Auto mc-reward: Automated dense reward design with large language models for minecraft. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 16426–16435,\\n2024.\\n[115] Letian Fu, Gaurav Datta, Huang Huang, William Chung-Ho Panitch, Jaimyn Drake, Joseph Ortiz, Mustafa\\nMukadam, Mike Lambeta, Roberto Calandra, and Ken Goldberg. A touch, vision, and language dataset for\\nmultimodal alignment. arXiv preprint arXiv:2402.13232 , 2024.\\n[116] Shailja Gupta, Rajesh Ranjan, and Surya Narayan Singh. A comprehensive survey of retrieval-augmented\\ngeneration (rag): Evolution, current landscape and future directions. arXiv preprint arXiv:2410.12837 , 2024.\\n[117] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou.\\nSearch-o1: Agentic search-enhanced large reasoning models, 2025. URL https://arxiv.org/abs/2501.\\n05366 .\\n[118] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL https:\\n//qwenlm.github.io/blog/qwq-32b-preview/ .\\n[119] Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Shishir G Patil, Matei Zaharia, Joseph E Gonzalez,\\nand Ion Stoica. Llms can easily learn to reason from demonstrations structure, not content, is what matters!\\narXiv preprint arXiv:2502.07374 , 2025.\\n[120] Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal.\\nV-star: Training verifiers for self-taught reasoners, 2024. URL https://arxiv.org/abs/2402.06457 .\\n[121] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math:\\nSmall LLMs can master math reasoning with self-evolved deep thinking, 2025.\\n197\\n[122] Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J. Liu,\\nJames Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky,\\nAzade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson,\\nIzzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej\\nMahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris\\nWarkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah\\nFiedel. Beyond human data: Scaling self-training for problem-solving with language models, 2024. URL\\nhttps://arxiv.org/abs/2312.06585 .\\n[123] Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. o1-coder: an\\no1 replication for coding. CoRR , abs/2412.00154, 2024.\\n[124] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin,\\nand Chuan Wu. Hybridflow: A flexible and efficient RLHF framework. CoRR , abs/2409.19256, 2024.\\n[125] Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Yuhang Wang, Jinlin Xiao, and Jitao Sang. Openrft: Adapting\\nreasoning foundation model for domain-specific tasks with reinforcement fine-tuning. CoRR , abs/2412.16849,\\n2024.\\n[126] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training\\nsoftware engineering agents and verifiers with swe-gym. CoRR , abs/2412.21139, 2024.\\n[127] Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. React meets actre: Autonomous\\nannotations of agent trajectories for contrastive self-training. arXiv preprint arXiv:2403.14589 , 2024.\\n[128] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. In Advances\\nin Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems\\n2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 , 2023.\\n[129] Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan\\nKim, Dongha Lee, and Jinyoung Yeo. Web agents with world models: Learning and leveraging environment\\ndynamics in web navigation. In The Thirteenth International Conference on Learning Representations , 2024.\\n[130] Kewei Cheng, Jingfeng Yang, Haoming Jiang, Zhengyang Wang, Binxuan Huang, Ruirui Li, Shiyang Li, Zheng\\nLi, Yifan Gao, Xian Li, et al. Inductive or deductive? rethinking the fundamental reasoning abilities of llms.\\narXiv preprint arXiv:2408.00114 , 2024.\\n[131] Brett K Hayes, Evan Heit, and Haruka Swendsen. Inductive reasoning. Wiley interdisciplinary reviews:\\nCognitive science , 1(2):278–292, 2010.\\n[132] Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, and Yuyu Luo. Atom of thoughts for\\nmarkov llm test-time scaling, 2025. URL https://arxiv.org/abs/2502.12018 .\\n[133] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan\\nLarson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models outcompete special-purpose tuning?\\ncase study in medicine. arXiv preprint arXiv:2311.16452 , 2023.\\n[134] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise\\nranking and generative fusion. In Annual Meeting of the Association for Computational Linguistics , 2023. URL\\nhttps://api.semanticscholar.org/CorpusID:259075564 .\\n[135] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi\\nFaltings. Refiner: Reasoning feedback on intermediate representations. In Proceedings of the 18th Conference\\nof the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\\n1100–1126, 2024.\\n[136] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language\\nmodels are zero-shot reasoners. Advances in neural information processing systems , 35:22199–22213, 2022.\\n[137] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large\\nlanguage models. In The Eleventh International Conference on Learning Representations , 2023.\\n[138] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire\\nCui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex reasoning in large language\\nmodels. In The Eleventh International Conference on Learning Representations , 2023.\\n[139] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step\\nreasoning. In The Eleventh International Conference on Learning Representations , 2023.\\n[140] Yihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. Rephrase and respond: Let large language\\nmodels ask better questions for themselves. CoRR , abs/2311.04205, 2023.\\n198\\n[141] Ruixin Hong, Hongming Zhang, Xiaoman Pan, Dong Yu, and Changshui Zhang. Abstraction-of-thought makes\\nlanguage models better reasoners. arXiv preprint arXiv:2406.12442 , 2024.\\n[142] Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Ruoxi Jia, and Ming Jin. Algorithm of thoughts: Enhancing\\nexploration of ideas in large language models. arXiv preprint arXiv:2308.10379 , 2023.\\n[143] Tianhe Lin, Jian Xie, Siyu Yuan, and Deqing Yang. Implicit reasoning in transformers is reasoning through\\nshortcuts. arXiv preprint arXiv:2503.07604 , 2025.\\n[144] Allen Newell, John Calman Shaw, and Herbert A Simon. Elements of a theory of human problem solving.\\nPsychological review , 65(3):151, 1958.\\n[145] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang,\\nand Enhong Chen. Understanding the planning of llm agents: A survey. arXiv preprint arXiv:2402.02716 ,\\n2024.\\n[146] Haoming Li, Zhaoliang Chen, Jonathan Zhang, and Fei Liu. Lasp: Surveying the state-of-the-art in large\\nlanguage model-assisted ai planning. arXiv preprint arXiv:2409.01806 , 2024.\\n[147] Subbarao Kambhampati. Can large language models reason and plan? Annals of the New York Academy of\\nSciences , 1534(1):15–18, 2024.\\n[148] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning\\nabilities of large language models-a critical investigation. Advances in Neural Information Processing Systems ,\\n36:75993–76005, 2023.\\n[149] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Biplav Srivastava, Lior Horesh,\\nFrancesco Fabiano, and Andrea Loreggia. Understanding the capabilities of large language models for\\nautomated planning. arXiv preprint arXiv:2305.16151 , 2023.\\n[150] Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas\\nSaldyt, and Anil Murthy. Llms can’t plan, but can help planning in llm-modulo frameworks. arXiv preprint\\narXiv:2402.01817 , 2024.\\n[151] Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar\\nKhot. Adapt: As-needed decomposition and planning with language models. In Findings of the Association for\\nComputational Linguistics: NAACL 2024 , pages 4226–4252, 2024.\\n[152] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving\\nai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems , 36,\\n2024.\\n[153] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng\\nGao. Chameleon: Plug-and-play compositional reasoning with large language models. In Advances in Neural\\nInformation Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023,\\nNeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 , 2023.\\n[154] Fangru Lin, Emanuele La Malfa, Valentin Hofmann, Elle Michelle Yang, Anthony Cohn, and Janet B Pierrehum-\\nbert. Graph-enhanced large language models in asynchronous plan reasoning. arXiv preprint arXiv:2402.02805 ,\\n2024.\\n[155] Amrith Setlur, Nived Rajaraman, Sergey Levine, and Aviral Kumar. Scaling test-time compute without\\nverification or rl is suboptimal. arXiv preprint arXiv:2502.12118 , 2025.\\n[156] Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya\\nSamavedhi, Qiyue Gao, et al. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning\\nwith large language models. In First Conference on Language Modeling , 2024.\\n[157] Jinghan Zhang and Kunpeng Liu. Thought space explorer: Navigating and expanding thought space for large\\nlanguage model reasoning. In 2024 IEEE International Conference on Big Data (BigData) , pages 8259–8251.\\nIEEE, 2024.\\n[158] Siheng Xiong, Ali Payani, Ramana Kompella, and Faramarz Fekri. Large language models can learn temporal\\nreasoning. CoRR , 2024.\\n[159] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree\\nsearch unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406 , 2023.\\n[160] Owen Burns, Dana Hughes, and Katia Sycara. Plancritic: Formal planning with human feedback. arXiv\\npreprint arXiv:2412.00300 , 2024.\\n199\\n[161] Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale\\ntask planning. Advances in Neural Information Processing Systems , 36, 2024.\\n[162] Zhiting Hu and Tianmin Shu. Language models, agent models, and world models: The law for machine\\nreasoning and planning. arXiv preprint arXiv:2312.05230 , 2023.\\n[163] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p:\\nEmpowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477 , 2023.\\n[164] Sadegh Mahdavi, Raquel Aoki, Keyi Tang, and Yanshuai Cao. Leveraging environment interaction for\\nautomated PDDL translation and planning with large language models. In NeurIPS , 2024.\\n[165] Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging pre-trained large\\nlanguage models to construct and utilize world models for model-based task planning. In Thirty-seventh\\nConference on Neural Information Processing Systems , 2023.\\n[166] Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri,\\nLucas Paul Saldyt, and Anil B Murthy. Position: Llms can’t plan, but can help planning in llm-modulo\\nframeworks. In Forty-first International Conference on Machine Learning , 2024.\\n[167] Jiaxin Wen, Jian Guan, Hongning Wang, Wei Wu, and Minlie Huang. Unlocking reasoning potential in large\\nlangauge models by scaling code-form planning. arXiv preprint arXiv:2409.12452 , 2024.\\n[168] Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie,\\nFei Huang, and Huajun Chen. Agent planning with world knowledge model. Advances in Neural Information\\nProcessing Systems , 37:114843–114871, 2024.\\n[169] Jun Wang, Jiaming Tong, Kaiyuan Tan, Yevgeniy V orobeychik, and Yiannis Kantaros. Conformal temporal\\nlogic planning using large language models. arXiv preprint arXiv:2309.10092 , 2023.\\n[170] Richard C Atkinson. Human memory: A proposed system and its control processes. The psychology of learning\\nand motivation , 2, 1968.\\n[171] Kieran CR Fox, Nicholas S Fitz, and Peter B Reiner. The multiplicity of memory enhancement: Practical\\nand ethical implications of the diverse neural substrates underlying human memory systems. Neuroethics , 10:\\n375–388, 2017.\\n[172] Alan Baddeley. Working memory. Science , 255(5044):556–559, 1992.\\n[173] George Sperling. The information available in brief visual presentations. Psychological monographs: General\\nand applied , 74(11):1, 1960.\\n[174] Max Coltheart. Iconic memory and visible persistence. Perception & psychophysics , 27:183–228, 1980.\\n[175] JM Gardiner. On recency and echoic memory. Philosophical Transactions of the Royal Society of London. B,\\nBiological Sciences , 302(1110):267–282, 1983.\\n[176] Bart Aben, Sven Stapert, and Arjan Blokland. About the distinction between working memory and short-term\\nmemory. Frontiers in psychology , 3:301, 2012.\\n[177] Nelson Cowan. What are the differences between long-term, short-term, and working memory? Progress in\\nbrain research , 169:323–338, 2008.\\n[178] Richard M Shiffrin and Richard C Atkinson. Storage and retrieval processes in long-term memory. Psychological\\nreview , 76(2):179, 1969.\\n[179] Dennis Norris. Short-term memory and long-term memory are still different. Psychological bulletin , 143(9):\\n992, 2017.\\n[180] Hermann Ebbinghaus. Memory: A contribution to experimental psychology. Annals of neurosciences , 20(4):\\n155, 2013.\\n[181] Howard Eichenbaum. Declarative memory: Insights from cognitive neurobiology. Annual review of psychology ,\\n48(1):547–572, 1997.\\n[182] Abhilasha A Kumar. Semantic memory: A review of methods, models, and current challenges. Psychonomic\\nbulletin & review , 28(1):40–80, 2021.\\n[183] Endel Tulving. Episodic memory: From mind to brain. Annual review of psychology , 53(1):1–25, 2002.\\n[184] Robyn Fivush. The development of autobiographical memory. Annual review of psychology , 62(1):559–582,\\n2011.\\n[185] Larry R Squire. Declarative and nondeclarative memory: Multiple brain systems supporting learning and\\nmemory. Journal of cognitive neuroscience , 4(3):232–243, 1992.\\n200\\n[186] Prahlad Gupta and Neal J Cohen. Theoretical and computational analysis of skill learning, repetition priming,\\nand procedural memory. Psychological review , 109(2):401, 2002.\\n[187] Neal J Cohen and Larry R Squire. Preserved learning and retention of pattern-analyzing skill in amnesia:\\nDissociation of knowing how and knowing that. Science , 210(4466):207–210, 1980.\\n[188] Endel Tulving and Daniel L Schacter. Priming and human memory systems. Science , 247(4940):301–306,\\n1990.\\n[189] Robert E Clark, Joseph R Manns, and Larry R Squire. Classical conditioning, awareness, and brain systems.\\nTrends in cognitive sciences , 6(12):524–531, 2002.\\n[190] Androulla Ioannou and Xenia Anastassiou-Hadjicharalambous. Non-associative learning. Encyclopedia of\\nevolutionary psychological science , pages 5419–5432, 2021.\\n[191] Martin A Conway and Christopher W Pleydell-Pearce. The construction of autobiographical memories in the\\nself-memory system. Psychological review , 107(2):261, 2000.\\n[192] Alan D Baddeley, Graham Hitch, and Gordon H Bower. Working memory. volume 8 of. Psychology of Learning\\nand Motivation , pages 47–89, 1974.\\n[193] Alan Baddeley. The episodic buffer: a new component of working memory? Trends in cognitive sciences , 4\\n(11):417–423, 2000.\\n[194] Nelson Cowan. Evolving conceptions of memory storage, selective attention, and their mutual constraints\\nwithin the human information-processing system. Psychological bulletin , 104(2):163, 1988.\\n[195] Endel Tulving. Memory and consciousness. Canadian Psychology/Psychologie canadienne , 26(1):1, 1985.\\n[196] Bernard J Baars. A cognitive theory of consciousness . Cambridge University Press, 1993.\\n[197] Stan Franklin. Artificial minds . MIT press, 1997.\\n[198] Stan Franklin, Arpad Kelemen, and Lee McCauley. Ida: A cognitive agent architecture. In SMC’98 Conference\\nProceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No. 98CH36218) ,\\nvolume 3, pages 2646–2651. IEEE, 1998.\\n[199] John R Anderson. How can the human mind occur in the physical universe? Oxford University Press, 2009.\\n[200] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai\\nWang, Feng Yin, Junhua Zhao, et al. Exploring large language model based intelligent agents: Definitions,\\nmethods, and prospects. arXiv preprint arXiv:2401.03428 , 2024.\\n[201] Alan Baddeley. Working memory. Current biology , 20(4):R136–R140, 2010.\\n[202] Jose Camacho-Collados and Mohammad Taher Pilehvar. From word to sense embeddings: A survey on vector\\nrepresentations of meaning. Journal of Artificial Intelligence Research , 63:743–788, 2018.\\n[203] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. Think-in-\\nmemory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint arXiv:2311.08719 ,\\n2023.\\n[204] Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. In Findings\\nof the Association for Computational Linguistics ACL 2024 , pages 3132–3149, 2024.\\n[205] Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua\\nSong, Wayne Xin Zhao, et al. User behavior simulation with large language model based agents. arXiv preprint\\narXiv:2306.02552 , 2023.\\n[206] Yujia Zhou, Qiannan Zhu, Jiajie Jin, and Zhicheng Dou. Cognitive personalized search integrating large\\nlanguage models with an efficient memory mechanism. In Proceedings of the ACM on Web Conference 2024 ,\\npages 1464–1473, 2024.\\n[207] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language\\nmodels with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38,\\npages 19724–19731, 2024.\\n[208] Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen MacNeil. Memory sandbox: Transparent\\nand interactive memory management for conversational agents. In Adjunct Proceedings of the 36th Annual\\nACM Symposium on User Interface Software and Technology , pages 1–3, 2023.\\n[209] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: A memory-\\naugmented multimodal agent for video understanding. In European Conference on Computer Vision , pages\\n75–92. Springer, 2024.\\n201\\n[210] Zhiqi Ge, Hongzhe Huang, Mingze Zhou, Juncheng Li, Guoming Wang, Siliang Tang, and Yueting Zhuang.\\nWorldgpt: Empowering LLM as multimodal world model. In Proceedings of the 32nd ACM International\\nConference on Multimedia , pages 7346–7355, 2024.\\n[211] Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic\\nframework that uses computers like a human. arXiv preprint arXiv:2410.08164 , 2024.\\n[212] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and\\nLingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint\\narXiv:2402.07456 , 2024.\\n[213] Sen Li, Ruochen Wang, Cho-Jui Hsieh, Minhao Cheng, and Tianyi Zhou. Mulan: Multimodal-llm agent for\\nprogressive multi-object diffusion. arXiv preprint arXiv:2402.12741 , 2024.\\n[214] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G Patil, Ion Stoica, and Joseph E Gonzalez.\\nMemgpt: Towards LLMs as operating systems. arXiv preprint arXiv:2310.08560 , 2023.\\n[215] Zixuan Wang, Bo Yu, Junzhe Zhao, Wenhao Sun, Sai Hou, Shuai Liang, Xing Hu, Yinhe Han, and Yiming\\nGan. Karma: Augmenting embodied ai agents with long-and-short term memory systems. arXiv preprint\\narXiv:2409.14908 , 2024.\\n[216] Zeru Shi, Kai Mei, Mingyu Jin, Yongye Su, Chaoji Zuo, Wenyue Hua, Wujiang Xu, Yujie Ren, Zirui Liu,\\nMengnan Du, et al. From commands to prompts: Llm-based semantic file system for aios. arXiv preprint\\narXiv:2410.11843 , 2024.\\n[217] Xiaoqiang Wang and Bang Liu. Oscar: Operating system control via state-aware reasoning and re-planning.\\narXiv preprint arXiv:2410.18963 , 2024.\\n[218] Kevin A Fischer. Reflective linguistic programming (rlp): A stepping stone in socially-aware agi (socialagi).\\narXiv preprint arXiv:2305.12647 , 2023.\\n[219] Andrew Zhu, Lara Martin, Andrew Head, and Chris Callison-Burch. Calypso: LLMs as dungeon master’s assis-\\ntants. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment ,\\nvolume 19, pages 380–390, 2023.\\n[220] Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, and Ping Luo. Hiagent: Hierarchical\\nworking memory management for solving long-horizon agent tasks with large language model. arXiv preprint\\narXiv:2408.09559 , 2024.\\n[221] Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, and Evgeny Burnaev.\\nArigraph: Learning knowledge graph world models with episodic memory for LLM agents. arXiv preprint\\narXiv:2407.04363 , 2024.\\n[222] Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically\\ninspired long-term memory for large language models. In NeurIPS , 2024.\\n[223] Sunjae Lee, Junyoung Choi, Jungjae Lee, Munim Hasan Wasi, Hojun Choi, Steven Y Ko, Sangeun Oh, and\\nInsik Shin. Explore, select, derive, and recall: Augmenting llm with human-like memory for mobile task\\nautomation. arXiv preprint arXiv:2312.03003 , 2023.\\n[224] Leonard Bärmann, Chad DeChant, Joana Plewnia, Fabian Peller-Konrad, Daniel Bauer, Tamim Asfour, and\\nAlex Waibel. Episodic memory verbalization using hierarchical representations of life-long robot experience.\\narXiv preprint arXiv:2409.17702 , 2024.\\n[225] Junyeong Park, Junmo Cho, and Sungjin Ahn. Mr. steve: Instruction-following agents in minecraft with\\nwhat-where-when memory. arXiv preprint arXiv:2411.06736 , 2024.\\n[226] K Roth, Rushil Gupta, Simon Halle, and Bang Liu. Pairing analogy-augmented generation with procedural\\nmemory for procedural q&a. arXiv preprint arXiv:2409.01344 , 2024.\\n[227] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang,\\nLongtao Zheng, Xinrun Xu, et al. Towards general computer control: A multimodal agent for red dead\\nredemption ii as a case study. In ICLR 2024 Workshop on Large Language Model (LLM) Agents , 2024.\\n[228] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng\\nHe, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with memory-augmented\\nmultimodal language models. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2024.\\n[229] Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, and Ji Yan. Larp: Language-agent role play for\\nopen-world games. arXiv preprint arXiv:2312.17653 , 2023.\\n202\\n[230] Yijun Liu, Wu Liu, Xiaoyan Gu, Yong Rui, Xiaodong He, and Yongdong Zhang. Lmagent: A large-scale\\nmultimodal agents society for multi-user simulation. arXiv preprint arXiv:2412.09237 , 2024.\\n[231] Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer. A human-inspired reading agent\\nwith gist memory of very long contexts. arXiv preprint arXiv:2402.09727 , 2024.\\n[232] Shuai Wang, Liang Ding, Yibing Zhan, Yong Luo, Zheng He, and Dapeng Tao. Leveraging metamemory\\nmechanisms for enhanced data-free code generation in llms. arXiv preprint arXiv:2501.07892 , 2025.\\n[233] Pengbo Hu and Xiang Ying. Unified mind model: Reimagining autonomous agents in the llm era. arXiv\\npreprint arXiv:2503.03459 , 2025.\\n[234] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks:\\nA survey. IEEE transactions on pattern analysis and machine intelligence , 44(9):5149–5169, 2021.\\n[235] Yuki Hou, Haruki Tamoto, and Homei Miyashita. “my agent understands me better”: Integrating dynamic\\nhuman-like memory recall and consolidation in llm-based agents. In Extended Abstracts of the CHI Conference\\non Human Factors in Computing Systems , pages 1–7, 2024.\\n[236] Bo Pan, Jiaying Lu, Ke Wang, Li Zheng, Zhen Wen, Yingchaojie Feng, Minfeng Zhu, and Wei Chen. Agent-\\ncoord: Visually exploring coordination strategy for llm-based multi-agent collaboration. arXiv preprint\\narXiv:2404.11943 , 2024.\\n[237] Hang Gao and Yongfeng Zhang. Memory sharing for large language model based agents. arXiv preprint\\narXiv:2404.09982 , 2024.\\n[238] Meng Chu, Yicong Li, and Tat-Seng Chua. Understanding long videos via llm-powered entity relation graphs.\\narXiv preprint arXiv:2501.15953 , 2025.\\n[239] Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. A-mem: Agentic memory for\\nllm agents. arXiv preprint arXiv:2502.12110 , 2025.\\n[240] Hassan Ali, Philipp Allgeuer, Carlo Mazzola, Giulia Belgiovine, Burak Can Kaplan, Lukáš Gajdošech, and\\nStefan Wermter. Robots can multitask too: Integrating a memory architecture and llms for enhanced cross-task\\nrobot action generation. In 2024 IEEE-RAS 23rd International Conference on Humanoid Robots (Humanoids) ,\\npages 811–818. IEEE, 2024.\\n[241] Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. Optimus-1: Hybrid\\nmultimodal memory empowered agents excel in long-horizon tasks. arXiv preprint arXiv:2408.03615 , 2024.\\n[242] Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. Optimus-2: Multimodal\\nminecraft agent with goal-observation-action conditioned policy. arXiv preprint arXiv:2502.19902 , 2025.\\n[243] Tenghao Huang, Kinjal Basu, Ibrahim Abdelaziz, Pavan Kapanipathi, Jonathan May, and Muhao Chen. R2d2:\\nRemembering, reflecting and dynamic decision making for web agents. arXiv preprint arXiv:2501.12485 ,\\n2025.\\n[244] Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji.\\nMobile-agent-e: Self-evolving mobile assistant for complex tasks. arXiv preprint arXiv:2501.11733 , 2025.\\n[245] Philippe Laban, Wojciech Kry ´sci´nski, Divyansh Agarwal, Alexander Richard Fabbri, Caiming Xiong, Shafiq\\nJoty, and Chien-Sheng Wu. Summedits: Measuring llm ability at factual reasoning through the lens of\\nsummarization. In Proceedings of the 2023 conference on empirical methods in natural language processing ,\\npages 9662–9676, 2023.\\n[246] Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun\\nLi. Enhancing large language model with self-controlled memory framework. arXiv preprint arXiv:2304.13343 ,\\n2023.\\n[247] Zhiyao Ren, Yibing Zhan, Baosheng Yu, Liang Ding, and Dacheng Tao. Healthcare copilot: Eliciting the power\\nof general llms for medical consultation. arXiv preprint arXiv:2402.13408 , 2024.\\n[248] Qingyue Wang, Liang Ding, Yanan Cao, Zhiliang Tian, Shi Wang, Dacheng Tao, and Li Guo. Recursively\\nsummarizing enables long-term dialogue memory in large language models. arXiv preprint arXiv:2308.15022 ,\\n2023.\\n[249] Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Ningyu Zhang, Shiwei Lyu, Yue Shen, Lei Liang, Jinjie\\nGu, and Huajun Chen. Knowagent: Knowledge-augmented planning for LLM-based agents. arXiv preprint\\narXiv:2403.03101 , 2024.\\n[250] Yudi Shi, Shangzhe Di, Qirui Chen, and Weidi Xie. Unlocking video-llm via agent-of-thoughts distillation.\\narXiv preprint arXiv:2412.01694 , 2024.\\n203\\n[251] Jiaqi Liu, Chengkai Xu, Peng Hang, Jian Sun, Mingyu Ding, Wei Zhan, and Masayoshi Tomizuka. Language-\\ndriven policy distillation for cooperative driving in multi-agent reinforcement learning. arXiv preprint\\narXiv:2410.24152 , 2024.\\n[252] Maryam Hashemzadeh, Elias Stengel-Eskin, Sarath Chandar, and Marc-Alexandre Cote. Sub-goal distillation:\\nA method to improve small language agents. arXiv preprint arXiv:2405.02749 , 2024.\\n[253] Justin Chih-Yao Chen, Swarnadeep Saha, Elias Stengel-Eskin, and Mohit Bansal. Magdi: structured distillation\\nof multi-agent interaction graphs improves reasoning in smaller language models. In Proceedings of the 41st\\nInternational Conference on Machine Learning , pages 7220–7235, 2024.\\n[254] Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo, Guangyu Robert\\nYang, and Andrew Ahn. Lyfe agents: Generative agents for low-cost real-time social interactions. arXiv\\npreprint arXiv:2310.02172 , 2023.\\n[255] Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, and Yong\\nLi. S3: Social-network simulation system with large language model-empowered agents. arXiv preprint\\narXiv:2307.14984 , 2023.\\n[256] Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, and Khaldoun Khashanah. Tradinggpt: Multi-agent system\\nwith layered memory and distinct characters for enhanced financial trading performance. arXiv preprint\\narXiv:2309.03736 , 2023.\\n[257] Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. Longmemeval: Benchmark-\\ning chat assistants on long-term interactive memory. arXiv preprint arXiv:2410.10813 , 2024.\\n[258] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew\\nLin, H Vicky Zhao, Lili Qiu, et al. On memory construction and retrieval for personalized conversational agents.\\narXiv preprint arXiv:2502.05589 , 2025.\\n[259] Guillaume Lample, Alexandre Sablayrolles, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. Large\\nmemory layers with product keys. Advances in Neural Information Processing Systems , 32, 2019.\\n[260] Jiaming Xu, Kaibin Guo, Wuxuan Gong, and Runyu Shi. Osagent: Copiloting operating system with llm-based\\nagent. In 2024 International Joint Conference on Neural Networks (IJCNN) , pages 1–9. IEEE, 2024.\\n[261] Dzmitry Bahdanau. Neural machine translation by jointly learning to align and translate. arXiv preprint\\narXiv:1409.0473 , 2014.\\n[262] Mete Demircigil, Judith Heusel, Matthias Löwe, Sven Upgang, and Franck Vermet. On a model of associative\\nmemory with huge storage capacity. Journal of Statistical Physics , 168:288–299, 2017.\\n[263] Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas\\nGruber, Markus Holzleitner, Milena Pavlovi ´c, Geir Kjetil Sandve, et al. Hopfield networks is all you need.\\narXiv preprint arXiv:2008.02217 , 2020.\\n[264] Alex Falcon, Giovanni D’Agostino, Oswald Lanz, Giorgio Brajnik, Carlo Tasso, and Giuseppe Serra. Neural\\nturing machines for the remaining useful life estimation problem. Computers in Industry , 143:103762, 2022.\\n[265] Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li,\\nBing Yin, Jingbo Shang, and Julian McAuley. Memoryllm: towards self-updatable large language models. In\\nProceedings of the 41st International Conference on Machine Learning , ICML’24. JMLR.org, 2024.\\n[266] Yu Wang, Xinshuang Liu, Xiusi Chen, Sean O’Brien, Junda Wu, and Julian McAuley. Self-updatable large\\nlanguage models with parameter integration. arXiv preprint arXiv:2410.00487 , 2024.\\n[267] Hongjin Qian, Peitian Zhang, Zheng Liu, Kelong Mao, and Zhicheng Dou. Memorag: Moving towards next-gen\\nrag via memory-inspired knowledge discovery. arXiv preprint arXiv:2409.05591 , 2024.\\n[268] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen,\\nXiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv\\npreprint arXiv:2407.04620 , 2024.\\n[269] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint\\narXiv:2501.00663 , 2024.\\n[270] Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, and Bang Liu. R3mem: Bridging memory retention and retrieval\\nvia reversible compression. arXiv preprint arXiv:2502.15957 , 2025.\\n[271] Xuanwang Zhang, Yunze Song, Yidong Wang, Shuyun Tang, Xinfeng Li, Zhengran Zeng, Zhen Wu, Wei Ye,\\nWenyuan Xu, Yue Zhang, et al. Raglab: A modular and research-oriented unified framework for retrieval-\\naugmented generation. arXiv preprint arXiv:2408.11381 , 2024.\\n204\\n[272] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to\\ntrust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings\\nof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\\n9802–9822, 2023.\\n[273] Mehrdad Farahani and Richard Johansson. Deciphering the interplay of parametric and non-parametric memory\\nin retrieval-augmented language models. arXiv preprint arXiv:2410.05162 , 2024.\\n[274] Ruifeng Yuan, Shichao Sun, Yongqi Li, Zili Wang, Ziqiang Cao, and Wenjie Li. Personalized large language\\nmodel assistant with evolving conditional memory. In Proceedings of the 31st International Conference on\\nComputational Linguistics , pages 3764–3777, 2025.\\n[275] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. Advances in Neural\\nInformation Processing Systems , 35:11079–11091, 2022.\\n[276] Aydar Bulatov, Yuri Kuratov, Yermek Kapushev, and Mikhail S Burtsev. Scaling transformer to 1m tokens and\\nbeyond with rmt. arXiv preprint arXiv:2304.11062 , 2023.\\n[277] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress\\ncontexts. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages\\n3829–3846, 2023.\\n[278] Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context\\ncompression in a large language model. arXiv preprint arXiv:2307.06945 , 2023.\\n[279] Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens. Advances in Neural\\nInformation Processing Systems , 36, 2024.\\n[280] Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang. Compact: Compressing\\nretrieved documents actively for question answering. In Proceedings of the 2024 Conference on Empirical\\nMethods in Natural Language Processing , pages 21424–21439, 2024.\\n[281] Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui, Yuxin Ye, Nithyashree Manohar,\\nZhuxiaona Wei, Tian Wu, Ben Echols, et al. Banishing llm hallucinations requires rethinking generalization.\\narXiv preprint arXiv:2406.17642 , 2024.\\n[282] Sangjun Park and JinYeong Bak. Memoria: Resolving fateful forgetting problem through human-inspired\\nmemory architecture. arXiv preprint arXiv:2310.03052 , 2023.\\n[283] Xu Owen He. Mixture of a million experts. arXiv preprint arXiv:2407.04153 , 2024.\\n[284] Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi Cheng. Retrieve only when it needs: Adaptive\\nretrieval augmentation for hallucination mitigation in large language models. arXiv preprint arXiv:2402.10612 ,\\n2024.\\n[285] Yingxu Wang, Dong Liu, and Ying Wang. Discovering the capacity of human memory. Brain and Mind , 4:\\n189–198, 2003.\\n[286] Jikun Kang, Romain Laroche, Xindi Yuan, Adam Trischler, Xue Liu, and Jie Fu. Think before you act: Decision\\ntransformers with internal working memory. arXiv preprint arXiv:2305.16338 , 2023.\\n[287] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\\nYu, Weizhu Chen, et al. Check your facts and try again: Improving large language models with external\\nknowledge and automated feedback. arXiv preprint arXiv:2302.12813 , 2023.\\n[288] Taewoon Kim, Michael Cochez, Vincent François-Lavet, Mark Neerincx, and Piek V ossen. A machine with\\nshort-term, episodic, and semantic memory systems. In Proceedings of the AAAI Conference on Artificial\\nIntelligence , volume 37, pages 48–56, 2023.\\n[289] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan\\nChen, Jianguo Zhang, Devansh Arpit, et al. Retroformer: Retrospective large language agents with policy\\ngradient optimization. arXiv preprint arXiv:2308.02151 , 2023.\\n[290] Siyuan Wang, Zhongyu Wei, Yejin Choi, and Xiang Ren. Symbolic working memory enhances language\\nmodels for complex rule application. In Proceedings of the 2024 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 17583–17604, 2024.\\n[291] Longtao Zheng, Rundong Wang, and Bo An. Synapse: Leveraging few-shot exemplars for human-level\\ncomputer control. arXiv preprint arXiv:2306.07863 , 2023.\\n[292] Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian D Reid, and Niko Suenderhauf. Sayplan:\\nGrounding large language models using 3d scene graphs for scalable task planning. CoRR , 2023.\\n205\\n[293] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. Llm-planner:\\nFew-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision , pages 2998–3009, 2023.\\n[294] Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, and Honglak\\nLee. Autoguide: Automated generation and selection of state-aware guidelines for large language model agents.\\narXiv preprint arXiv:2403.08978 , 2024.\\n[295] Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting\\nZhuang, and Weiming Lu. Agent-pro: Learning to evolve via policy-level reflection and optimization. arXiv\\npreprint arXiv:2402.17574 , 2024.\\n[296] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is a generalist web agent, if\\ngrounded. In ICML , 2024.\\n[297] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang,\\nXiaohan Zhang, Yuxiao Dong, et al. Autowebglm: A large language model-based web navigating agent. In\\nKDD , 2024.\\n[298] Paloma Sodhi, SRK Branavan, and Ryan McDonald. Heap: Hierarchical policies for web actions using LLMs.\\narXiv preprint arXiv:2310.03720 , 2023.\\n[299] Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. arXiv preprint\\narXiv:2409.07429 , 2024.\\n[300] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P Xing, and\\nZhiting Hu. PromptAgent: Strategic planning with language models enables expert-level prompt optimization.\\narXiv preprint arXiv:2310.16427 , 2023.\\n[301] Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Zihao Xie, Yifei Wang, Weize Chen, Cheng Yang, Xin Cong,\\nXiaoyin Che, et al. Experiential co-learning of software-developing agents. arXiv preprint arXiv:2312.17025 ,\\n2023.\\n[302] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu,\\nBaoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871 ,\\n2023.\\n[303] Chen Qian, Jiahao Li, Yufan Dang, Wei Liu, YiFei Wang, Zihao Xie, Weize Chen, Cheng Yang, Yingli\\nZhang, Zhiyuan Liu, et al. Iterative experience refinement of software-developing agents. arXiv preprint\\narXiv:2405.04219 , 2024.\\n[304] Shreyas Basavatia, Keerthiram Murugesan, and Shivam Ratnakar. Starling: Self-supervised training of text-\\nbased reinforcement learning agent with large language models. arXiv preprint arXiv:2406.05872 , 2024.\\n[305] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that\\nlearn and think like people. Behavioral and brain sciences , 40:e253, 2017.\\n[306] Anirudh Goyal, Riashat Islam, Daniel Strouse, Zafarali Ahmed, Matthew Botvinick, Hugo Larochelle, Yoshua\\nBengio, and Sergey Levine. Infobot: Transfer and exploration via the information bottleneck. arXiv preprint\\narXiv:1901.10902 , 2019.\\n[307] Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang\\nLi, Wanli Ouyang, et al. Graphreader: Building graph-based agent to enhance long-context abilities of\\nlarge language models. In Findings of the Association for Computational Linguistics: EMNLP 2024 , pages\\n12758–12786, 2024.\\n[308] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex\\nGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep\\nreinforcement learning. nature , 518(7540):529–533, 2015.\\n[309] Grace W Lindsay. Attention in psychology, neuroscience, and machine learning. Frontiers in computational\\nneuroscience , 14:29, 2020.\\n[310] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. Attention bottlenecks\\nfor multimodal fusion. Advances in neural information processing systems , 34:14200–14213, 2021.\\n[311] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An\\nHuang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-\\nscale knowledge. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural\\nInformation Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,\\n2022 , 2022.\\n206\\n[312] Yuheng Cheng, Huan Zhao, Xiyuan Zhou, Junhua Zhao, Yuji Cao, Chao Yang, and Xinlei Cai. A large language\\nmodel for advanced power dispatch. Scientific Reports , 15(1):8925, 2025.\\n[313] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint\\narXiv:1503.02531 , 2015.\\n[314] Larry R Squire, Lisa Genzel, John T Wixted, and Richard G Morris. Memory consolidation. Cold Spring\\nHarbor perspectives in biology , 7(8):a021766, 2015.\\n[315] Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen, Guolong Liu, Gaoqi Liang, Junhua Zhao, Jinyue\\nYan, and Yun Li. Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and\\nmethods. IEEE Transactions on Neural Networks and Learning Systems , 2024.\\n[316] N Reimers. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 ,\\n2019.\\n[317] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv\\npreprint arXiv:1609.02907 , 2016.\\n[318] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE Transactions\\non Big Data , 7(3):535–547, 2019.\\n[319] Wen Li, Ying Zhang, Yifang Sun, Wei Wang, Mingjie Li, Wenjie Zhang, and Xuemin Lin. Approximate nearest\\nneighbor search on high dimensional data—experiments, analyses, and improvement. IEEE Transactions on\\nKnowledge and Data Engineering , 32(8):1475–1488, 2019.\\n[320] Peiyan Zhang, Chaozhuo Li, Liying Kang, Feiran Huang, Senzhang Wang, Xing Xie, and Sunghun Kim.\\nHigh-frequency-aware hierarchical contrastive selective coding for representation learning on text attributed\\ngraphs. In Proceedings of the ACM Web Conference 2024 , pages 4316–4327, 2024.\\n[321] Zhenyi Wang, Enneng Yang, Li Shen, and Heng Huang. A comprehensive survey of forgetting in deep learning\\nbeyond continual learning. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2024.\\n[322] Bart Kosko. Bidirectional associative memories. IEEE Transactions on Systems, man, and Cybernetics , 18(1):\\n49–60, 1988.\\n[323] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning\\nwith memory-augmented neural networks. In International conference on machine learning , pages 1842–1850.\\nPMLR, 2016.\\n[324] Zihang Dai. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint\\narXiv:1901.02860 , 2019.\\n[325] Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. Transformer-patcher:\\nOne mistake worth one neuron. arXiv preprint arXiv:2301.09785 , 2023.\\n[326] Govind Krishnan Gangadhar and Karl Stratos. Model editing by standard fine-tuning. In Findings of the\\nAssociation for Computational Linguistics ACL 2024 , pages 5907–5913, 2024.\\n[327] Peiyan Zhang, Yuchen Yan, Chaozhuo Li, Senzhang Wang, Xing Xie, Guojie Song, and Sunghun Kim.\\nContinual learning on dynamic graphs via parameter isolation. In Proceedings of the 46th International ACM\\nSIGIR Conference on Research and Development in Information Retrieval , pages 601–611, 2023.\\n[328] Yu Wang, Ruihan Wu, Zexue He, Xiusi Chen, and Julian McAuley. Large scale knowledge washing. arXiv\\npreprint arXiv:2405.16720 , 2024.\\n[329] Wuyang Chen, Yanqi Zhou, Nan Du, Yanping Huang, James Laudon, Zhifeng Chen, and Claire Cui. Lifelong\\nlanguage pretraining with distribution-specialized experts. In International Conference on Machine Learning ,\\npages 5383–5395. PMLR, 2023.\\n[330] Yinpeng Chen, DeLesley Hutchins, Aren Jansen, Andrey Zhmoginov, David Racz, and Jesper Andersen.\\nMelodi: Exploring memory compression for long contexts. arXiv preprint arXiv:2410.03156 , 2024.\\n[331] Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, and Jonathan Richard Schwarz. Online\\nadaptation of language models with a memory of amortized contexts. In The Thirty-eighth Annual Conference on\\nNeural Information Processing Systems , 2024. URL https://openreview.net/forum?id=RIfgKCknTu .\\n[332] Yu Wang, Dmitry Krotov, Yuanzhe Hu, Yifan Gao, Wangchunshu Zhou, Julian McAuley, Dan Gutfreund,\\nRogerio Feris, and Zexue He. M+: Extending memoryllm with scalable long-term memory. arXiv preprint\\narXiv:2502.00592 , 2025.\\n207\\n[333] Shankar Padmanabhan, Yasumasa Onoe, Michael Zhang, Greg Durrett, and Eunsol Choi. Propagating knowl-\\nedge updates to lms through distillation. Advances in Neural Information Processing Systems , 36:47124–47142,\\n2023.\\n[334] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\\nKüttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-\\nintensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459–9474, 2020.\\n[335] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint\\narXiv:2004.05150 , 2020.\\n[336] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for\\naccelerated inference of large language models. In Proceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing , pages 13358–13376, 2023.\\n[337] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Longllm-\\nlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. In Proceedings of\\nthe 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\\n1658–1677, 2024.\\n[338] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander\\nMiller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods\\nin Natural Language Processing and the 9th International Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP) , pages 2463–2473, 2019.\\n[339] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer,\\nZac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they\\nknow. arXiv preprint arXiv:2207.05221 , 2022.\\n[340] Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language\\nmodels using semantic entropy. Nature , 630(8017):625–630, 2024.\\n[341] Edward C Tolman. Cognitive maps in rats and men. Psychological review , 55(4):189, 1948.\\n[342] Kenneth James Williams Craik. The nature of explanation , volume 445. CUP Archive, 1967.\\n[343] Dedre Gentner and Albert L Stevens. Mental models . Psychology Press, 2014.\\n[344] Andy Clark. Surfing uncertainty: Prediction, action, and the embodied mind . Oxford University Press, 2015.\\n[345] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin , 2\\n(4):160–163, 1991.\\n[346] Jürgen Schmidhuber. Making the world differentiable: on using self supervised fully recurrent neural networks\\nfor dynamic reinforcement learning and planning in non-stationary environments , volume 126. Inst. für\\nInformatik, 1990.\\n[347] Jürgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers.\\nIn1st International Conference on Simulation of Adaptive Behavior on From Animals to Animats , page 222–227,\\nCambridge, MA, USA, 1991. MIT Press. ISBN 0262631385.\\n[348] David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122 , 2018.\\n[349] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt,\\nArthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering Atari, Go, Chess and Shogi\\nby planning with a learned model. Nature , 588(7839):604–609, 2020.\\n[350] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors\\nby latent imagination. arXiv preprint arXiv:1912.01603 , 2019.\\n[351] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang,\\nYifu Yuan, He Wang, et al. Sapien: A simulated part-based interactive environment. In Proceedings of the\\nIEEE/CVF conference on computer vision and pattern recognition , pages 11097–11107, 2020.\\n[352] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer: World\\nmodels for physical robot learning. In Conference on Robot Learning , pages 2226–2240. PMLR, 2023.\\n[353] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and François Fleuret.\\nDiffusion for world modeling: Visual details matter in atari. arXiv preprint arXiv:2405.12399 , 2024.\\n[354] SM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S Morcos, Marta Garnelo, Avraham\\nRuderman, Andrei A Rusu, Ivo Danihelka, Karol Gregor, et al. Neural scene representation and rendering.\\nScience , 360(6394):1204–1210, 2018.\\n208\\n[355] Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In\\nProceedings of the 28th International Conference on machine learning (ICML-11) , pages 465–472, 2011.\\n[356] Chenxi Liu, Yongqiang Chen, Tongliang Liu, Mingming Gong, James Cheng, Bo Han, and Kun Zhang.\\nDiscovery of the hidden world with large language models. arXiv preprint arXiv:2402.03941 , 2024.\\n[357] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng\\nXu, Yichu Yang, et al. GR-2: A generative video-language-action model with web-scale knowledge for robot\\nmanipulation. arXiv preprint arXiv:2410.06158 , 2024.\\n[358] Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. Dino-wm: World models on pre-trained visual\\nfeatures enable zero-shot planning. arXiv preprint arXiv:2411.04983 , 2024.\\n[359] Haochen Shi, Huazhe Xu, Zhiao Huang, Yunzhu Li, and Jiajun Wu. Robocraft: Learning to see, simulate, and\\nshape elasto-plastic objects in 3d with graph networks. The International Journal of Robotics Research , 43(4):\\n533–549, 2024.\\n[360] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard\\nGhanem. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. Advances in neural\\ninformation processing systems , 35:23192–23204, 2022.\\n[361] Ganlong Zhao, Guanbin Li, Weikai Chen, and Yizhou Yu. Over-nav: Elevating iterative vision-and-language\\nnavigation with open-vocabulary detection and structured representation. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , pages 16296–16306, 2024.\\n[362] Basil Kouvaritakis and Mark Cannon. Model predictive control. Switzerland: Springer International Publishing ,\\n38(13-56):7, 2016.\\n[363] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for\\nmodel-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE international conference\\non robotics and automation (ICRA) , pages 7559–7566. IEEE, 2018.\\n[364] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world\\nmodels. arXiv preprint arXiv:2010.02193 , 2020.\\n[365] Boyan Li, Jiayi Zhang, Ju Fan, Yanwei Xu, Chong Chen, Nan Tang, and Yuyu Luo. Alpha-sql: Zero-shot\\ntext-to-sql using monte carlo tree search. CoRR , abs/2502.17248, 2025.\\n[366] Allen Newell. Unified theories of cognition . Harvard University Press, 1994.\\n[367] Pierre Harvey Richemond, Yunhao Tang, Daniel Guo, Daniele Calandriello, Mohammad Gheshlaghi Azar,\\nRafael Rafailov, Bernardo Avila Pires, Eugene Tarassov, Lucas Spangher, Will Ellsworth, et al. Offline\\nregularised reinforcement learning for large language models alignment. arXiv preprint arXiv:2405.19107 ,\\n2024.\\n[368] Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, and Chanjun Park. sdpo:\\nDon’t use your data all at once. arXiv preprint arXiv:2403.19270 , 2024.\\n[369] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko,\\nand Daniele Calandriello. A general theoretical paradigm to understand learning from human preferences. In\\nInternational Conference on Artificial Intelligence and Statistics , pages 4447–4455. PMLR, 2024.\\n[370] Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, and Xiangnan\\nHe.β-dpo: Direct preference optimization with dynamic β, 2024. URL https://arxiv.org/abs/2407.\\n08639 .\\n[371] Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model.\\narXiv preprint arXiv:2403.07691 , 2024.\\n[372] Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie.\\nDirect nash optimization: Teaching language models to self-improve with general preferences. arXiv preprint\\narXiv:2404.03715 , 2024.\\n[373] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse kl: Generalizing direct\\npreference optimization with diverse divergence constraints. arXiv preprint arXiv:2309.16240 , 2023.\\n[374] Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others:\\nIterative preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682 , 2023.\\n[375] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From rtoq∗: Your language model is secretly a\\nq-function. arXiv preprint arXiv:2404.12358 , 2024.\\n209\\n[376] Shiva Kumar Pentyala, Zhichao Wang, Bin Bi, Kiran Ramnath, Xiang-Bo Mao, Regunathan Radhakrish-\\nnan, Sitaram Asur, et al. Paft: A parallel training paradigm for effective llm fine-tuning. arXiv preprint\\narXiv:2406.17923 , 2024.\\n[377] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free\\nreward. Advances in Neural Information Processing Systems , 37:124198–124235, 2025.\\n[378] Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad Saleh,\\nSimon Baumgartner, Jialu Liu, et al. Lipo: Listwise preference optimization through learning-to-rank. arXiv\\npreprint arXiv:2402.01878 , 2024.\\n[379] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses\\nto align language models with human feedback without tears. arXiv preprint arXiv:2304.05302 , 2023.\\n[380] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference\\nranking optimization for human alignment. In Proceedings of the AAAI Conference on Artificial Intelligence ,\\nvolume 38, pages 18990–18998, 2024.\\n[381] Shitong Duan, Xiaoyuan Yi, Peng Zhang, Yan Liu, Zheng Liu, Tun Lu, Xing Xie, and Ning Gu. Negating\\nnegatives: Alignment with human negative samples via distributional dispreference optimization. arXiv preprint\\narXiv:2403.03419 , 2024.\\n[382] Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. Negative preference optimization: From catastrophic collapse\\nto effective unlearning. arXiv preprint arXiv:2404.05868 , 2024.\\n[383] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet\\nÜstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human\\nfeedback in llms. arXiv preprint arXiv:2402.14740 , 2024.\\n[384] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and\\nYoung Jin Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine\\ntranslation. arXiv preprint arXiv:2401.08417 , 2024.\\n[385] Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhao-\\nhan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from\\nhuman feedback. arXiv preprint arXiv:2312.00886 , 18, 2023.\\n[386] Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. A minimaximalist\\napproach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056 , 2024.\\n[387] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-\\nsupervised prediction. In International conference on machine learning , pages 2778–2787. PMLR, 2017.\\n[388] Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement. In\\nInternational conference on machine learning , pages 5062–5071. PMLR, 2019.\\n[389] Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to\\nexplore via self-supervised world models. In International conference on machine learning , pages 8583–8592.\\nPMLR, 2020.\\n[390] Yali Du, Lei Han, Meng Fang, Ji Liu, Tianhong Dai, and Dacheng Tao. Liir: Learning individual intrinsic\\nreward in multi-agent reinforcement learning. Advances in Neural Information Processing Systems , 32, 2019.\\n[391] Cédric Colas, Pierre Fournier, Mohamed Chetouani, Olivier Sigaud, and Pierre-Yves Oudeyer. Curious:\\nintrinsically motivated modular multi-goal reinforcement learning. In International conference on machine\\nlearning , pages 1331–1340. PMLR, 2019.\\n[392] Vitchyr H Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-fit: State-\\ncovering self-supervised reinforcement learning. arXiv preprint arXiv:1903.03698 , 2019.\\n[393] Ali Hassani, Amir Iranmanesh, Mahdi Eftekhari, and Abbas Salemi. Discern: diversity-based selection of\\ncentroids for k-estimation and rapid non-stochastic clustering. International Journal of Machine Learning and\\nCybernetics , 12:635–649, 2021.\\n[394] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason\\nWeston. Self-rewarding language models. arXiv preprint arXiv:2401.10020 , 2024.\\n[395] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as\\nprospect theoretic optimization. arXiv preprint arXiv:2402.01306 , 2024.\\n[396] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation.\\narXiv preprint arXiv:1810.12894 , 2018.\\n210\\n[397] Jean-Francois Ton, Muhammad Faaiz Taufiq, and Yang Liu. Understanding chain-of-thought in LLMs through\\ninformation theory. arXiv preprint arXiv:2411.11984 , 2024.\\n[398] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational\\ninformation maximizing exploration. Advances in neural information processing systems , 29, 2016.\\n[399] Hyoungseok Kim, Jaekyeom Kim, Yeonwoo Jeong, Sergey Levine, and Hyun Oh Song. Emi: Exploration with\\nmutual information. arXiv preprint arXiv:1810.01176 , 2018.\\n[400] Pranav Shyam, Wojciech Ja ´skowski, and Faustino Gomez. Model-based active exploration. In International\\nconference on machine learning , pages 5779–5788. PMLR, 2019.\\n[401] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop,\\nEthan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif vs. rlhf: Scaling reinforcement learning from human\\nfeedback with ai feedback. arXiv preprint arXiv:2309.00267 , 2023.\\n[402] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen,\\nAnna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback.\\narXiv preprint arXiv:2212.08073 , 2022.\\n[403] Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative\\npreference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. arXiv\\npreprint arXiv:2312.11456 , 2023.\\n[404] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo,\\nCaiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint\\narXiv:2405.07863 , 2024.\\n[405] Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. Token-level direct\\npreference optimization. arXiv preprint arXiv:2404.11999 , 2024.\\n[406] Robert G Lewis, Ermanno Florio, Daniela Punzo, and Emiliana Borrelli. The Brain’s reward system in health\\nand disease . Springer, 2021.\\n[407] Marc Fakhoury. The Brain Reward System . Springer, 2021.\\n[408] Vincent Breton-Provencher and Mriganka Sur. Active control of arousal by a locus coeruleus gabaergic circuit.\\nNature neuroscience , 22(2):218–228, 2019.\\n[409] Jia Qi, Shiliang Zhang, Hui-Ling Wang, Huikun Wang, Jose de Jesus Aceves Buendia, Alexander F Hoffman,\\nCarl R Lupica, Rebecca P Seal, and Marisela Morales. A glutamatergic reward input from the dorsal raphe to\\nventral tegmental area dopamine neurons. Nature communications , 5(1):5390, 2014.\\n[410] Melissa J Sharpe, Nathan J Marchant, Leslie R Whitaker, Christopher T Richie, Yajun J Zhang, Erin J Campbell,\\nPyry P Koivula, Julie C Necarsulmer, Carlos Mejias-Aponte, Marisela Morales, et al. Lateral hypothalamic\\ngabaergic neurons encode reward predictions that are relayed to the ventral tegmental area to regulate learning.\\nCurrent Biology , 27(14):2089–2100, 2017.\\n[411] MSD Manual. Neurotransmission, 2022. URL https://www.msdmanuals.cn/professional/\\nneurologic-disorders/neurotransmission/neurotransmission . Accessed: 2022-04-01.\\n[412] Anil Ananthaswamy. How close is AI to human-level intelligence? Nature , 636(8041):22–25, 2024.\\n[413] Eric G Ceballos, Asa Farahani, Zhen-Qi Liu, Filip Milisav, Justine Y Hansen, Alain Dagher, and Bratislav\\nMisic. Mapping neuropeptide sigaling in the human brain. bioRxiv , pages 2024–12, 2024.\\n[414] Jinghan Zhang, Xiting Wang, Yiqiao Jin, Changyu Chen, Xinhao Zhang, and Kunpeng Liu. Prototypical reward\\nnetwork for data-efficient rlhf. In ACL, 2024.\\n[415] Sebastian Thrun and Michael L Littman. Reinforcement learning: An introduction. AI Magazine , 21(1):\\n103–103, 2000.\\n[416] Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. The cringe\\nloss: Learning what language not to model. arXiv preprint arXiv:2211.05826 , 2022.\\n[417] Luiz Pessoa. Multiple influences of reward on perception and attention. Visual cognition , 23(1-2):272–290,\\n2015.\\n[418] Han-Xiao Li, Quan-Shan Long, An-Tao Chen, and Qing Li. The influence of reward motivation on emotion\\nregulation. Sheng li xue bao:[Acta Physiologica Sinica] , 71(4):562–574, 2019.\\n[419] Ewa A Miendlarzewska, Daphne Bavelier, and Sophie Schwartz. Influence of reward motivation on human\\ndeclarative memory. Neuroscience & Biobehavioral Reviews , 61:156–176, 2016.\\n211\\n[420] Marvin Lee Minsky, editor. The Emotion Machine: Commensense Thinking, Artificial Intelligence, and the\\nFuture of the Human Mind . Simon & Schuster, 2006.\\n[421] Paul Ekman. An argument for basic emotions. Cognition & Emotion , 6:169–200, 1992.\\n[422] Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, and\\nXing Xie. Large language models understand and can be enhanced by emotional stimuli. arXiv preprint arXiv:\\n2307.11760 , 2023.\\n[423] Xuena Wang, Xueting Li, Zi Yin, Yue Wu, and Jia Liu. Emotional intelligence of large language models.\\nJournal of Pacific Rim Psychology , 17:18344909231213958, 2023.\\n[424] Lisa Feldman Barrett. The theory of constructed emotion: an active inference account of interoception and\\ncategorization. Social Cognitive and Affective Neuroscience , 12:1833 – 1833, 2017.\\n[425] Rachael E. Jack, Oliver G. B. Garrod, Hui Yu, Roberto Caldara, and Philippe G. Schyns. Facial expres-\\nsions of emotion are not culturally universal. Proceedings of the National Academy of Sciences , 109(19):\\n7241–7244, 2012. doi:10.1073/pnas.1200155109. URL https://www.pnas.org/doi/abs/10.1073/pnas.\\n1200155109 .\\n[426] James Russell. A circumplex model of affect. Journal of Personality and Social Psychology , 39:1161–1178, 12\\n1980. doi:10.1037/h0077714.\\n[427] Albert Mehrabian. Pleasure-arousal-dominance: A general framework for describing and measuring individual\\ndifferences in temperament. Current Psychology , 14:261–292, 1996.\\n[428] Zhenyi Lu, Wei Wei, Xiaoye Qu, Xian-Ling Mao, Dangyang Chen, and Jixiong Chen. Miracle: Towards person-\\nalized dialogue generation with latent-space multiple personal attribute control. In Houda Bouamor, Juan Pino,\\nand Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 5933–\\n5957, Singapore, December 2023. Association for Computational Linguistics. doi:10.18653/v1/2023.findings-\\nemnlp.395. URL https://aclanthology.org/2023.findings-emnlp.395/ .\\n[429] Ala N. Tak and Jonathan Gratch. Is gpt a computational model of emotion? detailed analysis. arXiv preprint\\narXiv: 2307.13779 , 2023.\\n[430] Shudong Liu, Yiqiao Jin, Cheng Li, Derek F Wong, Qingsong Wen, Lichao Sun, Haipeng Chen, Xing Xie, and\\nJindong Wang. Culturevlm: Characterizing and improving cultural understanding of vision-language models\\nfor over 100 countries. arXiv:2501.01282 , 2025.\\n[431] Robert Plutchik. A general psychoevolutionary theory of emotion. Theories of emotion , 1:3–31, 1980.\\n[432] Klaus R. Scherer. The dynamic architecture of emotion: Evidence for the component process model. Cognition\\nand Emotion , 23:1307 – 1351, 2009.\\n[433] Andrew Ortony, Gerald L Clore, and Allan Collins. The cognitive structure of emotions . Cambridge university\\npress, 2022.\\n[434] Eva Hudlicka. Computational modeling of cognition-emotion interactions: Relevance to mechanisms of\\naffective disorders and therapeutic action. Cognitive Science , 36, 2014.\\n[435] Stacy Marsella and J. Gratch. Computationally modeling human emotion. Commun. ACM , 57:56–67, 2014.\\n[436] Hao Fei, Bobo Li, Qian Liu, Lidong Bing, Fei Li, and Tat seng Chua. Reasoning implicit sentiment\\nwith chain-of-thought prompting. Annual Meeting of the Association for Computational Linguistics , 2023.\\ndoi:10.48550/arXiv.2305.11255.\\n[437] Xiaofei Sun, Xiaoya Li, Shengyu Zhang, Shuhe Wang, Fei Wu, Jiwei Li, Tianwei Zhang, and Guoyin Wang.\\nSentiment analysis through LLM negotiations. arXiv preprint arXiv: 2311.01876 , 2023.\\n[438] Adam S Lowet, Qiao Zheng, Melissa Meng, Sara Matias, Jan Drugowitsch, and Naoshige Uchida. An opponent\\nstriatal circuit for distributional reinforcement learning. Nature , pages 1–10, 2025.\\n[439] Xin Hong, Yuan Gong, Vidhyasaharan Sethu, and Ting Dang. Aer-llm: Ambiguity-aware emotion recognition\\nleveraging large language models. arXiv preprint arXiv: 2409.18339 , 2024.\\n[440] Zebang Cheng, Zhi-Qi Cheng, Jun-Yan He, Kai Wang, Yuxiang Lin, Zheng Lian, Xiaojiang Peng, and\\nAlexander Hauptmann. Emotion-LLaMA: Multimodal emotion recognition and reasoning with instruc-\\ntion tuning. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang,\\neditors, Advances in Neural Information Processing Systems , volume 37, pages 110805–110853. Curran\\nAssociates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/\\nc7f43ada17acc234f568dc66da527418-Paper-Conference.pdf .\\n212\\n[441] Sahand Sabour, Siyang Liu, Zheyuan Zhang, June Liu, Jinfeng Zhou, Alvionna Sunaryo, Tatia Lee, Rada\\nMihalcea, and Minlie Huang. EmoBench: Evaluating the emotional intelligence of large language models.\\nIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers) , pages 5986–6004, Bangkok,\\nThailand, August 2024. Association for Computational Linguistics. doi:10.18653/v1/2024.acl-long.326. URL\\nhttps://aclanthology.org/2024.acl-long.326/ .\\n[442] Wenbin Wang, Liang Ding, Li Shen, Yong Luo, Han Hu, and Dacheng Tao. Wisdom: Improving multimodal\\nsentiment analysis by fusing contextual world knowledge. In Proceedings of the 32nd ACM International\\nConference on Multimedia , MM ’24, page 2282–2291, New York, NY , USA, 2024. Association for Computing\\nMachinery. ISBN 9798400706868. doi:10.1145/3664647.3681403. URL https://doi.org/10.1145/\\n3664647.3681403 .\\n[443] Jinyang Wu, Mingkuan Feng, Shuai Zhang, Feihu Che, Zengqi Wen, and Jianhua Tao. Beyond examples:\\nHigh-level automated reasoning paradigm in in-context learning via mcts. arXiv preprint arXiv:2411.18478 ,\\n2024.\\n[444] Zheng Lian, Haiyang Sun, Licai Sun, Hao Gu, Zhuofan Wen, Siyuan Zhang, Shun Chen, Mingyu Xu, Ke Xu,\\nKang Chen, Lan Chen, Shan Liang, Ya Li, Jiangyan Yi, Bin Liu, and Jianhua Tao. Explainable multimodal\\nemotion recognition. arXiv preprint arXiv: 2306.15401 , 2023.\\n[445] Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, Runqi Qiao, and Sirui Wang. Instructerc:\\nReforming emotion recognition in conversation with multi-task retrieval-augmented large language models.\\narXiv preprint arXiv: 2309.11911 , 2023.\\n[446] Zheng Lian, Licai Sun, Haiyang Sun, Kang Chen, Zhuofan Wen, Hao Gu, Bin Liu, and Jianhua Tao. GPT-4V\\nwith emotion: A zero-shot benchmark for generalized emotion recognition. Inf. Fusion , 108:102367, 2024.\\ndoi:10.1016/J.INFFUS.2024.102367. URL https://doi.org/10.1016/j.inffus.2024.102367 .\\n[447] Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, and Srijan Kumar. Mm-soc: Benchmarking multimodal\\nlarge language models in social media platforms. In ACL, 2024.\\n[448] William Stigall, Md Abdullah Al Hafiz Khan, Dinesh Attota, Francis Nweke, and Yong Pei. Large language\\nmodels performance comparison of emotion and sentiment classification. In Proceedings of the 2024 ACM\\nSoutheast Conference , ACMSE ’24, page 60–68, New York, NY , USA, 2024. Association for Computing\\nMachinery. ISBN 9798400702372. doi:10.1145/3603287.3651183. URL https://doi.org/10.1145/\\n3603287.3651183 .\\n[449] Steve Rathje, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire E. Robertson, and Jay Joseph Van Bavel.\\nGpt is an effective tool for multilingual psychological text analysis. Proceedings of the National Academy of\\nSciences of the United States of America , 121, 2024.\\n[450] Minxue Niu, Mimansa Jaiswal, and E. Provost. From text to emotion: Unveiling the emotion annotation\\ncapabilities of llms. INTERSPEECH , 2024. doi:10.21437/interspeech.2024-2282.\\n[451] Haiquan Zhao, Lingyu Li, Shisong Chen, Shuqi Kong, Jiaan Wang, Kexin Huang, Tianle Gu, Yixu Wang, Wang\\nJian, Dandan Liang, Zhixu Li, Yan Teng, Yanghua Xiao, and Yingchun Wang. Esc-eval: Evaluating emotion\\nsupport conversations in large language models. arXiv preprint arXiv: 2406.14952 , 2024.\\n[452] Yingjie Zhou, Zicheng Zhang, Jiezhang Cao, Jun Jia, Yanwei Jiang, Farong Wen, Xiaohong Liu, Xiongkuo Min,\\nand Guangtao Zhai. Memo-bench: A multiple benchmark for text-to-image and multimodal large language\\nmodels on human emotion analysis. arXiv preprint arXiv: 2411.11235 , 2024.\\n[453] Zheng Lian, Licai Sun, Yong Ren, Hao Gu, Haiyang Sun, Lan Chen, Bin Liu, and Jianhua Tao. Merbench: A\\nunified evaluation benchmark for multimodal emotion recognition. arXiv preprint arXiv: 2401.03429 , 2024.\\n[454] Mostafa M. Amin, Rui Mao, Erik Cambria, and Björn W. Schuller. A wide evaluation of chatgpt on affective\\ncomputing tasks. IEEE Trans. Affect. Comput. , 15(4):2204–2212, 2024. doi:10.1109/TAFFC.2024.3419593.\\nURL https://doi.org/10.1109/TAFFC.2024.3419593 .\\n[455] Weixiang Zhao, Yanyan Zhao, Xin Lu, Shilong Wang, Yanpeng Tong, and Bing Qin. Is chatgpt equipped with\\nemotional dialogue capabilities? arXiv preprint arXiv: 2304.09582 , 2023.\\n[456] Tom Sühr, Florian E. Dorner, Samira Samadi, and Augustin Kelava. Challenging the validity of personality\\ntests for large language models. arXiv preprint arXiv: 2311.05297 , 2023.\\n[457] Nikolay B Petrov, Gregory Serapio-García, and Jason Rentfrow. Limited ability of LLMs to simulate human\\npsychological behaviours: a psychometric analysis. arXiv preprint arXiv: 2405.07248 , 2024.\\n213\\n[458] Jen tse Huang, Wenxuan Wang, Eric John Li, Man Ho LAM, Shujie Ren, Youliang Yuan, Wenxiang Jiao,\\nZhaopeng Tu, and Michael Lyu. On the humanity of conversational AI: Evaluating the psychological portrayal\\nof LLMs. In The Twelfth International Conference on Learning Representations , 2024. URL https://\\nopenreview.net/forum?id=H3UayAQWoE .\\n[459] Jen tse Huang, Wenxiang Jiao, Man Ho Lam, Eric John Li, Wenxuan Wang, and Michael R. Lyu. Revisiting\\nthe reliability of psychological scales on large language models. arXiv preprint arXiv: 2305.19926 , 2023.\\n[460] Yiming Ai, Zhiwei He, Ziyin Zhang, Wenhong Zhu, Hongkun Hao, Kai Yu, Lingjun Chen, and Rui Wang. Is\\ncognition and action consistent or not: Investigating large language model’s personality. arXiv preprint arXiv:\\n2402.14679 , 2024.\\n[461] Xintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang\\nLeng, Wei Wang, Jiangjie Chen, Cheng Li, and Yanghua Xiao. Incharacter: Evaluating personality fidelity in\\nrole-playing agents through psychological interviews. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar,\\neditors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume\\n1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024 , pages 1840–1873. Association for\\nComputational Linguistics, 2024. doi:10.18653/V1/2024.ACL-LONG.102. URL https://doi.org/10.\\n18653/v1/2024.acl-long.102 .\\n[462] Marcel Binz and Eric Schulz. Turning large language models into cognitive models. In The Twelfth International\\nConference on Learning Representations , 2024. URL https://openreview.net/forum?id=eiC4BKypf1 .\\n[463] Thilo Hagendorff, Ishita Dasgupta, Marcel Binz, Stephanie C. Y . Chan, Andrew Lampinen, Jane X. Wang,\\nZeynep Akata, and Eric Schulz. Machine psychology. arXiv preprint arXiv: 2303.13988 , 2023.\\n[464] Julian Coda-Forno, Marcel Binz, Jane X. Wang, and Eric Schulz. Cogbench: a large language model walks into\\na psychology lab. International Conference on Machine Learning , 2024. doi:10.48550/arXiv.2402.18225.\\n[465] Jesse Roberts, Kyle Moore, Drew Wilenzick, and Doug Fisher. Using artificial populations to\\nstudy psychological phenomena in neural models. AAAI Conference on Artificial Intelligence , 2023.\\ndoi:10.1609/aaai.v38i17.29856.\\n[466] Maor Reuben, Ortal Slobodin, Aviad Elyshar, Idan-Chaim Cohen, Orna Braun-Lewensohn, Odeya Cohen,\\nand Rami Puzis. Assessment and manipulation of latent constructs in pre-trained language models using\\npsychometric scales. arXiv preprint arXiv: 2409.19655 , 2024.\\n[467] Jen tse Huang, Man Ho LAM, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, and\\nMichael Lyu. Apathetic or empathetic? evaluating LLMs’ emotional alignments with humans. In The Thirty-\\neighth Annual Conference on Neural Information Processing Systems , 2024. URL https://openreview.\\nnet/forum?id=pwRVGRWtGg .\\n[468] Bo Zhao, Maya Okawa, Eric J Bigelow, Rose Yu, Tomer Ullman, and Hidenori Tanaka. Emergence of\\nhierarchical emotion representations in large language models, 2025. URL https://openreview.net/\\nforum?id=wTm4W39GdD .\\n[469] Fiona Anting Tan, Gerard Christopher Yeo, Kokil Jaidka, Fanyou Wu, Weijie Xu, Vinija Jain, Aman Chadha,\\nYang Liu, and See-Kiong Ng. Phantom: Persona-based prompting has an effect on theory-of-mind reasoning in\\nlarge language models. arXiv preprint arXiv: 2403.02246 , 2024.\\n[470] Hang Jiang, Xiajie Zhang, Xubo Cao, Cynthia Breazeal, Deb Roy, and Jad Kabbara. PersonaLLM: Investigating\\nthe ability of large language models to express personality traits. In Kevin Duh, Helena Gomez, and Steven\\nBethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024 , pages 3605–3627,\\nMexico City, Mexico, June 2024. Association for Computational Linguistics. doi:10.18653/v1/2024.findings-\\nnaacl.229. URL https://aclanthology.org/2024.findings-naacl.229/ .\\n[471] Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, and Zeynep Akata. In-context impersonation\\nreveals large language models’ strengths and biases. In Alice Oh, Tristan Naumann, Amir Globerson, Kate\\nSaenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36 ,\\n2023.\\n[472] Lucio La Cava and Andrea Tagarelli. Open models, closed minds? on agents capabilities in mimicking human\\npersonalities through open large language models. arXiv preprint arXiv: 2401.07115 , 2024.\\n[473] Navya Jain, Zekun Wu, Cristian Munoz, Airlie Hilliard, Adriano Koshiyama, Emre Kazim, and Philip Treleaven.\\nFrom text to emoji: How peft-driven personality manipulation unleashes the emoji potential in llms. arXiv\\npreprint arXiv: 2409.10245 , 2024.\\n214\\n[474] Jen-tse Huang, Wenxiang Jiao, Man Ho Lam, Eric John Li, Wenxuan Wang, and Michael Lyu. On the relia-\\nbility of psychological scales on large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung\\nChen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Process-\\ning, pages 6152–6173, Miami, Florida, USA, November 2024. Association for Computational Linguistics.\\ndoi:10.18653/v1/2024.emnlp-main.354. URL https://aclanthology.org/2024.emnlp-main.354/ .\\n[475] Jia Deng, Tianyi Tang, Yanbin Yin, Wenhao Yang, Wayne Xin Zhao, and Ji-Rong Wen. Neuron-based\\npersonality trait induction in large language models. arXiv preprint arXiv: 2410.12327 , 2024.\\n[476] Lena Podoletz. We have to talk about emotional AI and crime. AI & SOCIETY , 38(3):1067–1082, 2023.\\n[477] Author Name(s). Emotional ai: Privacy, manipulation, and bias risks, 2024. URL https://\\nbusinesslawtoday.org/2024/09/emotional-ai-privacy-manipulation-bias-risks/ . Accessed\\nJanuary 18, 2025.\\n[478] Author Name(s). Emotional artificial intelligence: Risks and opportunities, 2024. URL https://www.\\nlinkedin.com/pulse/emotional-artificial-intelligence-risks-opportunities-vincent-mba-e2rre/ .\\nAccessed January 18, 2025.\\n[479] Julian Coda-Forno, Kristin Witte, Akshay K. Jagadish, Marcel Binz, Zeynep Akata, and Eric Schulz. Inducing\\nanxiety in large language models can induce bias, 2024. URL https://arxiv.org/abs/2304.11111 .\\n[480] Yiqiao Jin, Mohit Chandra, Gaurav Verma, Yibo Hu, Munmun De Choudhury, and Srijan Kumar. Better to ask\\nin english: Cross-lingual evaluation of large language models for healthcare queries. In Web Conference , pages\\n2627–2638, 2024.\\n[481] Peter Mantello and Manh-Tung Ho. Emotional AI and the future of wellbeing in the post-pandemic workplace.\\nAI & society , 39(4):1883–1889, 2024.\\n[482] Corina Pelau, Dan-Cristian Dabija, and Irina Ene. What makes an AI device human-like? the role of interaction\\nquality, empathy and perceived psychological anthropomorphic characteristics in the acceptance of artificial\\nintelligence in the service industry. Computers in Human Behavior , 122:106855, 2021.\\n[483] Jay Ratican and James Hutson. The six emotional dimension (6de) model: A multidimensional approach to\\nanalyzing human emotions and unlocking the potential of emotionally intelligent artificial intelligence (ai) via\\nlarge language models (llm). Journal of Artificial Intelligence and Robotics , 1(1), 2023.\\n[484] Baihan Lin, Djallel Bouneffouf, Guillermo Cecchi, and Kush R Varshney. Towards healthy ai: large language\\nmodels need therapists too. arXiv preprint arXiv:2308.04434 , 2023.\\n[485] Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint\\narXiv:1810.04805 , 2018.\\n[486] Yinhan Liu. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 364,\\n2019.\\n[487] Z Lan. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint\\narXiv:1909.11942 , 2019.\\n[488] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\\nProceedings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016.\\n[489] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\\nZagoruyko. End-to-end object detection with transformers. In European conference on computer vision ,\\npages 213–229. Springer, 2020.\\n[490] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma,\\nXiaoke Jiang, Yihao Chen, et al. Grounding dino 1.5: Advance the “edge” of open-set object detection. arXiv\\npreprint arXiv:2405.10300 , 2024.\\n[491] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu ˇci´c, and Cordelia Schmid. Vivit: A\\nvideo vision transformer. In Proceedings of the IEEE/CVF international conference on computer vision , pages\\n6836–6846, 2021.\\n[492] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient\\nlearners for self-supervised video pre-training. Advances in neural information processing systems , 35:10078–\\n10093, 2022.\\n[493] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2: Fast and\\nhigh-quality end-to-end text to speech. arXiv preprint arXiv:2006.04558 , 2020.\\n215\\n[494] Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-\\nAmbroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, et al. Seamless: Multilingual expressive and\\nstreaming speech translation. arXiv preprint arXiv:2312.05187 , 2023.\\n[495] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for\\nself-supervised learning of speech representations. Advances in neural information processing systems , 33:\\n12449–12460, 2020.\\n[496] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\\nTalking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671 , 2023.\\n[497] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,\\nMichael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv\\npreprint arXiv:2303.11381 , 2023.\\n[498] Dídac Surís, Sachit Menon, and Carl V ondrick. Vipergpt: Visual inference via python execution for reasoning.\\nInProceedings of the IEEE/CVF International Conference on Computer Vision , pages 11888–11898, 2023.\\n[499] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing\\nHong, Jiawei Huang, Jinglin Liu, et al. Audiogpt: Understanding and generating speech, music, sound, and\\ntalking head. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 23802–23804,\\n2024.\\n[500] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su,\\nJun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. In European Conference on\\nComputer Vision , pages 126–142. Springer, 2025.\\n[501] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li,\\nand Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In\\nInternational conference on machine learning , pages 4904–4916. PMLR, 2021.\\n[502] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang,\\nJoyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn.\\nopenai. com/papers/dall-e-3. pdf , 2(3):8, 2023.\\n[503] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and\\nperformant baseline for vision and language. arXiv preprint arXiv:1908.03557 , 2019.\\n[504] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer,\\nand Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. arXiv\\npreprint arXiv:2109.14084 , 2021.\\n[505] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Moham-\\nmad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation\\nfrom open domain textual description. arXiv preprint arXiv:2210.02399 , 2022.\\n[506] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron\\nAshual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint\\narXiv:2209.14792 , 2022.\\n[507] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan Pablo Bello. Wav2clip: Learning robust audio\\nrepresentations from clip. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and\\nSignal Processing (ICASSP) , pages 4563–4567. IEEE, 2022.\\n[508] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong.\\nVatt: Transformers for multimodal self-supervised learning from raw video, audio and text. Advances in neural\\ninformation processing systems , 34:24206–24221, 2021.\\n[509] Andrey Guzhov, Federico Raue, Jörn Hees, and Andreas Dengel. Audioclip: Extending clip to image, text\\nand audio. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing\\n(ICASSP) , pages 976–980. IEEE, 2022.\\n[510] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi\\nMalekshan. Clip-forge: Towards zero-shot text-to-shape generation. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , pages 18603–18613, 2022.\\n[511] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for\\ngenerating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751 , 2022.\\n216\\n[512] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,\\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified\\ninterface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478 , 2023.\\n[513] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next:\\nImproved reasoning, ocr, and world knowledge, 2024.\\n[514] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Ya ƒan Wang, Yean Cheng, Shiyu Huang,\\nJunhui Ji, Zhao Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv\\npreprint arXiv:2408.16500 , 2024.\\n[515] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang,\\nWenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution.\\narXiv preprint arXiv:2409.12191 , 2024.\\n[516] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu,\\nTiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 14398–14409, 2024.\\n[517] Zhengqing Yuan, Zhaoxu Li, Weiran Huang, Yanfang Ye, and Lichao Sun. Tinygpt-v: Efficient multimodal\\nlarge language model via small backbones. arXiv preprint arXiv:2312.16862 , 2023.\\n[518] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang,\\nBo Zhang, Xiaolin Wei, et al. Mobilevlm: A fast, strong and open vision language assistant for mobile devices.\\narXiv preprint arXiv:2312.16886 , 2023.\\n[519] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao,\\nZhihui He, et al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800 , 2024.\\n[520] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent.\\narXiv preprint arXiv:2408.00203 , 2024.\\n[521] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation.\\nInConference on robot learning , pages 894–906. PMLR, 2022.\\n[522] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana\\nGopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world\\ncontrol at scale. arXiv preprint arXiv:2212.06817 , 2022.\\n[523] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart,\\nSean Kirmani, Brianna Zitkovich, Fei Xia, et al. Open-world object manipulation using pre-trained vision-\\nlanguage models. arXiv preprint arXiv:2303.00905 , 2023.\\n[524] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for robotic\\nmanipulation. In Conference on Robot Learning , pages 785–799. PMLR, 2023.\\n[525] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran\\nSong. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics\\nResearch , page 02783649241273668, 2023.\\n[526] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv\\npreprint arXiv:2303.03378 , 2023.\\n[527] Yining Hong, Zishuo Zheng, Peihao Chen, Yian Wang, Junyan Li, and Chuang Gan. Multiply: A multisensory\\nobject-centric embodied large language model in 3d world. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition , pages 26406–26416, 2024.\\n[528] Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle, and Bryan Catanzaro. Audio flamingo: A\\nnovel audio language model with few-shot learning and dialogue abilities. arXiv preprint arXiv:2402.01831 ,\\n2024.\\n[529] Nilaksh Das, Saket Dingliwal, Srikanth Ronanki, Rohit Paturi, Zhaocheng Huang, Prashant Mathur, Jie Yuan,\\nDhanush Bekal, Xing Niu, Sai Muralidhar Jayanthi, et al. Speechverse: A large-scale generalizable audio\\nlanguage model. arXiv preprint arXiv:2405.08295 , 2024.\\n[530] Dongchao Yang, Haohan Guo, Yuanyuan Wang, Rongjie Huang, Xiang Li, Xu Tan, Xixin Wu, and Helen\\nMeng. Uniaudio 1.5: Large language model-driven audio codec is a few-shot audio task learner. arXiv preprint\\narXiv:2406.10056 , 2024.\\n[531] Dongting Li, Chenchong Tang, and Han Liu. Audio-llm: Activating the capabilities of large language models\\nto comprehend audio data. In International Symposium on Neural Networks , pages 133–142. Springer, 2024.\\n217\\n[532] Zhifei Xie and Changqiao Wu. Mini-omni: Language models can hear, talk while thinking in streaming. arXiv\\npreprint arXiv:2408.16725 , 2024.\\n[533] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt:\\nEmpowering large language models with intrinsic cross-modal conversational abilities. arXiv preprint\\narXiv:2305.11000 , 2023.\\n[534] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang\\nZhou. One-peace: Exploring one general representation model toward unlimited modalities. arXiv preprint\\narXiv:2305.11172 , 2023.\\n[535] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-\\nfollow them all. arXiv preprint arXiv:2305.16355 , 2023.\\n[536] Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and\\nZhaopeng Tu. Macaw-LLM: Multi-modal language modeling with image, audio, video, and text integration.\\narXiv preprint arXiv:2306.09093 , 2023.\\n[537] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang,\\nZongwei Li, et al. Languagebind: Extending video-language pretraining to n-modality by language-based\\nsemantic alignment. arXiv preprint arXiv:2310.01852 , 2023.\\n[538] Mustafa Shukor, Corentin Dancette, Alexandre Rame, and Matthieu Cord. Unival: Unified model for image,\\nvideo, audio and language tasks. Transactions on Machine Learning Research Journal , 2023.\\n[539] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. X-LLM:\\nBootstrapping advanced large language models by treating multi-modalities as foreign languages. arXiv preprint\\narXiv:2305.04160 , 2023.\\n[540] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. PointLLM: Empowering\\nlarge language models to understand point clouds. In European Conference on Computer Vision , pages 131–147.\\nSpringer, 2025.\\n[541] Yuan Tang, Xu Han, Xianzhi Li, Qiao Yu, Yixue Hao, Long Hu, and Min Chen. Minigpt-3d: Efficiently aligning\\n3d point clouds with large language models using 2d priors. In Proceedings of the 32nd ACM International\\nConference on Multimedia , pages 6617–6626, 2024.\\n[542] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal LLM.\\narXiv preprint arXiv:2309.05519 , 2023.\\n[543] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and\\nAniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio\\nand action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\\n26439–26455, 2024.\\n[544] Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. Codi-2: In-context\\ninterleaved and interactive any-to-any generation. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 27425–27434, 2024.\\n[545] Xinyu Wang, Bohan Zhuang, and Qi Wu. Modaverse: Efficiently transforming modalities with LLMs. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 26606–26616,\\n2024.\\n[546] Fiona Macpherson. The senses: Classic and contemporary philosophical perspectives , volume 11. Oxford\\nUniversity Press, 2011.\\n[547] Jamie Ward. The student’s guide to cognitive neuroscience . Routledge, 2019.\\n[548] Stanley Coren, Lawrence M Ward, and James T Enns. Sensation and perception . John Wiley & Sons Hoboken,\\nNJ, 2004.\\n[549] Simon Grondin. Timing and time perception: A review of recent behavioral and neuroscience findings and\\ntheoretical directions. Attention, Perception, & Psychophysics , 72(3):561–582, 2010.\\n[550] Henrik Mouritsen. Long-distance navigation and magnetoreception in migratory animals. Nature , 558(7708):\\n50–59, 2018.\\n[551] Chen Wang, Zhesi Chen, Chak Lam Jonathan Chan, Zhu’an Wan, Wenhao Ye, Wenying Tang, Zichao Ma,\\nBeitao Ren, Daquan Zhang, Zhilong Song, et al. Biomimetic olfactory chips based on large-scale monolithically\\nintegrated nanotube sensor arrays. Nature Electronics , 7(2):157–167, 2024.\\n218\\n[552] Caroline Bushdid, Marcelo O Magnasco, Leslie B V osshall, and Andreas Keller. Humans can discriminate\\nmore than 1 trillion olfactory stimuli. Science , 343(6177):1370–1372, 2014.\\n[553] Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and\\ntaxonomy. IEEE transactions on pattern analysis and machine intelligence , 41(2):423–443, 2018.\\n[554] Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, José Neira, Ian Reid, and John J\\nLeonard. Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age.\\nIEEE Transactions on robotics , 32(6):1309–1332, 2016.\\n[555] Yin Zhang, Rong Jin, and Zhi-Hua Zhou. Understanding bag-of-words model: a statistical framework.\\nInternational journal of machine learning and cybernetics , 1:43–52, 2010.\\n[556] OpenAI. Gpt-3.5: Language model, 2023. URL https://platform.openai.com/docs/models/gpt-3.\\n5-turbo .\\n[557] Glenn Jocher. YOLOv5 by Ultralytics, May 2020. URL https://github.com/ultralytics/yolov5 .\\n[558] Glenn Jocher, Jing Qiu, and Ayush Chaurasia. Ultralytics YOLO, January 2023. URL https://github.com/\\nultralytics/ultralytics .\\n[559] Chang Zeng, Xin Wang, Erica Cooper, Xiaoxiao Miao, and Junichi Yamagishi. Attention back-end for automatic\\nspeaker verification with multiple enrollment utterances. In ICASSP 2022-2022 IEEE International Conference\\non Acoustics, Speech and Signal Processing (ICASSP) , pages 6717–6721. IEEE, 2022.\\n[560] Zishuo Zhang and Bing Yan. Smart multiple photoresponsive tongue for sensing umami, sour and bitter tastes\\nbased on tb3+ functionalized hydrogen-bonded organic frameworks. Advanced Functional Materials , 34(25):\\n2316195, 2024.\\n[561] Raunaq Bhirangi, Venkatesh Pattabiraman, Enes Erciyes, Yifeng Cao, Tess Hellebrekers, and Lerrel Pinto.\\nAnyskin: Plug-and-play skin sensing for robotic touch. arXiv preprint arXiv:2409.08276 , 2024.\\n[562] Shashank Goel, Hritik Bansal, Sumit Bhatia, Ryan Rossi, Vishwa Vinay, and Aditya Grover. Cyclip: Cyclic\\ncontrastive language-image pretraining. Advances in Neural Information Processing Systems , 35:6704–6719,\\n2022.\\n[563] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and\\nIlya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning , pages\\n8821–8831. Pmlr, 2021.\\n[564] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\\nimage generation with clip latents. arXiv preprint arXiv:2204.06125 , 1(2):3, 2022.\\n[565] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition , pages 10684–10695, 2022.\\n[566] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for\\nunified vision-language understanding and generation. In International conference on machine learning , pages\\n12888–12900. PMLR, 2022.\\n[567] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\\nwith frozen image encoders and large language models. In International conference on machine learning , pages\\n19730–19742. PMLR, 2023.\\n[568] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder\\nfor end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision , pages\\n1728–1738, 2021.\\n[569] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh,\\nYaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv preprint arXiv:2209.15352 ,\\n2022.\\n[570] Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang,\\net al. Speecht5: Unified-modal encoder-decoder pre-training for spoken language processing. arXiv preprint\\narXiv:2110.07205 , 2021.\\n[571] Prakhar Bhardwaj, Sheethal Bhat, and Andreas Maier. Enhancing zero-shot learning in medical imaging:\\nintegrating clip with advanced techniques for improved chest x-ray analysis. arXiv preprint arXiv:2503.13134 ,\\n2025.\\n219\\n[572] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma,\\nChengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao,\\nKang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong\\nWang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal\\nunderstanding. arXiv preprint arXiv:2412.10302 , 2024. URL https://arxiv.org/abs/2412.10302 .\\n[573] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards\\ndetailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424 , 2023.\\n[574] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual\\nrepresentation by alignment before projection. arXiv preprint arXiv:2311.10122 , 2023.\\n[575] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual represen-\\ntation empowers large language models with image and video understanding. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , pages 13700–13710, 2024.\\n[576] Haiyang Xu, Qinghao Ye, Xuan Wu, Ming Yan, Yuan Miao, Jiabo Ye, Guohai Xu, Anwen Hu, Yaya Shi,\\nGuangwei Xu, et al. Youku-mplug: A 10 million large-scale chinese video-language dataset for pre-training\\nand benchmarks. arXiv preprint arXiv:2306.04362 , 2023.\\n[577] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin\\nDehghan. Slowfast-llava: A strong training-free baseline for video large language models. arXiv preprint\\narXiv:2407.15841 , 2024.\\n[578] Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César Teodoro\\nMendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al. Phi-2: The surprising power of\\nsmall language models. Microsoft Research Blog , 1(3):3, 2023.\\n[579] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-\\nlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592 , 2023.\\n[580] Boxun Li, Yadong Li, Zhiyuan Li, Congyi Liu, Weilin Liu, Guowei Niu, Zheyue Tan, Haiyang Xu, Zhuyu Yao,\\nTao Yuan, et al. Megrez-omni technical report. arXiv preprint arXiv:2502.15803 , 2025.\\n[581] Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen,\\nYupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint\\narXiv:2406.11317 , 2024.\\n[582] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding,\\nLiheng Chen, Paul Pu Liang, et al. Os-atlas: A foundation action model for generalist gui agents. arXiv preprint\\narXiv:2410.23218 , 2024.\\n[583] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn,\\nChuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Grounding language\\nin robotic affordances. arXiv preprint arXiv:2204.01691 , 2022.\\n[584] Zhihao Du, Jiaming Wang, Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu,\\nZiyang Ma, et al. Lauragpt: Listen, attend, understand, and regenerate audio with gpt. arXiv preprint\\narXiv:2310.04673 , 2023.\\n[585] Sreyan Ghosh, Sonal Kumar, Ashish Seth, Chandra Kiran Reddy Evuru, Utkarsh Tyagi, S Sakshi, Oriol Nieto,\\nRamani Duraiswami, and Dinesh Manocha. Gama: A large audio-language model with advanced audio\\nunderstanding and complex reasoning abilities. arXiv preprint arXiv:2406.11768 , 2024.\\n[586] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila\\nWelihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276 , 2024.\\n[587] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien\\nVincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of\\ntokens of context. arXiv preprint arXiv:2403.05530 , 2024.\\n[588] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and\\nIshan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 15180–15190, 2023.\\n[589] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\\nZhuohan Li, Dacheng Li, Eric Xing, et al. Judging LLM-as-a-judge with mt-bench and chatbot arena. Advances\\nin Neural Information Processing Systems , 36:46595–46623, 2023.\\n220\\n[590] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan,\\nGe Zhang, Linyang Li, et al. Anygpt: Unified multimodal LLM with discrete sequence modeling. arXiv\\npreprint arXiv:2402.12226 , 2024.\\n[591] Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. Llava-3d: A simple yet effective\\npathway to empowering lmms with 3d-awareness. arXiv preprint arXiv:2409.18125 , 2024.\\n[592] Sudharshan Suresh, Haozhi Qi, Tingfan Wu, Taosha Fan, Luis Pineda, Mike Lambeta, Jitendra Malik, Mrinal\\nKalakrishnan, Roberto Calandra, Michael Kaess, et al. Neuralfeels with neural fields: Visuotactile perception\\nfor in-hand manipulation. Science Robotics , 9(96):eadl0628, 2024.\\n[593] Zhizhao Duan, Hao Cheng, Duo Xu, Xi Wu, Xiangxie Zhang, Xi Ye, and Zhen Xie. Cityllava: Efficient\\nfine-tuning for vlms in city scenario. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR) Workshops , pages 7180–7189, June 2024.\\n[594] Junfeng Fang, Zac Bi, Ruipeng Wang, Houcheng Jiang, Yuan Gao, Kun Wang, An Zhang, Jie Shi, Xiang Wang,\\nand Tat-Seng Chua. Towards neuron attributions in multi-modal large language models. Advances in Neural\\nInformation Processing Systems , 37:122867–122890, 2024.\\n[595] Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, and Wenyu Liu.\\nYou only look at one sequence: Rethinking transformer in vision through object detection. Advances in Neural\\nInformation Processing Systems , 34:26183–26197, 2021.\\n[596] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng\\nLi, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv\\npreprint arXiv:2303.16199 , 2023.\\n[597] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt,\\nand predict: A systematic survey of prompting methods in natural language processing. ACM Computing\\nSurveys , 55(9):1–35, 2023.\\n[598] Qingbin Zeng, Qinglong Yang, Shunan Dong, Heming Du, Liang Zheng, Fengli Xu, and Yong Li. Perceive,\\nreflect, and plan: Designing LLM agent for goal-directed city navigation without instructions. arXiv preprint\\narXiv:2408.04168 , 2024.\\n[599] Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theoretical\\nperspective. arXiv preprint arXiv:2011.00583 , 2020.\\n[600] Zhenbei Guo, Fuliang Li, Jiaxing Shen, Tangzheng Xie, Shan Jiang, and Xingwei Wang. Configreco: Network\\nconfiguration recommendation with graph neural networks. IEEE Network , 2023.\\n[601] Huaxiang Zhang, Yaojia Mu, Guo-Niu Zhu, and Zhongxue Gan. Insightsee: Advancing multi-agent vision-\\nlanguage models for enhanced visual understanding. arXiv preprint arXiv:2405.20795 , 2024.\\n[602] Andrew Nash, Andrew Vardy, and Dave Churchill. Herd’s eye view: Improving game AI agent learning with\\ncollaborative perception. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive\\nDigital Entertainment , volume 19, pages 306–314, 2023.\\n[603] Zhehao Zhang, Ryan Rossi, Tong Yu, Franck Dernoncourt, Ruiyi Zhang, Jiuxiang Gu, Sungchul Kim, Xiang\\nChen, Zichao Wang, and Nedim Lipka. Vipact: Visual-perception enhancement via specialized vlm agent\\ncollaboration and tool-use. arXiv preprint arXiv:2410.16400 , 2024.\\n[604] Bingchen Li, Xin Li, Yiting Lu, and Zhibo Chen. Lossagent: Towards any optimization objectives for image\\nprocessing with LLM agents. arXiv preprint arXiv:2412.04090 , 2024.\\n[605] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal.\\nDecomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406 ,\\n2022.\\n[606] Jonathon Schwartz, Rhys Newbury, Dana Kulic, and Hanna Kurniawati. Posggym: A library for decision-\\ntheoretic planning and learning in partially observable, multi-agent environments. In Proceedings of the 33rd\\nInternational Conference on Automated Planning and Scheduling (ICAPS) , 2024.\\n[607] Zhonghan Zhao, Wenhao Chai, Xuan Wang, Boyi Li, Shengyu Hao, Shidong Cao, Tian Ye, and Gaoang Wang.\\nSee and think: Embodied agent in virtual environment. In European Conference on Computer Vision , pages\\n187–204. Springer, 2025.\\n[608] Sipeng Zheng, Jiazheng Liu, Yicheng Feng, and Zongqing Lu. Steve-eye: Equipping LLM-based embodied\\nagents with visual perception in open worlds. arXiv preprint arXiv:2310.13255 , 2023.\\n221\\n[609] Difei Gao, Siyuan Hu, Zechen Bai, Qinghong Lin, and Mike Zheng Shou. Assisteditor: Multi-agent collabora-\\ntion for gui workflow automation in video creation. In Proceedings of the 32nd ACM International Conference\\non Multimedia , pages 11255–11257, 2024.\\n[610] Zixuan Wang, Yu-Wing Tai, and Chi-Keung Tang. Audio-agent: Leveraging LLMs for audio generation, editing\\nand composition. arXiv preprint arXiv:2410.03335 , 2024.\\n[611] Shuoyi Zhou, Yixuan Zhou, Weiqing Li, Jun Chen, Runchuan Ye, Weihao Wu, Zijian Lin, Shun Lei, and\\nZhiyong Wu. The codec language model-based zero-shot spontaneous style tts system for covoc challenge\\n2024. In 2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP) , pages\\n496–500. IEEE, 2024.\\n[612] Kai Li and Yi Luo. Apollo: Band-sequence modeling for high-quality audio restoration. arXiv preprint\\narXiv:2409.08514 , 2024.\\n[613] Chunhui Wang, Chang Zeng, Bowen Zhang, Ziyang Ma, Yefan Zhu, Zifeng Cai, Jian Zhao, Zhonglin Jiang,\\nand Yong Chen. Ham-tts: Hierarchical acoustic modeling for token-based zero-shot text-to-speech with model\\nand data scaling. arXiv preprint arXiv:2403.05989 , 2024.\\n[614] Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, and Zhou Yu. Exact:\\nTeaching AI agents to explore with reflective-mcts and exploratory learning. arXiv preprint arXiv:2410.02052 ,\\n2024.\\n[615] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig,\\nShuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on\\nrealistic visual web tasks. arXiv preprint arXiv:2401.13649 , 2024.\\n[616] Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen\\nLiu, Shuai Wang, et al. Spa-bench: A comprehensive benchmark for smartphone agent evaluation. In NeurIPS\\n2024 Workshop on Open-World Agents , 2024.\\n[617] Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice\\nLi, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: A dynamic benchmarking\\nenvironment for autonomous agents. arXiv preprint arXiv:2405.14573 , 2024.\\n[618] Chengyou Jia, Minnan Luo, Zhuohang Dang, Qiushi Sun, Fangzhi Xu, Junlin Hu, Tianbao Xie, and Zhiyong\\nWu. Agentstore: Scalable integration of heterogeneous agents as specialized generalist computer assistant.\\narXiv preprint arXiv:2410.18603 , 2024.\\n[619] Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and\\nJie Tang. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. arXiv preprint\\narXiv:2412.02612 , 2024.\\n[620] Mike Lambeta, Tingfan Wu, Ali Sengul, Victoria Rose Most, Nolan Black, Kevin Sawyer, Romeo Mercado,\\nHaozhi Qi, Alexander Sohn, Byron Taylor, et al. Digitizing touch with an artificial multimodal fingertip. arXiv\\npreprint arXiv:2411.02479 , 2024.\\n[621] Peiyan Zhang, Haoyang Liu, Chaozhuo Li, Xing Xie, Sunghun Kim, and Haohan Wang. Foundation model-\\noriented robustness: Robust image model evaluation with pretrained models. In ICLR , 2024.\\n[622] Lu Wang, Fangkai Yang, Chaoyun Zhang, Junting Lu, Jiaxu Qian, Shilin He, Pu Zhao, Bo Qiao, Ray Huang,\\nSi Qin, Qisheng Su, Jiayi Ye, Yudi Zhang, Jian-Guang Lou, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang,\\nand Qi Zhang. Large action models: From inception to implementation. CoRR , abs/2412.10047, 2024.\\n[623] V olker Krüger, Danica Kragic, Aleš Ude, and Christopher Geib. The meaning of action: A review on action\\nrecognition and mapping. Advanced robotics , 21(13):1473–1501, 2007.\\n[624] Nico Dosenbach, Marus Raichle, and Evan Gordon. The brain’s action-mode network. Nature reviews.\\nNeuroscience , 26, 01 2025. doi:10.1038/s41583-024-00895-x.\\n[625] Significant Gravitas. Auto-gpt: An autonomous gpt-4 experiment. https://github.com/\\nSignificant-Gravitas/Auto-GPT , 2023.\\n[626] Sirui Hong, Mingchen Xia, Jonathan Wang, Zhanghao Li, Zili Chen, Junjue He, Jiazheng Fan, Chenyu Zhou,\\nBeining Mei, et al. MetaGPT: Meta programming for multi-agent collaborative framework. In International\\nConference on Learning Representations , 2023.\\n[627] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng\\nSu, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Chatdev: Communicative agents for\\nsoftware development, 2024. URL https://arxiv.org/abs/2307.07924 .\\n222\\n[628] John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and\\nOfir Press. Swe-agent: Agent-computer interfaces enable automated software engineering, 2024. URL\\nhttps://arxiv.org/abs/2405.15793 .\\n[629] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song,\\nBowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao,\\nNiklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and\\nGraham Neubig. OpenHands: An Open Platform for AI Software Developers as Generalist Agents, 2024. URL\\nhttps://arxiv.org/abs/2407.16741 .\\n[630] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun\\nZhang, and Chi Wang. Autogen: Enabling next-gen LLM applications via multi-agent conversation framework.\\narXiv preprint arXiv:2308.08155 , 2023.\\n[631] Xiao Shao, Weifu Jiang, Fei Zuo, and Mengqing Liu. Swarmbrain: Embodied agent for real-time strategy game\\nstarcraft II via large language models. CoRR , abs/2401.17749, 2024.\\n[632] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with\\nhuman feedback. arXiv preprint arXiv:2112.09332 , 2021.\\n[633] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web\\ninteraction with grounded language agents. In Advances in Neural Information Processing Systems 35: Annual\\nConference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November\\n28 - December 9, 2022 , 2022.\\n[634] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra\\nFaust. A real-world webagent with planning, long context understanding, and program synthesis, 2024. URL\\nhttps://arxiv.org/abs/2307.12856 .\\n[635] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang.\\nMobile-agent: Autonomous multi-modal mobile device agent with visual perception. CoRR , abs/2401.16158,\\n2024.\\n[636] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent:\\nMultimodal agents as smartphone users. CoRR , abs/2312.13771, 2023.\\n[637] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin,\\nSaravan Rajmohan, Dongmei Zhang, and Qi Zhang. UFO: A ui-focused agent for windows OS interaction.\\nCoRR , abs/2402.07939, 2024.\\n[638] Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu,\\nMing Zhong, Pengcheng Yin, Sida I Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge\\ngrounding with text-to-text language models. In Proceedings of the 2022 Conference on Empirical Methods in\\nNatural Language Processing . Association for Computational Linguistics, 2022.\\n[639] Yu Gu, Xiang Deng, and Yu Su. Don’t generate, discriminate: A proposal for grounding language models to\\nreal-world environments. In ACL, 2023.\\n[640] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiy-\\ning Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin Chen-Chuan Chang, Fei Huang,\\nReynold Cheng, and Yongbin Li. Can LLM already serve as A database interface? A big bench for\\nlarge-scale database grounded text-to-sqls. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,\\nMoritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: An-\\nnual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,\\nDecember 10 - 16, 2023 , 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/\\n83fc8fab1710363050bbd1d4b8cc0021-Abstract-Datasets_and_Benchmarks.html .\\n[641] Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, SU Hongjin, ZHAOQING SUO,\\nHongcheng Gao, Wenjing Hu, Pengcheng Yin, et al. Spider 2.0: Evaluating language models on real-world\\nenterprise text-to-sql workflows. In The Thirteenth International Conference on Learning Representations ,\\n2024.\\n[642] Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong, Jie Tang, Jayanth Srinivasa, Hugo Latapie, and Yu Su.\\nMiddleware for llms: Tools are instrumental for language agents in complex environments. In EMNLP , 2024.\\n[643] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli\\nDing, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web\\nknowledge to robotic control. arXiv preprint arXiv:2307.15818 , 2023.\\n223\\n[644] Abby O’Neill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar,\\nAbraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al. Open x-embodiment: Robotic learning\\ndatasets and rt-x models. arXiv preprint arXiv:2310.08864 , 2023.\\n[645] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy\\nGroom, Karol Hausman, Brian Ichter, et al. π0: A vision-language-action flow model for general robot control.\\narXiv preprint arXiv:2410.24164 , 2024.\\n[646] Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel\\nHo, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina\\nParada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu,\\nMengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego\\nReyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally\\nJesmonth, Nikhil J. Joshi, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan,\\nByron David, Andy Zeng, and Chuyuan Kelly Fu. Do as I can, not as I say: Grounding language in robotic\\naffordances. In Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand ,\\nvolume 205 of Proceedings of Machine Learning Research , pages 287–318. PMLR, 2022.\\n[647] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. V oxposer: Composable 3d\\nvalue maps for robotic manipulation with language models. In Conference on Robot Learning, CoRL 2023, 6-9\\nNovember 2023, Atlanta, GA, USA , volume 229 of Proceedings of Machine Learning Research , pages 540–562.\\nPMLR, 2023.\\n[648] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao,\\nand Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. In Advances in\\nNeural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems\\n2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 , 2023.\\n[649] Yunxuan Li, Yibing Du, Jiageng Zhang, Le Hou, Peter Grabowski, Yeqing Li, and Eugene Ie. Improving\\nmulti-agent debate with sparse communication topology. arXiv preprint arXiv:2406.11776 , 2024.\\n[650] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-\\nsolve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Proceedings of\\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\\n2609–2634, 2023.\\n[651] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen Schmidhuber.\\nGptswarm: Language agents as optimizable graphs. In ICML . OpenReview.net, 2024.\\n[652] Haiteng Zhao, Chang Ma, Guoyin Wang, Jing Su, Lingpeng Kong, Jingjing Xu, Zhi-Hong Deng, and Hongxia\\nYang. Empowering large language model agents through action learning. arXiv preprint arXiv:2402.15809 ,\\n2024.\\n[653] Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu,\\nYizhong Zhang, et al. Cogact: A foundational vision-language-action model for synergizing cognition and\\naction in robotic manipulation. arXiv preprint arXiv:2411.19650 , 2024.\\n[654] Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quon Vuong, Jonathan Tompson, Yevgen Cheb-\\notar, Debidatta Dwibedi, and Dorsa Sadigh. Rt-h: Action hierarchies using language. arXiv preprint\\narXiv:2403.01823 , 2024.\\n[655] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov,\\nEthan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv\\npreprint arXiv:2406.09246 , 2024.\\n[656] Jinliang Zheng, Jianxiong Li, Dongxiu Liu, Yinan Zheng, Zhihao Wang, Zhonghong Ou, Yu Liu, Jingjing\\nLiu, Ya-Qin Zhang, and Xianyuan Zhan. Universal actions for enhanced embodied foundation models. arXiv\\npreprint arXiv:2501.10105 , 2025.\\n[657] Weirui Ye, Yunsheng Zhang, Haoyang Weng, Xianfan Gu, Shengjie Wang, Tong Zhang, Mengchen Wang, Pieter\\nAbbeel, and Yang Gao. Reinforcement learning with foundation priors: Let the embodied agent efficiently\\nlearn on its own. arXiv preprint arXiv:2310.02635 , 2023.\\n[658] Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, and\\nJacob Andreas. Guiding pretraining in reinforcement learning with large language models. In International\\nConference on Machine Learning , pages 8657–8677. PMLR, 2023.\\n[659] Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shridhar, Chen Bao, Yuzhe Qin, Bailin Wang, Huazhe Xu,\\nand Xiaolong Wang. Gensim: Generating robotic simulation tasks via large language models. arXiv preprint\\narXiv:2310.01361 , 2023.\\n224\\n[660] Jie Wang, Alexandros Karatzoglou, Ioannis Arapakis, and Joemon M Jose. Reinforcement learning-based\\nrecommender systems with large language models for state reward and action modeling. In Proceedings of\\nthe 47th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages\\n375–385, 2024.\\n[661] Jiajun Chai, Sicheng Li, Yuqian Fu, Dongbin Zhao, and Yuanheng Zhu. Empowering LLM agents with\\nzero-shot optimal decision-making through q-learning. In Adaptive Foundation Models: Evolving AI for\\nPersonalized and Efficient Learning , 2024.\\n[662] Jing-Cheng Pang, Si-Hang Yang, Kaiyuan Li, Jiaji Zhang, Xiong-Hui Chen, Nan Tang, and Yang Yu. Kalm:\\nKnowledgeable agents by offline reinforcement learning from large language model rollouts. In The Thirty-\\neighth Annual Conference on Neural Information Processing Systems , 2024.\\n[663] Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, and Bin Liu. Enabling intelligent\\ninteractions between an agent and an llm: A reinforcement learning approach. arXiv preprint arXiv:2306.03604 ,\\n2023.\\n[664] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke\\nZhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language\\nmodels. arXiv preprint arXiv:2310.12931 , 2023.\\n[665] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model\\nagents via hierarchical multi-turn rl, 2024b. URL https://arxiv. org/pdf/2402 , 19446, 2024.\\n[666] Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Rin Metcalf, Walter Talbott, Natalie Mackraz,\\nR Devon Hjelm, and Alexander T Toshev. Large language models as generalizable policies for embodied tasks.\\nInThe Twelfth International Conference on Learning Representations , 2023.\\n[667] Xinyu Liu, Shuyu Shen, Boyan Li, Nan Tang, and Yuyu Luo. Nl2sql-bugs: A benchmark for detecting semantic\\nerrors in nl2sql translation, 2025. URL https://arxiv.org/abs/2503.11984 .\\n[668] Xuedi Qin, Chengliang Chai, Yuyu Luo, Tianyu Zhao, Nan Tang, Guoliang Li, Jianhua Feng, Xiang Yu, and\\nMourad Ouzzani. Interactively discovering and ranking desired tuples by data exploration. VLDB J. , 31(4):\\n753–777, 2022.\\n[669] Reg Revans. ABC of action learning . Routledge, 2017.\\n[670] Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. Offline\\ntraining of language model agents with functions as learnable weights. In Forty-first International Conference\\non Machine Learning , 2024.\\n[671] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre\\nFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features\\nwithout supervision. arXiv preprint arXiv:2304.07193 , 2023.\\n[672] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image\\npre-training. In Proceedings of the IEEE/CVF international conference on computer vision , pages 11975–11986,\\n2023.\\n[673] Abby O’Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn\\nPooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and\\nrt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and\\nAutomation (ICRA) , pages 6892–6903. IEEE, 2024.\\n[674] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat,\\nIsabel Sieh, Sean Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. arXiv preprint\\narXiv:2405.05941 , 2024.\\n[675] Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction , volume 1. MIT press\\nCambridge, 1998.\\n[676] Daeyeol Lee, Hyojung Seo, and Min Whan Jung. Neural basis of reinforcement learning and decision making.\\nAnnual review of neuroscience , 35(1):287–308, 2012.\\n[677] Jiabin Liu, Chengliang Chai, Yuyu Luo, Yin Lou, Jianhua Feng, and Nan Tang. Feature augmentation with\\nreinforcement learning. In ICDE , pages 3360–3372. IEEE, 2022.\\n[678] Chengliang Chai, Kaisen Jin, Nan Tang, Ju Fan, Lianpeng Qiao, Yuping Wang, Yuyu Luo, Ye Yuan, and Guoren\\nWang. Mitigating data scarcity in supervised machine learning through reinforcement learning guided data\\ngeneration. In ICDE , pages 3613–3626. IEEE, 2024.\\n225\\n[679] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and\\nMartin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 , 2013.\\n[680] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization\\nalgorithms. arXiv preprint arXiv:1707.06347 , 2017.\\n[681] Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms. CoRR , abs/2501.12599, 2025.\\n[682] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauck-\\nhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language process-\\ning: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint\\narXiv:2210.01241 , 2022.\\n[683] Jian Hu, Li Tao, June Yang, and Chandler Zhou. Aligning language models with offline learning from human\\nfeedback. arXiv preprint arXiv:2308.12050 , 2023.\\n[684] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning , 8:279–292, 1992.\\n[685] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms\\nto reason and leverage search engines with reinforcement learning, 2025. URL https://arxiv.org/abs/\\n2503.09516 .\\n[686] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-\\nRong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning, 2025. URL\\nhttps://arxiv.org/abs/2503.05592 .\\n[687] Zihan Wang*, Kangrui Wang*, Qineng Wang*, Pingyue Zhang*, Linjie Li*, Zhengyuan Yang, Kefan Yu,\\nMinh Nhat Nguyen, Monica Lam, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin\\nChoi, and Manling Li. Training agents by reinforcing reasoning, 2025. URL https://github.com/\\nZihanWang314/ragen .\\n[688] OpenManus-RL Team. Openmanus-rl: Open platform for generalist llm reasoning agents with rl optimization,\\n2025. URL https://github.com/OpenManus/OpenManus-RL .\\n[689] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke\\nZettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach them-\\nselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems , 2023. URL\\nhttps://openreview.net/forum?id=Yacmpz84TH .\\n[690] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill\\nQian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu,\\nand Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023. URL\\nhttps://arxiv.org/abs/2307.16789 .\\n[691] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected\\nwith massive APIs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems , 2024.\\nURL https://openreview.net/forum?id=tBRNC6YemY .\\n[692] Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen language models\\nwith massive tools via tool embeddings. In Advances in Neural Information Processing Systems 36: Annual\\nConference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December\\n10 - 16, 2023 , 2023.\\n[693] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large\\nlanguage model to use tools via self-instruction. In Alice Oh, Tristan Naumann, Amir Globerson, Kate\\nSaenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems\\n36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,\\nUSA, December 10 - 16, 2023 , 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/\\ne393677793767624f2821cec8bdd02f1-Abstract-Conference.html .\\n[694] Yu Du, Fangyun Wei, and Hongyang Zhang. Anytool: Self-reflective, hierarchical agents for large-scale api\\ncalls, 2024. URL https://arxiv.org/abs/2402.04253 .\\n[695] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon\\nRusinkiewicz, and Thomas Funkhouser. Tidybot: personalized robot assistance with large language models.\\nAutonomous Robots , 47(8):1087–1102, November 2023. ISSN 1573-7527. doi:10.1007/s10514-023-10139-z.\\nURL http://dx.doi.org/10.1007/s10514-023-10139-z .\\n[696] Chang Qi, Feng Jiang, and Shu Yang. Advanced honeycomb designs for improving mechanical\\nproperties: A review. Composites Part B: Engineering , 227:109393, 2021. ISSN 1359-8368.\\ndoi:https://doi.org/10.1016/j.compositesb.2021.109393.\\n226\\n[697] Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and Philippe Schwaller.\\nAugmenting large language models with chemistry tools. Nat. Mac. Intell. , 6(5):525–535, 2024.\\n[698] Huajun Chen, Keyan Ding, Jing Yu, Junjie Huang, Yuchen Yang, and Qiang Zhang. Scitoolagent: A knowledge\\ngraph-driven scientific agent for multi-tool integration. In ICLR , 2025.\\n[699] Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Yixin Cao,\\nand Aixin Sun. Sciagent: Tool-augmented language models for scientific reasoning. In Proceedings of the 2024\\nConference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November\\n12-16, 2024 , pages 15701–15736, 2024.\\n[700] Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A Rossi, Somdeb Sarkhel, and\\nChao Zhang. Toolchain*: Efficient action space navigation in large language models with a* search. arXiv\\npreprint arXiv:2310.13227 , 2023.\\n[701] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham\\nNeubig. PAL: program-aided language models. In International Conference on Machine Learning, ICML 2023,\\n23-29 July 2023, Honolulu, Hawaii, USA , volume 202, pages 10764–10799, 2023.\\n[702] Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers.\\narXiv preprint arXiv:2305.17126 , 2023.\\n[703] Cheng Qian, Chi Han, Yi Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. CREATOR: Tool creation for disentangling\\nabstract and concrete reasoning of large language models. In The 2023 Conference on Empirical Methods in\\nNatural Language Processing , 2023. URL https://openreview.net/forum?id=aCHq10rQiH .\\n[704] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao,\\nZiyue Li, Xingyu Zeng, and Rui Zhao. Tptu: Large language model-based ai agents for task planning and tool\\nusage, 2023. URL https://arxiv.org/abs/2308.03427 .\\n[705] Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong\\nWang, et al. Webcpm: Interactive web search for chinese long-form question answering. In Proceedings of\\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\\n8968–8988, 2023.\\n[706] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen\\nMen, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. In The Twelfth International Conference on\\nLearning Representations , 2024.\\n[707] Xuanhe Zhou, Guoliang Li, Zhaoyan Sun, Zhiyuan Liu, Weize Chen, Jianming Wu, Jiesi Liu, Ruohang\\nFeng, and Guoyang Zeng. D-bot: Database diagnosis system using large language models, 2023. URL\\nhttps://arxiv.org/abs/2312.01454 .\\n[708] Xinyu Liu, Shuyu Shen, Boyan Li, Peixian Ma, Runzhi Jiang, Yuxin Zhang, Ju Fan, Guoliang Li, Nan Tang,\\nand Yuyu Luo. A survey of NL2SQL with large language models: Where are we, and where are we going?,\\n2025. URL https://arxiv.org/abs/2408.05109 .\\n[709] Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. The dawn of natural language to SQL: are\\nwe fully ready? Proc. VLDB Endow. , 17(11):3318–3331, 2024.\\n[710] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code\\nactions elicit better LLM agents. arXiv preprint arXiv:2402.01030 , 2024.\\n[711] Xuedi Qin, Yuyu Luo, Nan Tang, and Guoliang Li. Making data visualization more efficient and effective: a\\nsurvey. VLDB J. , 29(1):93–117, 2020.\\n[712] Yuyu Luo, Xuedi Qin, Nan Tang, and Guoliang Li. Deepeye: Towards automatic data visualization. In ICDE ,\\npages 101–112. IEEE Computer Society, 2018.\\n[713] Xuedi Qin, Chengliang Chai, Yuyu Luo, Nan Tang, and Guoliang Li. Interactively discovering and ranking\\ndesired tuples without writing SQL queries. In SIGMOD Conference , pages 2745–2748. ACM, 2020.\\n[714] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Xuanhe Zhou, Yufei\\nHuang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun\\nZhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang\\nZhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian\\nSun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Guoliang Li, Zhiyuan Liu, and Maosong\\nSun. Tool learning with foundation models. ACM Comput. Surv. , 57(4), December 2024. ISSN 0360-0300.\\ndoi:10.1145/3704435. URL https://doi.org/10.1145/3704435 .\\n227\\n[715] Sadra Zargarzadeh, Maryam Mirzaei, Yafei Ou, and Mahdi Tavakoli. From decision to action in surgical\\nautonomy: Multi-modal large language models for robot-assisted blood suction. IEEE Robotics and Automation\\nLetters , 10(3):2598–2605, March 2025. ISSN 2377-3774. doi:10.1109/lra.2025.3535184. URL http:\\n//dx.doi.org/10.1109/LRA.2025.3535184 .\\n[716] Zhenjie Yang, Xiaosong Jia, Hongyang Li, and Junchi Yan. Llm4drive: A survey of large language models for\\nautonomous driving, 2023.\\n[717] Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, and Yue Wang. A language agent for autonomous driving.\\narXiv preprint arXiv:2311.10813 , 2023.\\n[718] Sherwood L Washburn. Tools and human evolution. Scientific American , 203(3):62–75, 1960.\\n[719] Nan Tang, Chenyu Yang, Ju Fan, Lei Cao, Yuyu Luo, and Alon Y . Halevy. Verifai: Verified generative AI. In\\nCIDR . www.cidrdb.org, 2024.\\n[720] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. A survey on large language\\nmodel (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing , 4(2):100211,\\nJune 2024. ISSN 2667-2952. doi:10.1016/j.hcc.2024.100211. URL http://dx.doi.org/10.1016/j.hcc.\\n2024.100211 .\\n[721] Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang, Nicholas Roy, and Chuchu Fan. Prompt optimiza-\\ntion in multi-step tasks (promst): Integrating human feedback and preference alignment. arXiv preprint\\narXiv:2402.08702 , 2024.\\n[722] Yurong Wu, Yan Gao, Bin Benjamin Zhu, Zineng Zhou, Xiaodi Sun, Sheng Yang, Jian-Guang Lou, Zhiming\\nDing, and Linjun Yang. StraGo: Harnessing strategic guidance for prompt optimization. In Yaser Al-Onaizan,\\nMohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP\\n2024 , pages 10043–10061, Miami, Florida, USA, November 2024. Association for Computational Linguistics.\\ndoi:10.18653/v1/2024.findings-emnlp.588. URL https://aclanthology.org/2024.findings-emnlp.\\n588.\\n[723] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu\\nYang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. In\\nICLR . OpenReview.net, 2024.\\n[724] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan,\\nSaiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. Dspy: Compiling declarative language\\nmodel calls into self-improving pipelines. arXiv preprint arXiv:2310.03714 , 2023.\\n[725] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic LLM-agent network: An LLM-agent\\ncollaboration framework with agent team optimization. arXiv preprint arXiv:2310.02170 , 2023.\\n[726] Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai\\nWang, Xiaohua Xu, Ningyu Zhang, et al. Symbolic learning enables self-evolving agents. arXiv preprint\\narXiv:2406.18532 , 2024.\\n[727] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected\\nwith massive apis. arXiv preprint arXiv:2305.15334 , 2023.\\n[728] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou.\\nTextgrad: Automatic “differentiation” via text. arXiv preprint arXiv:2406.07496 , 2024.\\n[729] Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, and Qingyun Wu. Stateflow: Enhancing LLM task-\\nsolving through state-driven workflows. In First Conference on Language Modeling , 2024. URL https:\\n//openreview.net/forum?id=3nTbuygoop .\\n[730] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large\\nlanguage models as optimizers. In The Twelfth International Conference on Learning Representations , 2024.\\nURL https://openreview.net/forum?id=Bb4VGOWELI .\\n[731] Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. Self-taught optimizer (stop): Recursively\\nself-improving code generation. arXiv preprint arXiv:2310.02304 , 2023.\\n[732] Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel. Prompt-\\nbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797 , 2023.\\n[733] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context?\\na case study of simple function classes. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun\\nCho, editors, Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/\\nforum?id=flNZJ2eOet .\\n228\\n[734] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is\\nin-context learning? investigations with linear models. In The Eleventh International Conference on Learning\\nRepresentations , 2023. URL https://openreview.net/forum?id=0g0X4H8yN4I .\\n[735] Deqing Fu, Tian qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn to achieve second-order convergence\\nrates for in-context linear regression. In The Thirty-eighth Annual Conference on Neural Information Processing\\nSystems , 2024. URL https://openreview.net/forum?id=L8h6cozcbn .\\n[736] Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce,\\nCraig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L Turner, Callum McDougall,\\nMonte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn,\\nShan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Extracting interpretable features from\\nclaude 3 sonnet. Transformer Circuits Thread , 2024. URL https://transformer-circuits.pub/2024/\\nscaling-monosemanticity/index.html .\\n[737] Juhao Liang, Ziwei Wang, Zhuoheng Ma, Jianquan Li, Zhiyi Zhang, Xiangbo Wu, and Benyou Wang. Online\\ntraining of large language models: Learn while chatting. arXiv preprint arXiv:2403.04790 , 2024.\\n[738] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from\\nfeedback with language models. Advances in Neural Information Processing Systems , 36, 2024.\\n[739] Zhiruo Wang, Daniel Fried, and Graham Neubig. Trove: Inducing verifiable and efficient toolboxes for solving\\nprogrammatic tasks. arXiv preprint arXiv:2401.12869 , 2024.\\n[740] Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara\\nPrabhakar, Haolin Chen, et al. xlam: A family of large action models to empower ai agent systems. arXiv\\npreprint arXiv:2409.03215 , 2024.\\n[741] Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. arXiv preprint arXiv:2408.08435 ,\\n2024.\\n[742] Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can LLMs Generate Novel Research Ideas? A Large-Scale\\nHuman Study with 100+ NLP Researchers, September 2024.\\n[743] Alireza Ghafarollahi and Markus J. Buehler. SciAgents: Automating Scientific Discovery Through Bioinspired\\nMulti-Agent Intelligent Graph Reasoning. Advanced Materials , n/a(n/a):2413523, December 2024. ISSN\\n1521-4095. doi:10.1002/adma.202413523.\\n[744] Ievgeniia A. Tiukova, Daniel Brunnsåker, Erik Y . Bjurström, Alexander H. Gower, Filip Kronström, Gabriel K.\\nReder, Ronald S. Reiserer, Konstantin Korovin, Larisa B. Soldatova, John P. Wikswo, and Ross D. King.\\nGenesis: Towards the Automation of Systems Biology Research, September 2024.\\n[745] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI Scientist: Towards\\nFully Automated Open-Ended Scientific Discovery, September 2024.\\n[746] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and\\nEmad Barsoum. Agent laboratory: Using LLM agents as research assistants. arXiv preprint arXiv:2501.04227 ,\\n2025.\\n[747] Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou, Pan Lu,\\nZhuosheng Zhang, Yilun Zhao, Arman Cohan, and Mark Gerstein. ChemAgent: Self-updating Library in Large\\nLanguage Models Improves Chemical Reasoning, January 2025.\\n[748] Malcolm Sim, Mohammad Ghazi Vakili, Felix Strieth-Kalthoff, Han Hao, Riley J. Hickman, Santiago\\nMiret, Sergio Pablo-García, and Alán Aspuru-Guzik. ChemOS 2.0: An orchestration architecture for chem-\\nical self-driving laboratories. Matter , 7(9):2959–2977, September 2024. ISSN 2590-2393, 2590-2385.\\ndoi:10.1016/j.matt.2024.04.022.\\n[749] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky,\\nFelix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang,\\nKatherine Chou, Avinatan Hassidim, Burak Gokturk, Amin Vahdat, Pushmeet Kohli, Yossi Matias, Andrew\\nCarroll, Kavita Kulkarni, Nenad Tomasev, Yuan Guan, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee,\\nTiago R. D. Costa, José R. Penadés, Gary Peltz, Yunhan Xu, Annalisa Pawlosky, Alan Karthikesalingam, and\\nVivek Natarajan. Towards an AI co-scientist, February 2025.\\n[750] Tianwei Dai, Sriram Vijayakrishnan, Filip T. Szczypi ´nski, Jean-François Ayme, Ehsan Simaei, Thomas\\nFellowes, Rob Clowes, Lyubomir Kotopanov, Caitlin E. Shields, Zhengxue Zhou, John W. Ward, and Andrew I.\\nCooper. Autonomous mobile robots for exploratory synthetic chemistry. Nature , pages 1–8, November 2024.\\nISSN 1476-4687. doi:10.1038/s41586-024-08173-7.\\n229\\n[751] Felix Strieth-Kalthoff, Han Hao, Vandana Rathore, Joshua Derasp, Théophile Gaudin, Nicholas H. Angello,\\nMartin Seifrid, Ekaterina Trushina, Mason Guy, Junliang Liu, Xun Tang, Masashi Mamada, Wesley Wang,\\nTuul Tsagaantsooj, Cyrille Lavigne, Robert Pollice, Tony C. Wu, Kazuhiro Hotta, Leticia Bodo, Shangyu\\nLi, Mohammad Haddadnia, Agnieszka Wołos, Rafał Roszak, Cher Tian Ser, Carlota Bozal-Ginesta, Riley J.\\nHickman, Jenya Vestfrid, Andrés Aguilar-Granda, Elena L. Klimareva, Ralph C. Sigerson, Wenduan Hou,\\nDaniel Gahler, Slawomir Lach, Adrian Warzybok, Oleg Borodin, Simon Rohrbach, Benjamin Sanchez-\\nLengeling, Chihaya Adachi, Bartosz A. Grzybowski, Leroy Cronin, Jason E. Hein, Martin D. Burke, and Alán\\nAspuru-Guzik. Delocalized, asynchronous, closed-loop discovery of organic laser emitters. Science , 384(6697):\\neadk9227, May 2024. doi:10.1126/science.adk9227.\\n[752] Kyle Swanson, Wesley Wu, Nash L Bulaong, John E Pak, and James Zou. The virtual lab: Ai agents design\\nnew sars-cov-2 nanobodies with experimental validation. bioRxiv , pages 2024–11, 2024.\\n[753] Trieu H. Trinh, Yuhuai Wu, Quoc V . Le, He He, and Thang Luong. Solving olympiad geometry without human\\ndemonstrations. Nature , 625(7995):476–482, January 2024. ISSN 1476-4687. doi:10.1038/s41586-023-06747-\\n5.\\n[754] Haoyang Liu, Yijiang Li, Jinglin Jian, Yuxuan Cheng, Jianrong Lu, Shuyi Guo, Jinglei Zhu, Mianchen Zhang,\\nMiantong Zhang, and Haohan Wang. Toward a Team of AI-made Scientists for Scientific Discovery from Gene\\nExpression Data, February 2024.\\n[755] Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin\\nWang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi\\nWang, Xiangru Tang, Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, and\\nChenglin Wu. Data Interpreter: An LLM Agent For Data Science, March 2024.\\n[756] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural\\nnetworks. Advances in neural information processing systems , 25, 2012.\\n[757] David G Lowe. Distinctive image features from scale-invariant keypoints. International journal of computer\\nvision , 60:91–110, 2004.\\n[758] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In 2005 IEEE computer\\nsociety conference on computer vision and pattern recognition (CVPR’05) , volume 1, pages 886–893. Ieee,\\n2005.\\n[759] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. Journal of\\nMachine Learning Research , 20(55):1–21, 2019. URL http://jmlr.org/papers/v20/18-598.html .\\n[760] Jiabin Liu, Fu Zhu, Chengliang Chai, Yuyu Luo, and Nan Tang. Automatic data acquisition for deep learning.\\nProc. VLDB Endow. , 14(12):2739–2742, 2021.\\n[761] Yuyu Luo, Nan Tang, Guoliang Li, Chengliang Chai, Wenbo Li, and Xuedi Qin. Synthesizing natural language\\nto visualization (NL2VIS) benchmarks from NL2SQL benchmarks. In SIGMOD Conference , pages 1235–1247.\\nACM, 2021.\\n[762] Jiawei Tang, Yuyu Luo, Mourad Ouzzani, Guoliang Li, and Hongyang Chen. Sevi: Speech-to-visualization\\nthrough neural machine translation. In SIGMOD Conference , pages 2353–2356. ACM, 2022.\\n[763] Bernd Bischl, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, Theresa\\nUllmann, Marc Becker, Anne-Laure Boulesteix, et al. Hyperparameter optimization: Foundations, algorithms,\\nbest practices, and open challenges. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery ,\\n13(2):e1484, 2023.\\n[764] Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola.\\nAutogluon-tabular: Robust and accurate automl for structured data. arXiv preprint arXiv:2003.06505 , 2020.\\n[765] Chi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. Flaml: A fast and lightweight automl library.\\nProceedings of Machine Learning and Systems , 3:434–447, 2021.\\n[766] Shaokun Zhang, Feiran Jia, Chi Wang, and Qingyun Wu. Targeted hyperparameter optimization with lexico-\\ngraphic preferences over multiple objectives. In The Eleventh international conference on learning representa-\\ntions , 2023.\\n[767] Shaokun Zhang, Yiran Wu, Zhonghua Zheng, Qingyun Wu, and Chi Wang. Hypertime: Hyperparameter\\noptimization for combating temporal distribution shifts. In Proceedings of the 32nd ACM International\\nConference on Multimedia , pages 4610–4619, 2024.\\n[768] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, and Xin Wang. A\\ncomprehensive survey of neural architecture search: Challenges and solutions. ACM Computing Surveys\\n(CSUR) , 54(4):1–34, 2021.\\n230\\n[769] Xiawu Zheng, Chenyi Yang, Shaokun Zhang, Yan Wang, Baochang Zhang, Yongjian Wu, Yunsheng Wu,\\nLing Shao, and Rongrong Ji. Ddpnas: Efficient neural architecture search via dynamic distribution pruning.\\nInternational Journal of Computer Vision , 131(5):1234–1249, 2023.\\n[770] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating\\nerrors. nature , 323(6088):533–536, 1986.\\n[771] Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. Tempera: Test-time\\nprompting via reinforcement learning. arXiv preprint arXiv:2211.11890 , 2022.\\n[772] Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi Fung, Hao Peng, and Heng Ji. CRAFT: Customizing LLMs by\\ncreating and retrieving from specialized toolsets. In 12th International Conference on Learning Representations ,\\n2024. URL https://openreview.net/forum?id=G0vdDSt9XM .\\n[773] Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin\\nCheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation. arXiv preprint\\narXiv:2410.10762 , 2024.\\n[774] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba.\\nLarge language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910 , 2022.\\n[775] Wenyi Wang, Hisham A Alyahya, Dylan R Ashley, Oleg Serikov, Dmitrii Khizbullin, Francesco Faccio, and\\nJürgen Schmidhuber. How to correctly do semantic backpropagation on language-based agentic systems. arXiv\\npreprint arXiv:2412.03624 , 2024.\\n[776] Xuanchang Zhang, Zhuosheng Zhang, and Hai Zhao. Glape: Gold label-agnostic prompt evaluation and\\noptimization for large language model. CoRR , abs/2402.02408, 2024.\\n[777] Xiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-Kiong Ng, Patrick Jaillet, and Bryan Kian Hsiang Low.\\nPrompt optimization with human feedback. arXiv preprint arXiv:2405.17346 , 2024.\\n[778] Jinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Fengwei Teng, Jinhao Tu, Xinbing Liang, Sirui Hong, Chenglin Wu,\\nand Yuyu Luo. Self-supervised prompt optimization. arXiv preprint arXiv:2502.06855 , 2025.\\n[779] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimiza-\\ntion with “gradient descent” and beam search. In EMNLP , pages 7957–7968. Association for Computational\\nLinguistics, 2023.\\n[780] Peiyan Zhang, Haibo Jin, Leyang Hu, Xinnuo Li, Liying Kang, Man Luo, Yangqiu Song, and Haohan Wang.\\nRevolve: Optimizing ai systems by tracking response evolution in textual optimization. arXiv preprint\\narXiv:2412.03092 , 2024.\\n[781] Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun\\nLiu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, et al. Agent-as-a-judge: Evaluate agents with\\nagents. arXiv preprint arXiv:2410.10934 , 2024.\\n[782] Cilin Yan, Jingyun Wang, Lin Zhang, Ruihui Zhao, Xiaopu Wu, Kai Xiong, Qingsong Liu, Guoliang Kang,\\nand Yangyang Kang. Efficient and accurate prompt optimization: the benefit of memory in exemplar-guided\\nreflection. CoRR , abs/2411.07446, 2024.\\n[783] Han Zhou, Xingchen Wan, Yinhong Liu, Nigel Collier, Ivan Vulic, and Anna Korhonen. Fairer preferences\\nelicit improved human-aligned large language model judgments. In EMNLP , pages 1241–1252. Association for\\nComputational Linguistics, 2024.\\n[784] Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Ö. Arik, and Tomas Pfister. Better zero-shot reasoning with\\nself-adaptive prompting. In ACL (Findings) , pages 3493–3514. Association for Computational Linguistics,\\n2023.\\n[785] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts\\nand where to find them: Overcoming few-shot prompt order sensitivity. In ACL (1) , pages 8086–8098.\\nAssociation for Computational Linguistics, 2022.\\n[786] Tal Ridnik, Dedy Kredo, and Itamar Friedman. Code generation with alphacodium: From prompt engineering\\nto flow engineering. CoRR , abs/2401.08500, 2024.\\n[787] Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, Lei Bai, and Xiang Wang. Multi-agent architecture\\nsearch via agentic supernet. arXiv preprint arXiv:2502.04180 , 2025.\\n[788] Yinjie Wang, Ling Yang, Guohao Li, Mengdi Wang, and Bryon Aragam. Scoreflow: Mastering LLM agent\\nworkflows via score-based preference optimization. arXiv preprint arXiv:2502.04306 , 2025.\\n231\\n[789] Jon Saad-Falcon, Adrian Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, Etash Guha,\\nE. Kelly Buchanan, Mayee Chen, Neel Guha, Christopher Ré, and Azalia Mirhoseini. Archon: An architecture\\nsearch framework for inference-time techniques, 2024. URL https://arxiv.org/abs/2409.15254 .\\n[790] Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, and Yun-Nung Chen. Let me\\nspeak freely? A study on the impact of format restrictions on performance of large language models. CoRR ,\\nabs/2408.02442, 2024.\\n[791] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yong-\\nbin Li. Api-bank: A comprehensive benchmark for tool-augmented LLMs. arXiv preprint arXiv:2304.08244 ,\\n2023.\\n[792] Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool manipulation\\ncapability of open-source large language models. arXiv preprint arXiv:2305.16504 , 2023.\\n[793] Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and\\nYang Liu. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models,\\n2024.\\n[794] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. Toolalpaca:\\nGeneralized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301 ,\\n2023.\\n[795] Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J\\nMaddison, and Tatsunori Hashimoto. Identifying the risks of lm agents with an lm-emulated sandbox. arXiv\\npreprint arXiv:2309.15817 , 2023.\\n[796] Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan,\\nNeil Zhenqiang Gong, and Lichao Sun. Metatool benchmark for large language models: Deciding whether to\\nuse tools and which to use, 2024. URL https://arxiv.org/abs/2310.03128 .\\n[797] Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou,\\nQi Zhang, Tao Gui, et al. Tooleyes: Fine-grained evaluation for tool learning capabilities of large language\\nmodels in real-world scenarios. arXiv preprint arXiv:2401.00741 , 2024.\\n[798] Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ-bench: A benchmark for tool-agent-user\\ninteraction in real-world domains, 2024. URL https://arxiv.org/abs/2406.12045 .\\n[799] Xunjian Yin, Xinyi Wang, Liangming Pan, Xiaojun Wan, and William Yang Wang. G \\\\\" odel agent: A\\nself-referential agent framework for recursive self-improvement. arXiv preprint arXiv:2410.04444 , 2024.\\n[800] Han Zhou, Xingchen Wan, Ruoxi Sun, Hamid Palangi, Shariq Iqbal, Ivan Vuli ´c, Anna Korhonen, and Sercan Ö.\\nArık. Multi-agent design: Optimizing agents with better prompts and topologies, 2025. URL https://arxiv.\\norg/abs/2502.02533 .\\n[801] Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM\\nreview , 60(2):223–311, 2018.\\n[802] James C Spall. Introduction to stochastic search and optimization: estimation, simulation, and control . John\\nWiley & Sons, 2005.\\n[803] Peter I Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811 , 2018.\\n[804] Nikolaus Hansen. The cma evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772 , 2016.\\n[805] Hao-Jun Michael Shi, Melody Qiming Xuan, Figen Oztoprak, and Jorge Nocedal. On the numerical performance\\nof finite-difference-based methods for derivative-free optimization. Optimization Methods and Software , 38(2):\\n289–311, 2023.\\n[806] OpenAI. Openai o3-mini system card, 2025. URL https://openai.com/index/openai-o3-mini/ .\\n[Online; accessed 2025-02-02].\\n[807] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug.\\narXiv preprint arXiv:2304.05128 , 2023.\\n[808] Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. Prompt engineering a prompt engineer.\\narXiv preprint arXiv:2311.05661 , 2023.\\n[809] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models’ sensitivity to\\nspurious features in prompt design or: How i learned to start worrying about prompt formatting. arXiv preprint\\narXiv:2310.11324 , 2023.\\n232\\n[810] Ruotian Ma, Xiaolei Wang, Xin Zhou, Jian Li, Nan Du, Tao Gui, Qi Zhang, and Xuanjing Huang. Are large\\nlanguage models good prompt optimizers? arXiv preprint arXiv:2402.02101 , 2024.\\n[811] Ting-Yun Chang and Robin Jia. Data curation alone can stabilize in-context learning. arXiv preprint\\narXiv:2212.10378 , 2022.\\n[812] Tai Nguyen and Eric Wong. In-context example selection with influences. arXiv preprint arXiv:2302.11042 ,\\n2023.\\n[813] Ching-An Cheng, Allen Nie, and Adith Swaminathan. Trace is the next autodiff: Generative optimization with\\nrich feedback, execution traces, and LLMs. In The Thirty-eighth Annual Conference on Neural Information\\nProcessing Systems , 2024. URL https://openreview.net/forum?id=rYs2Dmn9tD .\\n[814] Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014.\\n[815] I Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017.\\n[816] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint\\narXiv:2101.00190 , 2021.\\n[817] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning.\\narXiv preprint arXiv:2104.08691 , 2021.\\n[818] Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li, Yong Lin, Xiao Zhou, and Tong Zhang. Black-box\\nprompt learning for pre-trained language models. arXiv preprint arXiv:2201.08531 , 2022.\\n[819] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.\\nRethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837 ,\\n2022.\\n[820] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang,\\nDenny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846 ,\\n2023.\\n[821] Krista Opsahl-Ong, Michael J Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia, and\\nOmar Khattab. Optimizing instructions and demonstrations for multi-stage language model programs. In\\nYaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on\\nEmpirical Methods in Natural Language Processing , pages 9340–9366, Miami, Florida, USA, November\\n2024. Association for Computational Linguistics. doi:10.18653/v1/2024.emnlp-main.525. URL https:\\n//aclanthology.org/2024.emnlp-main.525 .\\n[822] Shuhei Watanabe. Tree-structured parzen estimator: Understanding its algorithm components and their roles\\nfor better empirical performance. arXiv preprint arXiv:2304.11127 , 2023.\\n[823] Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang, Nicholas Roy, and Chuchu Fan. PRompt optimization\\nin multi-step tasks (PROMST): Integrating human feedback and heuristic-based sampling. In Yaser Al-Onaizan,\\nMohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in\\nNatural Language Processing , pages 3859–3920, Miami, Florida, USA, November 2024. Association for\\nComputational Linguistics. doi:10.18653/v1/2024.emnlp-main.226. URL https://aclanthology.org/\\n2024.emnlp-main.226 .\\n[824] Brandon Amos et al. Tutorial on amortized optimization. Foundations and Trends ®in Machine Learning , 16\\n(5):592–732, 2023.\\n[825] Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, and Yuandong Tian. Advprompter: Fast\\nadaptive adversarial prompting for LLMs. arXiv preprint arXiv:2404.16873 , 2024.\\n[826] Ollie Liu, Deqing Fu, Dani Yogatama, and Willie Neiswanger. DeLLMa: Decision making under uncertainty\\nwith large language models. In The Thirteenth International Conference on Learning Representations , 2025.\\nURL https://openreview.net/forum?id=Acvo2RGSCy .\\n[827] Wenyue Hua, Ollie Liu, Lingyao Li, Alfonso Amayuelas, Julie Chen, Lucas Jiang, Mingyu Jin, Lizhou Fan,\\nFei Sun, William Wang, et al. Game-theoretic llm: Agent workflow for negotiation games. arXiv preprint\\narXiv:2411.05990 , 2024.\\n[828] Sicheng Zhu, Brandon Amos, Yuandong Tian, Chuan Guo, and Ivan Evtimov. Advprefix: An objective for\\nnuanced LLM jailbreaks. arXiv preprint arXiv:2412.10321 , 2024.\\n[829] Luke Metz, C Daniel Freeman, James Harrison, Niru Maheswaranathan, and Jascha Sohl-Dickstein. Practical\\ntradeoffs between memory, compute, and performance in learned optimizers. In Conference on Lifelong\\nLearning Agents , pages 142–164. PMLR, 2022.\\n233\\n[830] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers.\\narXiv preprint arXiv:1807.03819 , 2018.\\n[831] Laurent Hascoet and Mauricio Araya-Polo. Enabling user-driven checkpointing strategies in reverse-mode\\nautomatic differentiation. arXiv preprint cs/0606042 , 2006.\\n[832] Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-propagation for bilevel\\noptimization. In The 22nd International Conference on Artificial Intelligence and Statistics , pages 1723–1732.\\nPMLR, 2019.\\n[833] Johannes V on Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev,\\nAndrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In An-\\ndreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scar-\\nlett, editors, Proceedings of the 40th International Conference on Machine Learning , volume 202 of Pro-\\nceedings of Machine Learning Research , pages 35151–35174. PMLR, 23–29 Jul 2023. URL https:\\n//proceedings.mlr.press/v202/von-oswald23a.html .\\n[834] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as\\nimplicit bayesian inference. In International Conference on Learning Representations , 2022. URL https:\\n//openreview.net/forum?id=RdJVFCHjUMI .\\n[835] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\\nBosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint\\narXiv:2206.07682 , 2022.\\n[836] Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in\\nthe wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593 , 2022.\\n[837] Michael Hanna, Ollie Liu, and Alexandre Variengien. How does gpt-2 compute greater-than. Interpreting\\nmathematical abilities in a pre-trained language model , 2:11, 2023.\\n[838] Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga-Alonso. To-\\nwards automated circuit discovery for mechanistic interpretability. Advances in Neural Information Processing\\nSystems , 36:16318–16352, 2023.\\n[839] Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma,\\nJános Kramár, Anca Dragan, Rohin Shah, and Neel Nanda. Gemma scope: Open sparse autoencoders\\neverywhere all at once on gemma 2. arXiv preprint arXiv:2408.05147 , 2024.\\n[840] Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike,\\nand Jeffrey Wu. Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093 , 2024.\\n[841] Samuel Marks, Can Rager, Eric J Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse\\nfeature circuits: Discovering and editing interpretable causal graphs in language models. arXiv preprint\\narXiv:2403.19647 , 2024.\\n[842] Cem Anil, Esin Durmus, Nina Rimsky, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Meg\\nTong, Jesse Mu, Daniel J Ford, et al. Many-shot jailbreaking. In The Thirty-eighth Annual Conference on\\nNeural Information Processing Systems , 2024.\\n[843] Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse,\\nSteven Stenberg Hansen, Angelos Filos, Ethan Brooks, maxime gazeau, Himanshu Sahni, Satinder Singh, and\\nV olodymyr Mnih. In-context reinforcement learning with algorithm distillation. In The Eleventh International\\nConference on Learning Representations , 2023. URL https://openreview.net/forum?id=hy0a5MMPUv .\\n[844] Allen Nie, Yi Su, Bo Chang, Jonathan N Lee, Ed H Chi, Quoc V Le, and Minmin Chen. Evolve: Evaluating\\nand optimizing LLMs for exploration. arXiv preprint arXiv:2410.06238 , 2024.\\n[845] Akshay Krishnamurthy, Keegan Harris, Dylan J Foster, Cyril Zhang, and Aleksandrs Slivkins. Can large\\nlanguage models explore in-context? arXiv preprint arXiv:2403.15371 , 2024.\\n[846] Giovanni Monea, Antoine Bosselut, Kianté Brantley, and Yoav Artzi. Llms are in-context reinforcement\\nlearners. In ICLR , 2024.\\n[847] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John\\nSchulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The Twelfth International Conference\\non Learning Representations , 2023.\\n[848] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative\\nagents for “mind” exploration of large language model society. Advances in Neural Information Processing\\nSystems , 36:51991–52008, 2023.\\n234\\n[849] Collin Zhang, John X Morris, and Vitaly Shmatikov. Extracting prompts by inverting LLM outputs. arXiv\\npreprint arXiv:2405.15012 , 2024.\\n[850] Hao Xiang, Bowen Yu, Hongyu Lin, Keming Lu, Yaojie Lu, Xianpei Han, Le Sun, Jingren Zhou, and Junyang\\nLin. Aligning large language models via self-steering optimization. arXiv preprint arXiv:2410.17131 , 2024.\\n[851] Yizhang Zhu, Shiyin Du, Boyan Li, Yuyu Luo, and Nan Tang. Are large language models good statisticians?\\nInNeurIPS , 2024.\\n[852] Tianqi Luo, Chuhan Huang, Leixian Shen, Boyan Li, Shuyu Shen, Wei Zeng, Nan Tang, and Yuyu Luo.\\nnvbench 2.0: A benchmark for natural language to visualization under ambiguity, 2025. URL https://arxiv.\\norg/abs/2503.12880 .\\n[853] Teng Lin, Yizhang Zhu, Yuyu Luo, and Nan Tang. Srag: Structured retrieval-augmented generation for\\nmulti-entity question answering over wikipedia graph, 2025. URL https://arxiv.org/abs/2503.01346 .\\n[854] Zhengxuan Zhang, Yin Wu, Yuyu Luo, and Nan Tang. Fine-grained retrieval-augmented generation for visual\\nquestion answering, 2025. URL https://arxiv.org/abs/2502.20964 .\\n[855] Mingye Zhu, Yi Liu, Lei Zhang, Junbo Guo, and Zhendong Mao. Lire: listwise reward enhancement for\\npreference alignment. arXiv preprint arXiv:2405.13516 , 2024.\\n[856] Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, and\\nDavid Silver. Online and offline reinforcement learning by planning with a learned model. Advances in Neural\\nInformation Processing Systems , 34:27580–27591, 2021.\\n[857] Sili Huang, Jifeng Hu, Zhejian Yang, Liwei Yang, Tao Luo, Hechang Chen, Lichao Sun, and Bo Yang. Decision\\nmamba: Reinforcement learning via hybrid selective sequence modeling. arXiv preprint arXiv:2406.00079 ,\\n2024.\\n[858] Kun Lei, Zhengmao He, Chenhao Lu, Kaizhe Hu, Yang Gao, and Huazhe Xu. Uni-o4: Unifying online and\\noffline deep reinforcement learning with multi-step on-policy optimization. arXiv preprint arXiv:2311.03351 ,\\n2023.\\n[859] Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt MacDermott,\\nSören Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, Marc-Antoine Rondeau, Pierre-Luc\\nSt-Charles, and David Williams-King. Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer\\na Safer Path?, February 2025.\\n[860] Plato, Bernard Williams, M. J. Levett, and Myles Burnyeat. Theaetetus . Hackett Publishing, January 1992.\\nISBN 978-0-87220-158-3.\\n[861] Edmund L Gettier. Is Justified True Belief Knowledge? Analysis , June 1963. doi:10.1093/analys/23.6.121.\\n[862] Matthias Steup and Ram Neta. Epistemology. In Edward N. Zalta and Uri Nodelman, editors, The Stanford\\nEncyclopedia of Philosophy . Metaphysics Research Lab, Stanford University, winter 2024 edition, 2024.\\n[863] E. T. Jaynes. Probability Theory: The Logic of Science . Cambridge University Press, 2003. ISBN 978-0-521-\\n59271-0. doi:10.1017/CBO9780511790423.\\n[864] Thomas Parr, Giovanni Pezzulo, and Karl J. Friston. Active Inference: The Free Energy Principle in Mind,\\nBrain, and Behavior . MIT Press, 2022.\\n[865] François Chollet. On the Measure of Intelligence, November 2019.\\n[866] Thomas M Cover and Joy A Thomas. ELEMENTS OF INFORMATION THEORY . John Wiley & Sons, April\\n2005.\\n[867] Raymond B. Cattell. Theory of fluid and crystallized intelligence: A critical experiment. Journal of Educational\\nPsychology , 54(1):1–22, 1963. ISSN 1939-2176. doi:10.1037/h0046743.\\n[868] Raymond B. Cattell. Abilities: Their Structure, Growth, and Action . Houghton Mifflin, 1971. ISBN 978-0-395-\\n04275-5.\\n[869] Alexandr Ten, Pramod Kaushik, Pierre-Yves Oudeyer, and Jacqueline Gottlieb. Humans monitor learning\\nprogress in curiosity-driven exploration. Nature Communications , 12(1):5972, October 2021. ISSN 2041-1723.\\ndoi:10.1038/s41467-021-26196-w.\\n[870] Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A. Efros. Large-Scale\\nStudy of Curiosity-Driven Learning, August 2018.\\n[871] Eberhard O. V oit. Perspective: Dimensions of the scientific method. PLOS Computational Biology , 15(9):\\ne1007279, September 2019. ISSN 1553-7358. doi:10.1371/journal.pcbi.1007279.\\n235\\n[872] Kjell Jørgen Hole and Subutai Ahmad. A thousand brains: Toward biologically constrained AI. SN Applied\\nSciences , 3(8):743, July 2021. ISSN 2523-3971. doi:10.1007/s42452-021-04715-0.\\n[873] Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen\\nWei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui\\nHuang, Xia Ning, Song Gao, Yu Su, and Huan Sun. ScienceAgentBench: Toward Rigorous Assessment of\\nLanguage Agents for Data-Driven Scientific Discovery, October 2024.\\n[874] Michael H. Prince, Henry Chan, Aikaterini Vriza, Tao Zhou, Varuni K. Sastry, Yanqi Luo, Matthew T. Dearing,\\nRoss J. Harder, Rama K. Vasudevan, and Mathew J. Cherukara. Opportunities for retrieval and tool augmented\\nlarge language models in scientific facilities. npj Computational Materials , 10(1):1–8, November 2024. ISSN\\n2057-3960. doi:10.1038/s41524-024-01423-2.\\n[875] Karl Raimund Popper. Conjectures and Refutations: The Growth of Scientific Knowledge . Routledge, 1962.\\n[876] Karl R. Popper. The Logic of Scientific Discovery . Routledge Classics. Routledge, repr. 2008 (twice) edition,\\n2008. ISBN 978-0-415-27843-0 978-0-415-27844-7.\\n[877] Donald A. Gillies. Popper and computer induction. BioEssays , 23(9):859–860, 2001. ISSN 1521-1878.\\ndoi:10.1002/bies.1123.\\n[878] Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, and Jindong Wang. Agentreview:\\nExploring peer review dynamics with llm agents. In EMNLP , 2024.\\n[879] Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, and Nanqing\\nDong. Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea\\nGeneration, October 2024.\\n[880] Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. ResearchAgent: Iterative Research\\nIdea Generation over Scientific Literature with Large Language Models, April 2024.\\n[881] Alexander H. Gower, Konstantin Korovin, Daniel Brunnsåker, Ievgeniia A. Tiukova, and Ross D. King. LGEM+:\\nA First-Order Logic Framework for Automated Improvement of Metabolic Network Models Through Abduction.\\nIn Albert Bifet, Ana Carolina Lorena, Rita P. Ribeiro, João Gama, and Pedro H. Abreu, editors, Discovery\\nScience , pages 628–643. Springer Nature Switzerland, 2023. ISBN 978-3-031-45275-8. doi:10.1007/978-3-\\n031-45275-8_42.\\n[882] Anthony Coutant, Katherine Roper, Daniel Trejo-Banos, Dominique Bouthinon, Martin Carpenter, Jacek\\nGrzebyta, Guillaume Santini, Henry Soldano, Mohamed Elati, Jan Ramon, Celine Rouveirol, Larisa N.\\nSoldatova, and Ross D. King. Closed-loop cycles of experiment design, execution, and learning accelerate\\nsystems biology model development in yeast. Proceedings of the National Academy of Sciences , 116(36):\\n18142–18147, September 2019. doi:10.1073/pnas.1900548116.\\n[883] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba,\\nShichang Zhang, Yizhou Sun, and Wei Wang. SciBench: Evaluating College-Level Scientific Problem-Solving\\nAbilities of Large Language Models, June 2024.\\n[884] Haorui Wang, Marta Skreta, Cher-Tian Ser, Wenhao Gao, Lingkai Kong, Felix Strieth-Kalthoff, Chenru Duan,\\nYuchen Zhuang, Yue Yu, Yanqiao Zhu, Yuanqi Du, Alán Aspuru-Guzik, Kirill Neklyudov, and Chao Zhang.\\nEfficient Evolutionary Search Over Chemical Space with Large Language Models, July 2024.\\n[885] Shuyi Jia, Chao Zhang, and Victor Fung. LLMatDesign: Autonomous Materials Discovery with Large\\nLanguage Models, June 2024.\\n[886] Holland Hysmith, Elham Foadian, Shakti P. Padhy, Sergei V . Kalinin, Rob G. Moore, Olga S. Ovchinnikova,\\nand Mahshid Ahmadi. The future of self-driving laboratories: From human in the loop interactive AI to\\ngamification. Digital Discovery , 3(4):621–636, 2024. doi:10.1039/D4DD00040D.\\n[887] Yijia Xiao, Wanjia Zhao, Junkai Zhang, Yiqiao Jin, Han Zhang, Zhicheng Ren, Renliang Sun, Haixin Wang,\\nGuancheng Wan, Pan Lu, et al. Protein large language models: A comprehensive survey. arXiv:2502.17504 ,\\n2025.\\n[888] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil,\\nOri Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido,\\nand Alexander Rives. Evolutionary-scale prediction of atomic-level protein structure with a language model.\\nScience , 379(6637):1123–1130, March 2023. doi:10.1126/science.ade2574.\\n[889] Richard Evans, Michael O’Neill, Alexander Pritzel, Natasha Antropova, Andrew Senior, Tim Green, Augustin\\nŽídek, Russ Bates, Sam Blackwell, Jason Yim, Olaf Ronneberger, Sebastian Bodenstein, Michal Zielinski, Alex\\nBridgland, Anna Potapenko, Andrew Cowie, Kathryn Tunyasuvunakool, Rishub Jain, Ellen Clancy, Pushmeet\\nKohli, John Jumper, and Demis Hassabis. Protein complex prediction with AlphaFold-Multimer, October 2021.\\n236\\n[890] Veda Sheersh Boorla, Ratul Chowdhury, Ranjani Ramasubramanian, Brandon Ameglio, Rahel Frick, Jeffrey J.\\nGray, and Costas D. Maranas. De novo design and Rosetta-based assessment of high-affinity antibody variable\\nregions (Fv) against the SARS-CoV-2 spike receptor binding domain (RBD). Proteins: Structure, Function,\\nand Bioinformatics , 91(2):196–208, 2023. ISSN 1097-0134. doi:10.1002/prot.26422.\\n[891] Jiefu Ou, Arda Uzunoglu, Benjamin Van Durme, and Daniel Khashabi. WorldAPIs: The World Is Worth How\\nMany APIs? A Thought Experiment, July 2024.\\n[892] Yuxing Fei, Bernardus Rendy, Rishi Kumar, Olympia Dartsi, Hrushikesh P. Sahasrabuddhe, Matthew J. McDer-\\nmott, Zheren Wang, Nathan J. Szymanski, Lauren N. Walters, David Milsted, Yan Zeng, Anubhav Jain, and Ger-\\nbrand Ceder. AlabOS: A Python-based reconfigurable workflow management framework for autonomous labo-\\nratories. Digital Discovery , 3(11):2275–2288, November 2024. ISSN 2635-098X. doi:10.1039/D4DD00129J.\\n[893] Andrew D McNaughton, Gautham Krishna Sankar Ramalaxmi, Agustin Kruel, Carter R Knutson, Rohith A\\nVarikoti, and Neeraj Kumar. CACTUS: Chemistry agent connecting tool usage to science. ACS Omega , 9(46):\\n46563–46573, 2024.\\n[894] Rafael Vescovi, Tobias Ginsburg, Kyle Hippe, Doga Ozgulbas, Casey Stone, Abraham Stroka, Rory Butler,\\nBen Blaiszik, Tom Brettin, Kyle Chard, Mark Hereld, Arvind Ramanathan, Rick Stevens, Aikaterini Vriza, Jie\\nXu, Qingteng Zhang, and Ian Foster. Towards a modular architecture for science factories. Digital Discovery , 2\\n(6):1980–1998, 2023.\\n[895] Daniil A. Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large\\nlanguage models. Nature , 624(7992):570–578, December 2023. ISSN 1476-4687. doi:10.1038/s41586-023-\\n06792-0.\\n[896] Emerald Cloud Lab. ECL Documentation. https://www.emeraldcloudlab.com/documentation/objects/, 2025.\\n[897] Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. A Survey of Embodied AI: From\\nSimulators to Research Tasks, January 2022.\\n[898] Rafael Vescovi, Ryan Chard, Nickolaus D Saint, Ben Blaiszik, Jim Pruyne, Tekin Bicer, Alex Lavens,\\nZhengchun Liu, Michael E Papka, Suresh Narayanan, Nicholas Schwarz, Kyle Chard, and Ian T. Foster.\\nLinking scientific instruments and computation: Patterns, technologies, and experiences. Patterns , 3(10), 2022.\\n[899] Doga Yamac Ozgulbas, Don Jensen Jr, Rory Butler, Rafael Vescovi, Ian T Foster, Michael Irvin, Yasukazu\\nNakaye, Miaoqi Chu, Eric M Dufresne, Soenke Seifert, et al. Robotic pendant drop: Containerless liquid for\\nµs-resolved, AI-executable XPCS. Light: Science & Applications , 12(1):196, 2023.\\n[900] Chandima Fernando, Daniel Olds, Stuart I Campbell, and Phillip M Maffettone. Facile integration of robots\\ninto experimental orchestration at scientific user facilities. In IEEE International Conference on Robotics and\\nAutomation , pages 9578–9584. IEEE, 2024.\\n[901] Stanley Lo, Sterling G Baird, Joshua Schrier, Ben Blaiszik, Nessa Carson, Ian Foster, Andrés Aguilar-Granda,\\nSergei V Kalinin, Benji Maruyama, Maria Politi, Helen Tran, Taylor D. Sparks, and Alan Aspuru-Guzik.\\nReview of low-cost self-driving laboratories in chemistry and materials science: The “frugal twin” concept.\\nDigital Discovery , 3(5):842–868, 2024.\\n[902] David Abel, André Barreto, Michael Bowling, Will Dabney, Shi Dong, Steven Hansen, Anna Harutyunyan,\\nKhimya Khetarpal, Clare Lyle, Razvan Pascanu, Georgios Piliouras, Doina Precup, Jonathan Richens, Mark\\nRowland, Tom Schaul, and Satinder Singh. Agency Is Frame-Dependent, February 2025.\\n[903] Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman\\nOlsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli Järviniemi, Matthew Barnett, Robert\\nSandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart,\\nBogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, and Mark Wildon. FrontierMath: A\\nBenchmark for Evaluating Advanced Mathematical Reasoning in AI, December 2024.\\n[904] Solim LeGris, Wai Keen V ong, Brenden M. Lake, and Todd M. Gureckis. H-ARC: A Robust Estimate of\\nHuman Performance on the Abstraction and Reasoning Corpus Benchmark, September 2024.\\n[905] Junjie Wu, Mo Yu, Lemao Liu, Dit-Yan Yeung, and Jie Zhou. Understanding LLMs’ Fluid Intelligence\\nDeficiency: An Analysis of the ARC Task, February 2025.\\n[906] Zeyuan Allen-Zhu and Xiaoli Xu. DOGE: Reforming AI Conferences and Towards a Future Civilization of\\nFairness and Justice, February 2025.\\n[907] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\\nDenny Zhou. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, January 2023.\\n237\\n[908] Andrew D. White, Glen M. Hocky, Heta A. Gandhi, Mehrad Ansari, Sam Cox, Geemi P. Wellawatte, Subarna\\nSasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh, and Willmor J. Peña Ccoa. Assessment of chemistry\\nknowledge in large language models that generate code. Digital Discovery , 2(2):368–376, 2023. ISSN\\n2635-098X. doi:10.1039/D2DD00087C.\\n[909] Botao Yu, Frazier N Baker, Ziru Chen, Garrett Herb, Boyu Gou, Daniel Adu-Ampratwum, Xia Ning, and Huan\\nSun. Tooling or not tooling? the impact of tools on language agents for chemistry problem solving. arXiv\\npreprint arXiv:2411.07228 , 2024.\\n[910] Franck Cappello, Sandeep Madireddy, Robert Underwood, Neil Getty, Nicholas Lee-Ping Chia, Nesar Ra-\\nmachandra, Josh Nguyen, Murat Keceli, Tanwi Mallick, Zilinghan Li, Marieme Ngom, Chenhui Zhangx, Angel\\nYanguas-Gilxi, Evan Antoniuk, Bhavya Kailkhura, Minyang Tian, Yufeng Du, Yuan-Sen Ting, Azton Wells,\\nBogdan Nicolae, Avinash Maurya, M. Mustafa Rafique, Eliu Huerta, Bo Li, Ian Foster, and Rick Stevens.\\nEAIRA: Establishing a methodology for evaluating AI models as scientific research assistants. arXiv preprint\\narXiv:2502.20309 , 2025.\\n[911] Paul Raccuglia, Katherine C. Elbert, Philip D. F. Adler, Casey Falk, Malia B. Wenny, Aurelio Mollo,\\nMatthias Zeller, Sorelle A. Friedler, Joshua Schrier, and Alexander J. Norquist. Machine-learning-assisted\\nmaterials discovery using failed experiments. Nature , 533(7601):73–76, May 2016. ISSN 1476-4687.\\ndoi:10.1038/nature17439.\\n[912] OpenAI. Introducing deep research. https://openai.com/index/introducing-deep-research/, 2025.\\n[913] Steven N. Goodman. Introduction to Bayesian methods I: Measuring the strength of evidence. Clin-\\nical Trials (London, England) , 2(4):282–290; discussion 301–304, 364–378, 2005. ISSN 1740-7745.\\ndoi:10.1191/1740774505cn098oa.\\n[914] Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. A survey on llm-based multi-agent systems: Workflow,\\ninfrastructure, and challenges. Vicinagearth , 1(1):9, 10 2024. doi:10.1007/s44336-024-00009-2. URL\\nhttps://doi.org/10.1007/s44336-024-00009-2 .\\n[915] James Surowiecki. The wisdom of crowds. Surowiecki, J , 2005.\\n[916] Chris Frith and Uta Frith. Theory of mind. Current biology , 15(17):R644–R645, 2005.\\n[917] Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, and Katia Sycara.\\nTheory of mind for multi-agent collaboration via large language models. arXiv preprint arXiv:2310.10701 ,\\n2023.\\n[918] Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. Reconcile: Round-table conference improves\\nreasoning via consensus among diverse LLMs. arXiv preprint arXiv:2309.13007 , 2023.\\n[919] Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, De-Yong Ye, Peilin Zhao, Ee-Peng Lim, Hui Xiong, and Hao\\nWang. Llm-based agent society investigation: Collaboration and confrontation in avalon gameplay. In Confer-\\nence on Empirical Methods in Natural Language Processing , 2023. URL https://api.semanticscholar.\\norg/CorpusID:264436387 .\\n[920] Wei Wang, Dan Zhang, Tao Feng, Boyan Wang, and Jie Tang. Battleagentbench: A benchmark for evalu-\\nating cooperation and competition capabilities of language models in multi-agent systems. arXiv preprint\\narXiv:2408.15971 , 2024.\\n[921] Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and Yang Liu. Agent\\nhospital: A simulacrum of hospital with evolvable medical agents. arXiv preprint arXiv:2405.02957 , 2024.\\n[922] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark\\nGerstein. MedAgents: Large language models as collaborators for zero-shot medical reasoning. In Findings of\\nthe Association for Computational Linguistics: ACL 2024 , pages 599–621, Bangkok, Thailand, 2024.\\n[923] Hao Wei, Jianing Qiu, Haibao Yu, and Wu Yuan. Medco: Medical education copilots based on a multi-agent\\nframework. arXiv preprint arXiv:2408.12496 , 2024.\\n[924] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, and\\nChuang Gan. Building cooperative embodied agents modularly with large language models. arXiv preprint\\narXiv:2307.02485 , 2023.\\n[925] Yubo Dong, Xukun Zhu, Zhengzhe Pan, Linchao Zhu, and Yi Yang. VillagerAgent: A graph-based multi-agent\\nframework for coordinating complex task dependencies in Minecraft. In Findings of the Association for\\nComputational Linguistics: ACL 2024 , 2024.\\n[926] Saaket Agashe, Yue Fan, Anthony Reyna, and Xin Eric Wang. Llm-coordination: evaluating and analyzing\\nmulti-agent coordination abilities in large language models. arXiv preprint arXiv:2310.03903 , 2023.\\n238\\n[927] Jiaqi Chen, Yuxian Jiang, Jiachen Lu, and Li Zhang. S-agents: self-organizing agents in open-ended environ-\\nment. arXiv preprint arXiv:2402.04578 , 2024.\\n[928] Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song, Shudan Zhang, Hanyu Lai, Xinyi Liu,\\nHanlin Zhao, et al. Visualagentbench: Towards large multimodal models as visual foundation agents. In The\\nThirteenth International Conference on Learning Representations , 2025.\\n[929] Bo Qiao, Liqun Li, Xu Zhang, Shilin He, Yu Kang, Chaoyun Zhang, Fangkai Yang, Hang Dong, Jue Zhang,\\nLu Wang, Minghua Ma, Pu Zhao, Si Qin, Xiaoting Qin, Chao Du, Yong Xu, Qingwei Lin, Saravan Rajmohan,\\nand Dongmei Zhang. Taskweaver: A code-first agent framework, 2024. URL https://arxiv.org/abs/\\n2311.17541 .\\n[930] Wannita Takerngsaksiri, Jirat Pasuksmit, Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Ruixiong\\nZhang, Fan Jiang, Jing Li, Evan Cook, Kun Chen, and Ming Wu. Human-in-the-loop software development\\nagents, 2025. URL https://arxiv.org/abs/2411.12924 .\\n[931] Anthropic. Model context protocol, 2025. URL https://www.anthropic.com/news/\\nmodel-context-protocol . Accessed: 2025-01-07.\\n[932] Samuele Marro, Emanuele La Malfa, Jesse Wright, Guohao Li, Nigel Shadbolt, Michael Wooldridge, and\\nPhilip Torr. A scalable communication protocol for networks of large language models, 2024. URL https:\\n//arxiv.org/abs/2410.11905 .\\n[933] Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng Yang, Ruobing Xie,\\nZhiyuan Liu, and Maosong Sun. Internet of agents: Weaving a web of heterogeneous agents for collaborative\\nintelligence, 2024. URL https://arxiv.org/abs/2407.07061 .\\n[934] Gabriel Mukobi, Hannah Erlebach, Niklas Lauffer, Lewis Hammond, Alan Chan, and Jesse Clifton. Welfare\\ndiplomacy: Benchmarking language model cooperation. ArXiv , abs/2310.08901, 2023. URL https://api.\\nsemanticscholar.org/CorpusID:264127980 .\\n[935] Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu,\\nGuoliang Dong, Artem Aliev, et al. Coder: Issue resolving with multi-agent and task graphs. arXiv preprint\\narXiv:2406.01304 , 2024.\\n[936] Ziyi Yang, Zaibin Zhang, Zirui Zheng, Yuxian Jiang, Ziyue Gan, Zhiyu Wang, Zijian Ling, Jinsong Chen,\\nMartz Ma, Bowen Dong, et al. Oasis: Open agents social interaction simulations on one million agents. arXiv\\npreprint arXiv:2411.11581 , 2024.\\n[937] Joanne Leong, John Tang, Edward Cutrell, Sasa Junuzovic, Gregory Paul Baribault, and Kori Inkpen. Dittos:\\nPersonalized, embodied agents that participate in meetings when you are unavailable. Proc. ACM Hum.-Comput.\\nInteract. , 8(CSCW2), November 2024.\\n[938] Ge Gao, Alexey Taymanov, Eduardo Salinas, Paul Mineiro, and Dipendra Misra. Aligning LLM agents by\\nlearning latent preference from user edits, 2024. URL https://arxiv.org/abs/2404.15269 .\\n[939] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen\\nJiang, Carrie J. Cai, Michael Terry, Quoc V . Le, and Charles Sutton. Program synthesis with large language\\nmodels. CoRR , abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.07732 .\\n[940] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and\\nChristopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Ellen\\nRiloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference\\non Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018 ,\\npages 2369–2380. Association for Computational Linguistics, 2018. doi:10.18653/V1/D18-1259. URL\\nhttps://doi.org/10.18653/v1/d18-1259 .\\n[941] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-\\nhardt. Measuring massive multitask language understanding. In 9th International Conference on Learn-\\ning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL\\nhttps://openreview.net/forum?id=d7KBjmI3GmQ .\\n[942] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word\\nproblems? In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy,\\nSteven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021\\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, NAACL-HLT 2021, Online, June 6-11, 2021 , pages 2080–2094. Association for Computational\\nLinguistics, 2021. doi:10.18653/V1/2021.NAACL-MAIN.168. URL https://doi.org/10.18653/v1/\\n2021.naacl-main.168 .\\n239\\n[943] Subhro Roy and Dan Roth. Solving general arithmetic word problems. CoRR , abs/1608.01413, 2016. URL\\nhttp://arxiv.org/abs/1608.01413 .\\n[944] Haochen Sun, Shuwen Zhang, Lei Ren, Hao Xu, Hao Fu, Caixia Yuan, and Xiaojie Wang. Collab-overcooked:\\nBenchmarking and evaluating large language models as collaborative agents, 2025. URL https://arxiv.\\norg/abs/2502.20073 .\\n[945] Longling Geng and Edward Y . Chang. Realm-bench: A real-world planning benchmark for llms and multi-agent\\nsystems, 2025. URL https://arxiv.org/abs/2502.18836 .\\n[946] Matthew Chang, Gunjan Chhablani, Alexander Clegg, Mikael Dallaire Cote, Ruta Desai, Michal Hlavac,\\nVladimir Karashchuk, Jacob Krantz, Roozbeh Mottaghi, Priyam Parashar, Siddharth Patki, Ishita Prasad, Xavier\\nPuig, Akshara Rai, Ram Ramrakhya, Daniel Tran, Joanne Truong, John M. Turner, Eric Undersander, and\\nTsung-Yen Yang. Partnr: A benchmark for planning and reasoning in embodied multi-agent tasks, 2024.\\n[947] Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Weiwen Xu, Deli Zhao, and Lidong Bing. Auto-arena:\\nAutomating llm evaluations with agent peer battles and committee discussions, 2024. URL https://arxiv.\\norg/abs/2405.20267 .\\n[948] Kunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong Wang, Cheng\\nQian, Xiangru Tang, Heng Ji, and Jiaxuan You. Multiagentbench: Evaluating the collaboration and competition\\nof llm agents, 2025. URL https://arxiv.org/abs/2503.01935 .\\n[949] Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song,\\nMan Lan, and Furu Wei. Llm as a mastermind: A survey of strategic reasoning with large language models.\\narXiv preprint arXiv:2404.01230 , 2024.\\n[950] Alonso Silva. Large language models playing mixed strategy nash equilibrium games. In International\\nConference on Network Games, Artificial Intelligence, Control and Optimization , pages 142–152. Springer,\\n2024.\\n[951] John J Horton. Large language models as simulated economic agents: What can we learn from homo silicus?\\nTechnical report, National Bureau of Economic Research, 2023.\\n[952] Ian Gemp, Yoram Bachrach, Marc Lanctot, Roma Patel, Vibhavari Dasagi, Luke Marris, Georgios Piliouras,\\nSiqi Liu, and Karl Tuyls. States as strings as strategies: Steering language models with game-theoretic solvers.\\narXiv preprint arXiv:2402.01704 , 2024.\\n[953] Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang, Tao Ge, and Furu Wei.\\nAlympics: Llm agents meet game theory–exploring strategic decision-making with ai agents. arXiv preprint\\narXiv:2311.03220 , 2023.\\n[954] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz. Playing\\nrepeated games with large language models. arXiv preprint arXiv:2305.16867 , 2023.\\n[955] Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See Kiong Ng, and Jiashi Feng.\\nMagic: Investigation of large language model powered multi-agent in cognition, adaptability, rationality and\\ncollaboration, 2024. URL https://arxiv.org/abs/2311.08562 .\\n[956] Kanishk Gandhi, Dorsa Sadigh, and Noah D Goodman. Strategic reasoning with language models. arXiv\\npreprint arXiv:2305.19165 , 2023.\\n[957] Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit\\nBansal, Tianlong Chen, and Kaidi Xu. Gtbench: Uncovering the strategic reasoning limitations of llms via\\ngame-theoretic evaluations. arXiv preprint arXiv:2402.12348 , 2024.\\n[958] Nian Li, Chen Gao, Yong Li, and Qingmin Liao. Large language model-empowered agents for simulating\\nmacroeconomic activities. Available at SSRN 4606937 , 2023.\\n[959] Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, and Xing Xie. Competeai:\\nUnderstanding the competition behaviors in large language model-based agents. In ICML , 2024.\\n[960] Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang, Yang Yang, and Rui Wang. Measuring\\nbargaining abilities of llms: A benchmark and a buyer-enhancement method. arXiv preprint arXiv:2402.15813 ,\\n2024.\\n[961] Karthik Sreedhar and Lydia Chilton. Simulating human strategic behavior: Comparing single and multi-agent\\nllms. ArXiv , abs/2402.08189, 2024. URL https://api.semanticscholar.org/CorpusID:267636591 .\\n[962] Ryan Y Lin, Siddhartha Ojha, Kevin Cai, and Maxwell F Chen. Strategic collusion of LLM agents: Market\\ndivision in multi-commodity competitions. arXiv preprint arXiv:2410.00031 , 2024.\\n240\\n[963] Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Schölkopf, Mrinmaya Sachan, and Rada Mihalcea.\\nCooperate or collapse: Emergence of sustainable cooperation in a society of LLM agents. In The Thirty-eighth\\nAnnual Conference on Neural Information Processing Systems , 2024.\\n[964] Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents with reinforcement learning for strategic\\nplay in the werewolf game. arXiv preprint arXiv:2310.18940 , 2023.\\n[965] Silin Du and Xiaowei Zhang. Helmsman of the masses? evaluate the opinion leadership of large language\\nmodels in the werewolf game. arXiv preprint arXiv:2404.01602 , 2024.\\n[966] Xuanfa Jin, Ziyan Wang, Yali Du, Meng Fang, Haifeng Zhang, and Jun Wang. Learning to discuss strategically:\\nA case study on one night ultimate werewolf. arXiv preprint arXiv:2405.19946 , 2024.\\n[967] Simon Stepputtis, Joseph Campbell, Yaqi Xie, Zhengyang Qi, Wenxin Sharon Zhang, Ruiyi Wang, Sanketh\\nRangreji, Michael Lewis, and Katia Sycara. Long-horizon dialogue understanding for role identification in the\\ngame of avalon with large language models. arXiv preprint arXiv:2311.05720 , 2023.\\n[968] Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei\\nWang, Shiji Song, and Gao Huang. Avalon’s game of thoughts: Battle against deception through recursive\\ncontemplation. arXiv preprint arXiv:2310.01320 , 2023.\\n[969] Zijing Shi, Meng Fang, Shunfeng Zheng, Shilong Deng, Ling Chen, and Yali Du. Cooperation on the fly:\\nExploring language agents for ad hoc teamwork in the avalon game. arXiv preprint arXiv:2312.17515 , 2023.\\n[970] Dekun Wu, Haochen Shi, Zhiyuan Sun, and Bang Liu. Deciphering digital detectives: Understanding LLM\\nbehaviors and capabilities in multi-agent mystery games. arXiv preprint arXiv:2312.00746 , 2023.\\n[971] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring\\nlarge language models for communication games: An empirical study on werewolf, 2024. URL https:\\n//arxiv.org/abs/2309.04658 .\\n[972] Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. Avalonbench: Evaluating LLMs playing the game of\\navalon, 2023. URL https://arxiv.org/abs/2310.05036 .\\n[973] Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and Yongfeng\\nZhang. War and peace (waragent): Large language model-based multi-agent simulation of world wars. arXiv\\npreprint arXiv:2311.17227 , 2023.\\n[974] Mingyu Jin, Beichen Wang, Zhaoqian Xue, Suiyuan Zhu, Wenyue Hua, Hua Tang, Kai Mei, Mengnan Du, and\\nYongfeng Zhang. What if LLMs have different world views: Simulating alien civilizations with llm-based\\nagents. arXiv preprint arXiv:2402.13184 , 2024.\\n[975] Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, and Yong Li. Large\\nlanguage models empowered agent-based modeling and simulation: A survey and perspectives. Humanities\\nand Social Sciences Communications , 11(1):1–24, 2024.\\n[976] Nian Li, Chen Gao, Mingyu Li, Yong Li, and Qingmin Liao. Econagent: Large language model-empowered\\nagents for simulating macroeconomic activities. In Proceedings of the 62nd Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers) , pages 15523–15536, 2024.\\n[977] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry O’Sullivan, and Hoang D Nguyen.\\nMulti-agent collaboration mechanisms: A survey of llms. arXiv preprint arXiv:2501.06322 , 2025.\\n[978] Xinnong Zhang, Jiayu Lin, Libo Sun, Weihong Qi, Yihang Yang, Yue Chen, Hanjia Lyu, Xinyi Mou, Siming\\nChen, Jiebo Luo, et al. Electionsim: Massive population election simulation powered by large language model\\ndriven agents. arXiv preprint arXiv:2410.20746 , 2024.\\n[979] Antonino Ferraro, Antonio Galli, Valerio La Gatta, Marco Postiglione, Gian Marco Orlando, Diego Russo,\\nGiuseppe Riccio, Antonio Romano, and Vincenzo Moscato. Agent-based modelling meets generative AI in\\nsocial network simulations. arXiv preprint arXiv:2411.16031 , 2024.\\n[980] Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh, Robert Hawkins, Sijia Yang, Dhavan\\nShah, Junjie Hu, and Timothy T Rogers. Simulating opinion dynamics with networks of LLM-based agents.\\narXiv preprint arXiv:2311.09618 , 2023.\\n[981] Yuhan Liu, Xiuying Chen, Xiaoqing Zhang, Xing Gao, Ji Zhang, and Rui Yan. From skepticism to acceptance:\\nSimulating the attitude dynamics toward fake news. arXiv preprint arXiv:2403.09498 , 2024.\\n[982] Jiakai Tang, Heyang Gao, Xuchen Pan, Lei Wang, Haoran Tan, Dawei Gao, Yushuo Chen, Xu Chen, Yankai\\nLin, Yaliang Li, et al. Gensim: A general social simulation platform with large language model based agents.\\narXiv preprint arXiv:2410.04360 , 2024.\\n241\\n[983] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun.\\nCommunicative agents for software development. arXiv preprint arXiv:2307.07924 , 6(3), 2023.\\n[984] Xudong Guo, Kaixuan Huang, Jiale Liu, Wenhui Fan, Natalia Vélez, Qingyun Wu, Huazheng Wang, Thomas L.\\nGriffiths, and Mengdi Wang. Embodied LLM agents learn to cooperate in organized teams, 2024. URL\\nhttps://arxiv.org/abs/2403.12482 .\\n[985] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and\\nreasoning in language models through multiagent debate. In Forty-first International Conference on Machine\\nLearning , 2023.\\n[986] Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang,\\nTinghui Zhu, et al. From persona to personalization: A survey on role-playing language agents. arXiv preprint\\narXiv:2404.18231 , 2024.\\n[987] Jingyun Sun, Chengxiao Dai, Zhongze Luo, Yangbo Chang, and Yang Li. Lawluo: A multi-agent collaborative\\nframework for multi-round chinese legal consultation, 2024. URL https://arxiv.org/abs/2407.16252 .\\n[988] Wenhao Yu, Jie Peng, Yueliang Ying, Sai Li, Jianmin Ji, and Yanyong Zhang. Mhrc: Closed-loop decentralized\\nmulti-heterogeneous robot collaboration with large language models, 2024. URL https://arxiv.org/abs/\\n2409.16030 .\\n[989] Altera. AL, Andrew Ahn, Nic Becker, Stephanie Carroll, Nico Christie, Manuel Cortes, Arda Demirci, Melissa\\nDu, Frankie Li, Shuying Luo, Peter Y Wang, Mathew Willows, Feitong Yang, and Guangyu Robert Yang. Project\\nsid: Many-agent simulations toward AI civilization, 2024. URL https://arxiv.org/abs/2411.00114 .\\n[990] Ryosuke Takata, Atsushi Masumori, and Takashi Ikegami. Spontaneous emergence of agent individuality\\nthrough social interactions in llm-based communities, 2024. URL https://arxiv.org/abs/2411.03252 .\\n[991] Shubham Gandhi, Manasi Patwardhan, Lovekesh Vig, and Gautam Shroff. Budgetmlagent: A cost-effective\\nllm multi-agent system for automating machine learning tasks, 2025. URL https://arxiv.org/abs/2411.\\n07464 .\\n[992] Yuxing Lu and Jinzhuo Wang. Karma: Leveraging multi-agent llms for automated knowledge graph enrichment,\\n2025. URL https://arxiv.org/abs/2502.06472 .\\n[993] Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu,\\nQingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. Large language model-brained gui agents: A\\nsurvey, 2025. URL https://arxiv.org/abs/2411.18279 .\\n[994] Dong Huang, Jie M. Zhang, Michael Luck, Qingwen Bu, Yuhao Qing, and Heming Cui. Agentcoder: Multi-\\nagent-based code generation with iterative testing and optimisation, 2024. URL https://arxiv.org/abs/\\n2312.13010 .\\n[995] Zixuan Wang, Chi-Keung Tang, and Yu-Wing Tai. Audio-agent: Leveraging LLMs for audio generation, editing\\nand composition, 2025. URL https://arxiv.org/abs/2410.03335 .\\n[996] Dong Zhang, Zhaowei Li, Pengyu Wang, Xin Zhang, Yaqian Zhou, and Xipeng Qiu. Speechagents: Human-\\ncommunication simulation with multi-modal multi-agent systems, 2024. URL https://arxiv.org/abs/\\n2401.03945 .\\n[997] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir\\nBalaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake\\nBerdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd,\\nAnna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie\\nCampbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis\\nChantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,\\nHyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah\\nDeutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,\\nTyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford,\\nLeo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,\\nJonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo,\\nChris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey,\\nWade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie\\nJonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar,\\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie\\n242\\nKiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic,\\nGretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy,\\nChak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia\\nLue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie\\nMayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake\\nMcNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie\\nMonaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair,\\nReiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen\\nO’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish,\\nEmy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,\\nMichael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly\\nPowell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\\nCameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario\\nSaltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman,\\nDaniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor,\\nEric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie\\nStaudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B.\\nThompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan\\nFelipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea V oss, Carroll Wainwright, Justin Jay Wang,\\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi\\nWeng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren\\nWorkman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech\\nZaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang,\\nWilliam Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774 .\\n[998] Yuyu Luo, Nan Tang, Guoliang Li, Jiawei Tang, Chengliang Chai, and Xuedi Qin. Natural language to\\nvisualization by neural machine translation. IEEE Trans. Vis. Comput. Graph. , 28(1):217–226, 2022.\\n[999] Shuyu Shen, Sirong Lu, Leixian Shen, Zhonghua Sheng, Nan Tang, and Yuyu Luo. Ask humans or ai? exploring\\ntheir roles in visualization troubleshooting. CoRR , abs/2412.07673, 2024.\\n[1000] Xudong Yang, Yifan Wu, Yizhang Zhu, Nan Tang, and Yuyu Luo. Askchart: Universal chart understanding\\nthrough textual enhancement. arXiv preprint arXiv:2412.19146 , 2024.\\n[1001] Zhilin Wang, Yu Ying Chiu, and Yu Cheung Chiu. Humanoid agents: Platform for simulating human-\\nlike generative agents. In Yansong Feng and Els Lefever, editors, Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing: System Demonstrations , pages 167–176, Singapore,\\nDecember 2023. Association for Computational Linguistics. doi:10.18653/v1/2023.emnlp-demo.15. URL\\nhttps://aclanthology.org/2023.emnlp-demo.15/ .\\n[1002] Gaowei Chang. Agentnetworkprotocol, 2025. URL https://github.com/chgaowei/\\nAgentNetworkProtocol . GitHub repository, Accessed: 2025-01-07.\\n[1003] Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving language model negotiation with self-play and\\nin-context learning from ai feedback. arXiv preprint arXiv:2305.10142 , 2023.\\n[1004] Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. Examining inter-consistency of large language\\nmodels collaboration: An in-depth analysis via debate. arXiv preprint arXiv:2305.11595 , 2023.\\n[1005] Haotian Wang, Xiyuan Du, Weijiang Yu, Qianglong Chen, Kun Zhu, Zheng Chu, Lian Yan, and Yi Guan.\\nApollo’s oracle: Retrieval-augmented reasoning in multi-agent debates. arXiv preprint arXiv:2312.04854 ,\\n2023.\\n[1006] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen Schmidhuber.\\nLanguage agents as optimizable graphs. arXiv preprint arXiv:2402.16823 , 2024.\\n[1007] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu.\\nChateval: Towards better llm-based evaluators through multi-agent debate, 2023. URL https://arxiv.org/\\nabs/2308.07201 .\\n[1008] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F. Karlsson, Jie Fu, and Yemin Shi.\\nAutoagents: A framework for automatic agent generation, 2024. URL https://arxiv.org/abs/2309.\\n17288 .\\n[1009] Bingzheng Gan, Yufan Zhao, Tianyi Zhang, Jing Huang, Yusu Li, Shu Xian Teo, Changwang Zhang, and Wei Shi.\\nMaster: A multi-agent system with llm specialized mcts, 2025. URL https://arxiv.org/abs/2501.14304 .\\n243\\n[1010] Bin Lei, Yi Zhang, Shan Zuo, Ali Payani, and Caiwen Ding. Macm: Utilizing a multi-agent system for condition\\nmining in solving complex mathematical problems, 2024. URL https://arxiv.org/abs/2404.04735 .\\n[1011] Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan Dang, Weize Chen, and Cheng Yang.\\nMulti-agent software development through cross-team collaboration. arXiv preprint arXiv:2406.08979 , 2024.\\n[1012] Guozheng Li, Runfei Li, Yunshan Feng, Yu Zhang, Yuyu Luo, and Chi Harold Liu. Coinsight: Visual\\nstorytelling for hierarchical tables with connected insights. IEEE Transactions on Visualization and Computer\\nGraphics , 2024.\\n[1013] Yilin Ye, Jianing Hao, Yihan Hou, Zhan Wang, Shishi Xiao, Yuyu Luo, and Wei Zeng. Generative ai for\\nvisualization: State of the art and future directions. Visual Informatics , 2024.\\n[1014] Yifan Wu, Lutao Yan, Leixian Shen, Yunhai Wang, Nan Tang, and Yuyu Luo. Chartinsights: Evaluating\\nmultimodal large language models for low-level chart question answering. In Findings of the Association for\\nComputational Linguistics: EMNLP 2024 , pages 12174–12200, 2024.\\n[1015] Yunfan Zhang, Changlun Li, Yuyu Luo, and Nan Tang. Sketchfill: Sketch-guided code generation for imputing\\nderived missing values. arXiv preprint arXiv:2412.19113 , 2024.\\n[1016] Chengliang Chai, Nan Tang, Ju Fan, and Yuyu Luo. Demystifying artificial intelligence for data preparation. In\\nCompanion of the 2023 International Conference on Management of Data , pages 13–20, 2023.\\n[1017] Leixian Shen, Haotian Li, Yun Wang, Tianqi Luo, Yuyu Luo, and Huamin Qu. Data playwright: Authoring\\ndata videos with annotated narration. IEEE Transactions on Visualization and Computer Graphics , 2024.\\n[1018] Yupeng Xie, Yuyu Luo, Guoliang Li, and Nan Tang. Haichart: Human and AI paired visualization system.\\nProc. VLDB Endow. , 17(11):3178–3191, 2024.\\n[1019] Patara Trirat, Wonyong Jeong, and Sung Ju Hwang. Automl-agent: A multi-agent llm framework for full-\\npipeline automl. arXiv preprint arXiv:2410.02958 , 2024.\\n[1020] Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian\\nYang, Jiaheng Liu, Wanjun Zhong, Wangchunshu Zhou, Wenhao Huang, and Ge Zhang. Autokaggle: A\\nmulti-agent framework for autonomous data science competitions, 2024.\\n[1021] Suma Bailis, Jane Friedhoff, and Feiyang Chen. Werewolf arena: A case study in LLM evaluation via social\\ndeduction. arXiv preprint arXiv:2407.13943 , 2024.\\n[1022] Yuwei Hu, Runlin Lei, Xinyi Huang, Zhewei Wei, and Yongchao Liu. Scalable and accurate graph reasoning\\nwith llm-based multi-agents, 2024. URL https://arxiv.org/abs/2410.05130 .\\n[1023] Sumedh Rasal and E. J. Hauer. Navigating complexity: Orchestrated problem solving with multi-agent llms,\\n2024. URL https://arxiv.org/abs/2402.16713 .\\n[1024] Cheng Li, Damien Teney, Linyi Yang, Qingsong Wen, Xing Xie, and Jindong Wang. Culturepark: Boosting\\ncross-cultural understanding in large language models, 2024. URL https://arxiv.org/abs/2405.15145 .\\n[1025] Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo, Guangyu Robert\\nYang, and Andrew Ahn. Lyfe agents: Generative agents for low-cost real-time social interactions, 2023. URL\\nhttps://arxiv.org/abs/2310.02172 .\\n[1026] Thorsten Händler. Balancing autonomy and alignment: A multi-dimensional taxonomy for autonomous\\nllm-powered multi-agent architectures, 2023. URL https://arxiv.org/abs/2310.03659 .\\n[1027] Weize Chen, Jiarui Yuan, Chen Qian, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Optima: Optimizing\\neffectiveness and efficiency for llm-based multi-agent system. arXiv preprint arXiv:2410.08115 , 2024.\\n[1028] Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan\\nLiu, and Maosong Sun. Scaling large-language-model-based multi-agent collaboration. arXiv preprint\\narXiv:2406.07155 , 2024.\\n[1029] Hanqing Yang, Jingdi Chen, Marie Siew, Tania Lorido-Botran, and Carlee Joe-Wong. Llm-powered decentral-\\nized generative agents with adaptive hierarchical knowledge graph for cooperative planning. arXiv preprint\\narXiv:2502.05453 , 2025.\\n[1030] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson, Jie Fu, and Yemin Shi.\\nAutoagents: A framework for automatic agent generation. arXiv preprint arXiv:2309.17288 , 2023.\\n[1031] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and\\nZhaopeng Tu. Encouraging divergent thinking in large language models through multi-agent debate. arXiv\\npreprint arXiv:2305.19118 , 2023.\\n244\\n[1032] Yaoxiang Wang, Zhiyong Wu, Junfeng Yao, and Jinsong Su. Tdag: A multi-agent framework based on dynamic\\ntask decomposition and agent generation. Neural Networks , page 107200, 2025.\\n[1033] Boye Niu, Yiliao Song, Kai Lian, Yifan Shen, Yu Yao, Kun Zhang, and Tongliang Liu. Flow: A modular\\napproach to automated agentic workflow generation. arXiv preprint arXiv:2501.07834 , 2025.\\n[1034] Shilong Wang, Guibin Zhang, Miao Yu, Guancheng Wan, Fanci Meng, Chongye Guo, Kun Wang, and Yang\\nWang. G-safeguard: A topology-guided security lens and treatment on llm-based multi-agent systems. arXiv\\npreprint arXiv:2502.11127 , 2025.\\n[1035] Dawei Gao, Zitao Li, Xuchen Pan, Weirui Kuang, Zhijian Ma, Bingchen Qian, Fei Wei, Wenhao Zhang,\\nYuexiang Xie, Daoyuan Chen, et al. Agentscope: A flexible yet robust multi-agent platform. arXiv preprint\\narXiv:2402.14034 , 2024.\\n[1036] Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, and Jingren Zhou. Ai\\nhospital: Benchmarking large language models in a multi-agent medical interaction simulator. arXiv preprint\\narXiv:2402.09742 , 2024.\\n[1037] Xiutian Zhao, Ke Wang, and Wei Peng. An electoral approach to diversify llm-based multi-agent collective\\ndecision-making. arXiv preprint arXiv:2410.15168 , 2024.\\n[1038] Yoichi Ishibashi and Yoshimasa Nishimura. Self-organized agents: A LLM multi-agent framework toward ultra\\nlarge-scale code generation and optimization. arXiv preprint arXiv:2404.02183 , 2024.\\n[1039] Thorsten Händler. A taxonomy for autonomous llm-powered multi-agent architectures. In KMIS , pages 85–98,\\n2023.\\n[1040] Jinghua Piao, Yuwei Yan, Jun Zhang, Nian Li, Junbo Yan, Xiaochong Lan, Zhihong Lu, Zhiheng Zheng, Jing Yi\\nWang, Di Zhou, Chen Gao, Fengli Xu, Fang Zhang, Ke Rong, Jun Su, and Yong Li. Agentsociety: Large-scale\\nsimulation of llm-driven generative agents advances understanding of human behaviors and society, 2025. URL\\nhttps://arxiv.org/abs/2502.08691 .\\n[1041] Hung Du, Srikanth Thudumu, Rajesh Vasa, and Kon Mouzakis. A survey on context-aware multi-agent systems:\\ntechniques, challenges and future directions. arXiv preprint arXiv:2402.01968 , 2024.\\n[1042] Ziyuan Zhou, Guanjun Liu, and Ying Tang. Multi-agent reinforcement learning: Methods, applications,\\nvisionary prospects, and challenges. arXiv preprint arXiv:2305.10091 , 2023.\\n[1043] Changxi Zhu, Mehdi Dastani, and Shihan Wang. A survey of multi-agent deep reinforcement learning with\\ncommunication. Autonomous Agents and Multi-Agent Systems , 38(1):4, 2024.\\n[1044] Jingqing Ruan, Xiaotian Hao, Dong Li, and Hangyu Mao. Learning to collaborate by grouping: A consensus-\\noriented strategy for multi-agent reinforcement learning. In ECAI 2023 , pages 2010–2017. IOS Press, 2023.\\n[1045] Huaben Chen, Wenkang Ji, Lufeng Xu, and Shiyu Zhao. Multi-agent consensus seeking via large language\\nmodels. arXiv preprint arXiv:2310.20151 , 2023.\\n[1046] Yu Han Kim, Chanwoo Park, Hyewon Jeong, Yik Siu Chan, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and\\nHae Won Park. Mdagents: An adaptive collaboration of LLMs for medical decision-making. In NeurIPS , 2024.\\n[1047] Marios Papachristou, Longqi Yang, and Chin-Chia Hsu. Leveraging large language models for collective\\ndecision-making. arXiv preprint arXiv:2311.04928 , 2023.\\n[1048] Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Schölkopf, Mrinmaya Sachan, and Rada Mihalcea.\\nCooperate or collapse: Emergence of sustainable cooperation in a society of llm agents. Advances in Neural\\nInformation Processing Systems , 37:111715–111759, 2025.\\n[1049] Zichen Zhu, Hao Tang, Yansi Li, Kunyao Lan, Yixuan Jiang, Hao Zhou, Yixiao Wang, Situo Zhang, Liangtai\\nSun, Lu Chen, et al. Moba: A two-level agent system for efficient mobile task automation. arXiv preprint\\narXiv:2410.13757 , 2024.\\n[1050] Zhenran Xu, Senbao Shi, Baotian Hu, Jindi Yu, Dongfang Li, Min Zhang, and Yuxiang Wu. Towards reasoning\\nin large language models via multi-agent peer review collaboration. arXiv preprint arXiv:2311.08152 , 2023.\\n[1051] Guhong Chen, Liyang Fan, Zihan Gong, Nan Xie, Zixuan Li, Ziqiang Liu, Chengming Li, Qiang Qu, Shiwen\\nNi, and Min Yang. Agentcourt: Simulating court with adversarial evolvable lawyer agents. arXiv preprint\\narXiv:2408.08089 , 2024.\\n[1052] Zhangyue Yin, Qiushi Sun, Cheng Chang, Qipeng Guo, Junqi Dai, Xuanjing Huang, and Xipeng Qiu. Exchange-\\nof-thought: Enhancing large language model capabilities through cross-model communication. arXiv preprint\\narXiv:2312.01823 , 2023.\\n245\\n[1053] Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian\\nYang, Jiaheng Liu, et al. Autokaggle: A multi-agent framework for autonomous data science competitions.\\narXiv preprint arXiv:2410.20424 , 2024.\\n[1054] Chuyi Shang, Amos You, Sanjay Subramanian, Trevor Darrell, and Roei Herzig. Traveler: A modular\\nmulti-lmm agent framework for video question-answering. arXiv preprint arXiv:2404.01476 , 2024.\\n[1055] Junzhi Chen, Juhao Liang, and Benyou Wang. Smurfs: Leveraging multiple proficiency agents with context-\\nefficiency for tool planning, 2024.\\n[1056] Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila\\nTakayama, Fei Xia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, and Anirudha Majumdar. Robots that\\nask for help: Uncertainty alignment for large language model planners. In Proceedings of the Conference on\\nRobot Learning (CoRL) , 2023.\\n[1057] Yijia Shao, Vinay Samuel, Yucheng Jiang, John Yang, and Diyi Yang. Collaborative gym: A framework for\\nenabling and evaluating human-agent collaboration, 2025. URL https://arxiv.org/abs/2412.15701 .\\n[1058] Varun Nair, Elliot Schumacher, Geoffrey Tso, and Anitha Kannan. Dera: enhancing large language model\\ncompletions with dialog-enabled resolving agents. arXiv preprint arXiv:2303.17071 , 2023.\\n[1059] Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, and Jordan Boyd-Graber. Getting more out of mixture\\nof language model reasoning experts. In Findings of the Association for Computational Linguistics: EMNLP\\n2023 , pages 8234–8249, 2023.\\n[1060] Philip Schroeder, Nathaniel Morgan, Hongyin Luo, and James Glass. Thread: Thinking deeper with recursive\\nspawning. arXiv preprint arXiv:2405.17402 , 2024.\\n[1061] Tongxuan Liu, Xingyu Wang, Weizhe Huang, Wenjiang Xu, Yuting Zeng, Lei Jiang, Hailong Yang, and Jing\\nLi. Groupdebate: Enhancing the efficiency of multi-agent debate using group discussion. arXiv preprint\\narXiv:2409.14051 , 2024.\\n[1062] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. A dynamic llm-powered agent network for\\ntask-oriented agent collaboration. In First Conference on Language Modeling , 2024.\\n[1063] Jintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu, Bryan Hooi, and Shumin Deng. Exploring collaboration\\nmechanisms for LLM agents: A social psychology view. arXiv preprint arXiv:2310.02124 , 2023.\\n[1064] Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, Zhaozhuo Xu, and Chaoyang He. Llm multi-agent\\nsystems: Challenges and open problems. arXiv preprint arXiv:2402.03578 , 2024.\\n[1065] Siyue Ren, Zhiyao Cui, Ruiqi Song, Zhen Wang, and Shuyue Hu. Emergence of social norms in generative\\nagent societies: principles and architecture. arXiv preprint arXiv:2403.08251 , 2024.\\n[1066] Aron Vallinder and Edward Hughes. Cultural evolution of cooperation among llm agents. arXiv preprint\\narXiv:2412.10270 , 2024.\\n[1067] Nathalia Nascimento, Paulo Alencar, and Donald Cowan. Self-adaptive large language model (LLM)-based\\nmultiagent systems. In 2023 IEEE International Conference on Autonomic Computing and Self-Organizing\\nSystems Companion (ACSOS-C) , pages 104–109. IEEE, 2023.\\n[1068] Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See Kiong Ng, and Jiashi Feng.\\nMAgIC: Benchmarking large language model powered multi-agent in cognition, adaptability, rationality and\\ncollaboration. arXiv preprint arXiv:2311.08562 , 2023.\\n[1069] Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang,\\nAnji Liu, Song-Chun Zhu, et al. Proagent: Building proactive cooperative AI with large language models.\\nCoRR , 2023.\\n[1070] Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun Gong, Chao Zhang, and Yelong Shen. Adapting LLM\\nagents through communication. arXiv preprint arXiv:2310.01444 , 2023.\\n[1071] Vighnesh Subramaniam, Yilun Du, Joshua B Tenenbaum, Antonio Torralba, Shuang Li, and Igor Mordatch.\\nMultiagent finetuning: Self improvement with diverse reasoning chains. arXiv preprint arXiv:2501.05707 ,\\n2025.\\n[1072] Wanjia Zhao, Mert Yuksekgonul, Shirley Wu, and James Zou. Sirius: Self-improving multi-agent systems via\\nbootstrapped reasoning. arXiv preprint arXiv:2502.04780 , 2025.\\n[1073] Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and Xian Li.\\nSweet-rl: Training multi-turn llm agents on collaborative reasoning tasks. arXiv preprint arXiv:2503.15478 ,\\n2025.\\n246\\n[1074] Haoyi Xiong, Zhiyuan Wang, Xuhong Li, Jiang Bian, Zeke Xie, Shahid Mumtaz, Anwer Al-Dulaimi, and\\nLaura E Barnes. Converging paradigms: The synergy of symbolic and connectionist ai in LLM-empowered\\nautonomous agents. arXiv preprint arXiv:2407.08516 , 2024.\\n[1075] Houcheng Jiang, Junfeng Fang, Tianyu Zhang, An Zhang, Ruipeng Wang, Tao Liang, and Xiang Wang.\\nNeuron-level sequential editing for large language models. arXiv preprint arXiv:2410.04045 , 2024.\\n[1076] Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. minif2f: a cross-system benchmark for formal\\nolympiad-level mathematics. In The Tenth International Conference on Learning Representations, ICLR\\n2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022. URL https://openreview.net/forum?\\nid=9ZPegFuFTFv .\\n[1077] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harri\\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael\\nPetrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,\\nAlethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such,\\nDave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen\\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William\\nSaunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec\\nRadford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario\\nAmodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained\\non code. CoRR , abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374 .\\n[1078] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\\nSamir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with\\nAPPS. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing\\nSystems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021,\\nvirtual , 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\\nc24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html .\\n[1079] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering\\nchallenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio, editors,\\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019,\\nVolume 1 (Long and Short Papers) , pages 4149–4158. Association for Computational Linguistics, 2019.\\ndoi:10.18653/V1/N19-1421. URL https://doi.org/10.18653/v1/n19-1421 .\\n[1080] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a\\nlaptop? A question answering benchmark with implicit reasoning strategies. Trans. Assoc. Comput. Linguistics ,\\n9:346–361, 2021. doi:10.1162/TACL_A_00370. URL https://doi.org/10.1162/tacl_a_00370 .\\n[1081] Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa: a\\nnovel resource for question answering on scholarly articles. Int. J. Digit. Libr. , 23(3):289–301, 2022.\\ndoi:10.1007/S00799-022-00329-Y. URL https://doi.org/10.1007/s00799-022-00329-y .\\n[1082] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised\\nchallenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July\\n30 - August 4, Volume 1: Long Papers , pages 1601–1611. Association for Computational Linguistics, 2017.\\ndoi:10.18653/V1/P17-1147. URL https://doi.org/10.18653/v1/P17-1147 .\\n[1083] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to\\nsolve math word problems. CoRR , abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168 .\\n[1084] Albert Qiaochu Jiang, Wenda Li, Jesse Michael Han, and Yuhuai Wu. Lisa: Language models of isabelle proofs.\\nIn6th Conference on Artificial Intelligence and Theorem Proving , pages 378–392, 2021.\\n[1085] Wei Xiong, Chengshuai Shi, Jiaming Shen, Aviv Rosenberg, Zhen Qin, Daniele Calandriello, Misha Khalman,\\nRishabh Joshi, Bilal Piot, Mohammad Saleh, Chi Jin, Tong Zhang, and Tianqi Liu. Building math agents with\\nmulti-turn iterative preference learning, 2025. URL https://arxiv.org/abs/2409.02392 .\\n[1086] Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency,\\nYonatan Bisk, Daniel Fried, Graham Neubig, and Maarten Sap. Sotopia: Interactive evaluation for social\\nintelligence in language agents, 2024. URL https://arxiv.org/abs/2310.11667 .\\n[1087] Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles,\\nJames Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume,\\n247\\nIgor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Mol-\\nloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu,\\nand Oriol Vinyals. Competition-level code generation with alphacode. CoRR , abs/2203.07814, 2022.\\ndoi:10.48550/ARXIV .2203.07814. URL https://doi.org/10.48550/arXiv.2203.07814 .\\n[1088] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caim-\\ning Xiong. Codegen: An open large language model for code with multi-turn program synthesis. In The\\nEleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 .\\nOpenReview.net, 2023. URL https://openreview.net/forum?id=iaYcJKpY2B_ .\\n[1089] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-Tau Yih, Daniel\\nFried, Sida I. Wang, and Tao Yu. DS-1000: A natural and reliable benchmark for data science code generation.\\nIn Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett,\\neditors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,\\nUSA, volume 202 of Proceedings of Machine Learning Research , pages 18319–18345. PMLR, 2023. URL\\nhttps://proceedings.mlr.press/v202/lai23b.html .\\n[1090] Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. Execution-based evaluation for open-domain\\ncode generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for\\nComputational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023 , pages 1271–1290. Association for\\nComputational Linguistics, 2023. doi:10.18653/V1/2023.FINDINGS-EMNLP.89. URL https://doi.org/\\n10.18653/v1/2023.findings-emnlp.89 .\\n[1091] Jiangyi Deng, Xinfeng Li, Yanjiao Chen, Yijie Bai, Haiqin Weng, Yan Liu, Tao Wei, and Wenyuan Xu.\\nRaconteur: A knowledgeable, insightful, and portable llm-powered shell command explainer. In In the 32nd\\nAnnual Network and Distributed System Security Symposium (NDSS) , 2025.\\n[1092] Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish\\nSabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark. Think you have solved direct-answer question\\nanswering? try arc-da, the direct-answer AI2 reasoning challenge. CoRR , abs/2102.03315, 2021. URL\\nhttps://arxiv.org/abs/2102.03315 .\\n[1093] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.\\nBoolq: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and\\nThamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,\\nJune 2-7, 2019, Volume 1 (Long and Short Papers) , pages 2924–2936. Association for Computational Linguistics,\\n2019. doi:10.18653/V1/N19-1300. URL https://doi.org/10.18653/v1/n19-1300 .\\n[1094] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity?\\nA new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and\\nJun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro-\\ncessing, Brussels, Belgium, October 31 - November 4, 2018 , pages 2381–2391. Association for Computational\\nLinguistics, 2018. doi:10.18653/V1/D18-1260. URL https://doi.org/10.18653/v1/d18-1260 .\\n[1095] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial\\nwinograd schema challenge at scale. Commun. ACM , 64(9):99–106, 2021. doi:10.1145/3474381. URL\\nhttps://doi.org/10.1145/3474381 .\\n[1096] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really\\nfinish your sentence? In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings of\\nthe 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-\\nAugust 2, 2019, Volume 1: Long Papers , pages 4791–4800. Association for Computational Linguistics, 2019.\\ndoi:10.18653/V1/P19-1472. URL https://doi.org/10.18653/v1/p19-1472 .\\n[1097] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social iqa: Commonsense reason-\\ning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings\\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7,\\n2019 , pages 4462–4472. Association for Computational Linguistics, 2019. doi:10.18653/V1/D19-1454. URL\\nhttps://doi.org/10.18653/v1/D19-1454 .\\n[1098] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical\\ncommonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020,\\nThe Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI\\nSymposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12,\\n248\\n2020 , pages 7432–7439. AAAI Press, 2020. doi:10.1609/AAAI.V34I05.6239. URL https://doi.org/10.\\n1609/aaai.v34i05.6239 .\\n[1099] Keisuke Sakaguchi, Chandra Bhagavatula, Ronan Le Bras, Niket Tandon, Peter Clark, and Yejin Choi. proscript:\\nPartially ordered scripts generation via pre-trained language models. CoRR , abs/2104.08251, 2021. URL\\nhttps://arxiv.org/abs/2104.08251 .\\n[1100] Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-\\nthought. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,\\nMay 1-5, 2023 . OpenReview.net, 2023. URL https://openreview.net/forum?id=qFVVBzXxR2V .\\n[1101] Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english\\nmath word problem solvers. CoRR , abs/2106.15772, 2021. URL https://arxiv.org/abs/2106.15772 .\\n[1102] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.\\nMathqa: Towards interpretable math word problem solving with operation-based formalisms. In Jill Burstein,\\nChristy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , pages 2357–2367. Association\\nfor Computational Linguistics, 2019. doi:10.18653/V1/N19-1245. URL https://doi.org/10.18653/v1/\\nn19-1245 .\\n[1103] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning\\nto solve and explain algebraic word problems. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of\\nthe 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada,\\nJuly 30 - August 4, Volume 1: Long Papers , pages 158–167. Association for Computational Linguistics, 2017.\\ndoi:10.18653/V1/P17-1015. URL https://doi.org/10.18653/v1/P17-1015 .\\n[1104] Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS:\\nA math word problem repository. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, NAACL\\nHLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016 , pages 1152–\\n1157. The Association for Computational Linguistics, 2016. doi:10.18653/V1/N16-1136. URL https:\\n//doi.org/10.18653/v1/n16-1136 .\\n[1105] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP:\\nA reading comprehension benchmark requiring discrete reasoning over paragraphs. In Jill Burstein, Christy\\nDoran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis,\\nMN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , pages 2368–2378. Association for Computational\\nLinguistics, 2019. doi:10.18653/V1/N19-1246. URL https://doi.org/10.18653/v1/n19-1246 .\\n[1106] Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hanna Hajishirzi, Yejin Choi, and Kyunghyun Cho. Naturalproofs:\\nMathematical theorem proving in natural language. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceed-\\nings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and\\nBenchmarks 2021, December 2021, virtual , 2021. URL https://datasets-benchmarks-proceedings.\\nneurips.cc/paper/2021/hash/d9d4f495e875a2e075a1a4a6e1b9770f-Abstract-round1.html .\\n[1107] Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir Radev, and Jeremy Avi-\\ngad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. CoRR , abs/2302.12433,\\n2023. doi:10.48550/ARXIV .2302.12433. URL https://doi.org/10.48550/arXiv.2302.12433 .\\n[1108] Wei Liu, Chenxi Wang, Yifei Wang, Zihao Xie, Rennai Qiu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng\\nYang, and Chen Qian. Autonomous agents for collaborative task under information asymmetry, 2024. URL\\nhttps://arxiv.org/abs/2406.14928 .\\n[1109] Timothy Ossowski, Jixuan Chen, Danyal Maqbool, Zefan Cai, Tyler Bradshaw, and Junjie Hu. Comma: A\\ncommunicative multimodal multi-agent benchmark, 2025. URL https://arxiv.org/abs/2410.07553 .\\n[1110] Yang Liu, Peng Sun, and Hang Li. Large language models as agents in two-player games, 2024. URL\\nhttps://arxiv.org/abs/2402.08078 .\\n[1111] Taehoon Kim. Ethereum AI agent coordinator (EAAC): A framework for AI agent activity coordination. In\\nAgentic Markets Workshop at ICML 2024 , 2024. URL https://openreview.net/forum?id=n2dVVwZwPP .\\n[1112] Yohei Nakajima. Babyagi arena, 2023. URL https://github.com/yoheinakajima/babyagi-arena .\\n[1113] Shankar Kumar Jeyakumar, Alaa Alameer Ahmad, and Adrian Garret Gabriel. Advancing agentic systems:\\nDynamic task decomposition, tool integration and evaluation using novel metrics and dataset. In NeurIPS 2024\\nWorkshop on Open-World Agents , 2024. URL https://openreview.net/forum?id=kRRLhPp7CO .\\n249\\n[1114] Haofei Yu, Zhaochen Hong, Zirui Cheng, Kunlun Zhu, Keyang Xuan, Jinwei Yao, Tao Feng, and Jiaxuan\\nYou. Researchtown: Simulator of human research community, 2024. URL https://arxiv.org/abs/2412.\\n17767 .\\n[1115] Xian Gao, Zongyun Zhang, Mingye Xie, Ting Liu, and Yuzhuo Fu. Graph of ai ideas: Leveraging knowledge\\ngraphs and llms for ai research idea generation, 2025. URL https://arxiv.org/abs/2503.08549 .\\n[1116] Junzhe Chen, Xuming Hu, Shuodi Liu, Shiyu Huang, Wei-Wei Tu, Zhaofeng He, and Lijie Wen. Llmarena:\\nAssessing capabilities of large language models in dynamic multi-agent environments, 2024. URL https:\\n//arxiv.org/abs/2402.16499 .\\n[1117] Richard Zhuang, Akshat Gupta, Richard Yang, Aniket Rahane, Zhengyu Li, and Gopala Anumanchipalli.\\nPokerbench: Training large language models to become professional poker players, 2025. URL https:\\n//arxiv.org/abs/2501.08328 .\\n[1118] Nicoló Fontana, Francesco Pierri, and Luca Maria Aiello. Nicer than humans: How do large language models\\nbehave in the prisoner’s dilemma?, 2024. URL https://arxiv.org/abs/2406.13605 .\\n[1119] Sihao Hu, Tiansheng Huang, and Ling Liu. Pokellmon: A human-parity agent for pokemon battles with large\\nlanguage models, 2024. URL https://arxiv.org/abs/2402.01118 .\\n[1120] Qiuejie Xie, Qiming Feng, Tianqi Zhang, Qingqiu Li, Linyi Yang, Yuejie Zhang, Rui Feng, Liang He, Shang\\nGao, and Yue Zhang. Human simulacra: Benchmarking the personification of large language models, 2025.\\nURL https://arxiv.org/abs/2402.18180 .\\n[1121] Rong Ye, Yongxin Zhang, Yikai Zhang, Haoyu Kuang, Zhongyu Wei, and Peng Sun. Multi-agent kto:\\nReinforcing strategic interactions of large language model in language game, 2025. URL https://arxiv.\\norg/abs/2501.14225 .\\n[1122] Chengxing Xie and Difan Zou. A human-like reasoning framework for multi-phases planning task with large\\nlanguage models, 2024. URL https://arxiv.org/abs/2405.18208 .\\n[1123] Yauwai Yim, Chunkit Chan, Tianyu Shi, Zheye Deng, Wei Fan, Tianshi Zheng, and Yangqiu Song. Evaluating\\nand enhancing llms agent based on theory of mind in guandan: A multi-player cooperative game under imperfect\\ninformation, 2024. URL https://arxiv.org/abs/2408.02559 .\\n[1124] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu,\\nYi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou.\\nAgentverse: Facilitating multi-agent collaboration and exploring emergent behaviors, 2023. URL https:\\n//arxiv.org/abs/2308.10848 .\\n[1125] Han Wang, Binbin Chen, Tieying Zhang, and Baoxiang Wang. Learning to communicate through implicit\\ncommunication channels, 2025. URL https://arxiv.org/abs/2411.01553 .\\n[1126] Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and Yongfeng\\nZhang. War and peace (waragent): Large language model-based multi-agent simulation of world wars, 2024.\\nURL https://arxiv.org/abs/2311.17227 .\\n[1127] Yizhe Huang, Xingbo Wang, Hao Liu, Fanqi Kong, Aoyang Qin, Min Tang, Song-Chun Zhu, Mingjie Bi, Siyuan\\nQi, and Xue Feng. Adasociety: An adaptive environment with social structures for multi-agent decision-making,\\n2025. URL https://arxiv.org/abs/2411.03865 .\\n[1128] Jen tse Huang, Jiaxu Zhou, Tailin Jin, Xuhui Zhou, Zixi Chen, Wenxuan Wang, Youliang Yuan, Michael R.\\nLyu, and Maarten Sap. On the resilience of llm-based multi-agent collaboration with faulty agents, 2025. URL\\nhttps://arxiv.org/abs/2408.00989 .\\n[1129] Zehang Deng, Yongjian Guo, Changzhou Han, Wanlun Ma, Junwu Xiong, Sheng Wen, and Yang Xiang. Ai\\nagents under threat: A survey of key security challenges and future pathways. ACM Computing Surveys , 2024.\\n[1130] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang, Tianwei Zhang, Yepang Liu,\\nHaoyu Wang, Yan Zheng, et al. Prompt injection attack against llm-integrated applications. arXiv preprint\\narXiv:2306.05499 , 2023.\\n[1131] Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, and Xu Sun. Watch out for your agents!\\ninvestigating backdoor threats to LLM-based agents. arXiv preprint arXiv:2402.11208 , 2024.\\n[1132] Haibo Jin, Leyang Hu, Xinuo Li, Peiyan Zhang, Chonghan Chen, Jun Zhuang, and Haohan Wang. Jailbreakzoo:\\nSurvey, landscapes, and horizons in jailbreaking large language and vision-language models. arXiv preprint\\narXiv:2407.01599 , 2024.\\n[1133] Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, and Qi Li. Jailbreak attacks and\\ndefenses against large language models: A survey. arXiv preprint arXiv:2407.04295 , 2024.\\n250\\n[1134] Andy Zou, Zifan Wang, Norman Mu, and Jacob Andreas. Universal and transferable adversarial attacks on\\naligned language models. arXiv preprint arXiv:2307.15043 , 2023.\\n[1135] Yihao Zhang and Zeming Wei. Boosting jailbreak attack with momentum. arXiv preprint arXiv:2405.01229 ,\\n2024.\\n[1136] Xiaojun Jia, Tianyu Pang, Chao Du, Yihao Huang, Jindong Gu, Yang Liu, Xiaochun Cao, and Min Lin. Improved\\ntechniques for optimization-based jailbreaking on large language models. arXiv preprint arXiv:2405.21018 ,\\n2024.\\n[1137] Yifan Luo, Zhennan Zhou, Meitan Wang, and Bin Dong. Jailbreak instruction-tuned LLMs via end-of-sentence\\nmlp re-weighting. arXiv preprint arXiv:2410.10150 , 2024.\\n[1138] Tianlong Li, Xiaoqing Zheng, and Xuanjing Huang. Open the pandora’s box of llms: Jailbreaking LLMs\\nthrough representation engineering. arXiv preprint arXiv:2401.06824 , 2024.\\n[1139] Leyang Hu and Boran Wang. DROJ: A prompt-driven attack against large language models. arXiv preprint\\narXiv:2411.09125 , 2024.\\n[1140] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on\\naligned large language models. arXiv preprint arXiv:2310.04451 , 2023.\\n[1141] Xuancun Lu, Zhengxian Huang, Xinfeng Li, Xiaoyu Ji, and Wenyuan Xu. Poex: Policy executable embodied\\nAI jailbreak attacks. arXiv preprint arXiv:2412.16633 , 2024.\\n[1142] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does LLM safety training fail?\\nAdvances in Neural Information Processing Systems , 36, 2023.\\n[1143] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. Jailbreaking\\nblack box large language models in twenty queries. In arXiv preprint arXiv:2310.08419 , 2023.\\n[1144] Haibo Jin, Andy Zhou, Joe Menke, and Haohan Wang. Jailbreaking large language models against moderation\\nguardrails via cipher characters. Advances in Neural Information Processing Systems , 37:59408–59435, 2025.\\n[1145] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. Visual\\nadversarial examples jailbreak aligned large language models. In AAAI Conference on Artificial Intelligence ,\\nvolume 38, pages 21527–21536, 2024.\\n[1146] Haibo Jin, Ruoxi Chen, Andy Zhou, Yang Zhang, and Haohan Wang. Guard: Role-playing to generate natural-\\nlanguage jailbreakings to test guideline adherence of large language models. arXiv preprint arXiv:2402.03299 ,\\n2024.\\n[1147] Teng Ma, Xiaojun Jia, Ranjie Duan, Xinfeng Li, Yihao Huang, Zhixuan Chu, Yang Liu, and Wenqi Ren.\\nHeuristic-induced multimodal risk distribution jailbreak attack for multimodal large language models. arXiv\\npreprint arXiv:2412.05934 , 2024.\\n[1148] Sensen Gao, Xiaojun Jia, Yihao Huang, Ranjie Duan, Jindong Gu, Yang Liu, and Qing Guo. Rt-attack:\\nJailbreaking text-to-image models via random token. arXiv preprint arXiv:2408.13896 , 2024.\\n[1149] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what\\nyou’ve signed up for: Compromising real-world LLM-integrated applications with indirect prompt injection. In\\nProceedings of the 16th ACM Workshop on Artificial Intelligence and Security , pages 79–90, 2023.\\n[1150] Xiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, and Chaowei Xiao. Automatic and universal prompt\\ninjection attacks against large language models. arXiv preprint arXiv:2403.04957 , 2024.\\n[1151] Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, and Neil Zhenqiang Gong.\\nOptimization-based prompt injection attack to LLM-as-a-judge. In Proceedings of the 2024 on ACM SIGSAC\\nConference on Computer and Communications Security , pages 660–674, 2024.\\n[1152] Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang. Benchmarking indirect prompt injections in\\ntool-integrated large language model agents. arXiv preprint arXiv:2403.02691 , 2024.\\n[1153] Johann Rehberger. Trust no ai: Prompt injection along the cia security triad. arXiv preprint arXiv:2412.06090 ,\\n2024.\\n[1154] Subaru Kimura, Ryota Tanaka, Shumpei Miyawaki, Jun Suzuki, and Keisuke Sakaguchi. Empirical anal-\\nysis of large vision-language models against goal hijacking via visual prompt injection. arXiv preprint\\narXiv:2408.03554 , 2024.\\n[1155] Edoardo Debenedetti, Javier Rando, Daniel Paleka, Silaghi Fineas Florin, Dragos Albastroiu, Niv Cohen, Yuval\\nLemberg, Reshmi Ghosh, Rui Wen, Ahmed Salem, et al. Dataset and lessons learned from the 2024 satml LLM\\ncapture-the-flag competition. arXiv preprint arXiv:2406.07954 , 2024.\\n251\\n[1156] Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-François Bouchard, Chenglei Si, Svetlina Anati, Valen\\nTagliabue, Anson Kost, Christopher Carnahan, and Jordan Boyd-Graber. Ignore this title and hackaprompt:\\nExposing systemic vulnerabilities of LLMs through a global prompt hacking competition. In Proceedings of\\nthe 2023 Conference on Empirical Methods in Natural Language Processing , pages 4945–4977, 2023.\\n[1157] Yucheng Zhang, Qinfeng Li, Tianyu Du, Xuhong Zhang, et al. Hijackrag: Hijacking attacks against retrieval-\\naugmented large language models. arXiv preprint arXiv:2410.22832 , 2025.\\n[1158] Cody Clop and Yannick Teglia. Backdoored retrievers for prompt injection attacks on retrieval augmented\\ngeneration of large language models. arXiv preprint arXiv:2410.14479 , 2024.\\n[1159] Donghyun Lee and Mo Tiwari. Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems.\\narXiv preprint arXiv:2410.07283 , 2024.\\n[1160] Fredrik Nestaas, Edoardo Debenedetti, and Florian Tramèr. Adversarial search engine optimization for large\\nlanguage models. arXiv preprint arXiv:2406.18382 , 2024.\\n[1161] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto,\\nand Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys , 55(12):\\n1–38, 2023.\\n[1162] Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, and Mark Steedman.\\nSources of hallucination by large language models on inference tasks. arXiv preprint arXiv:2305.14552 , 2023.\\n[1163] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua\\nPeng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models: Principles,\\ntaxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232 , 2023.\\n[1164] Mobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun Araki, Arsalan Gundroo, Bingqing Wang, Rakesh R Menon,\\nMd Rizwan Parvez, and Zhe Feng. DELUCIONQA: Detecting Hallucinations in Domain-specific Question\\nAnswering. Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 3737–3748, 2023.\\n[1165] Haoqiang Kang and Xiao-Yang Liu. Deficiency of large language models in finance: An empirical examination\\nof hallucination. In I Can’t Believe It’s Not Better Workshop: Failure Modes in the Age of Foundation Models ,\\n2023.\\n[1166] Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large\\nlanguage models. arXiv preprint arXiv:2401.11817 , 2024.\\n[1167] Jio Oh, Soyeon Kim, Junseok Seo, Jindong Wang, Ruochen Xu, Xing Xie, and Steven Euijong Whang. Erbench:\\nAn entity-relationship based automatically verifiable hallucination benchmark for large language models. arXiv\\npreprint arXiv:2403.05266 , 2024.\\n[1168] Tian Yu, Shaolei Zhang, and Yang Feng. Truth-aware context selection: Mitigating the hallucinations of large\\nlanguage models being misled by untruthful contexts. arXiv preprint arXiv:2403.07556 , 2024.\\n[1169] Yiyi Chen, Qiongxiu Li, Russa Biswas, and Johannes Bjerva. Large language models are easily confused: A\\nquantitative metric, security implications and typological analysis. arXiv preprint arXiv:2410.13237 , 2024.\\n[1170] Zhiying Zhu, Zhiqing Sun, and Yiming Yang. Halueval-wild: Evaluating hallucinations of language models in\\nthe wild. arXiv preprint arXiv:2403.04307 , 2024.\\n[1171] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and\\nHuaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. In International\\nConference on Learning Representations , 2023.\\n[1172] Linxi Zhao, Yihe Deng, Weitong Zhang, and Quanquan Gu. Mitigating object hallucination in large vision-\\nlanguage models via classifier-free guidance. arXiv preprint arXiv:2402.08680 , 2024.\\n[1173] Leonardo Ranaldi and Giulia Pucci. When large language models contradict humans? large language models’\\nsycophantic behaviour. arXiv preprint arXiv:2311.09410 , 2023.\\n[1174] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen,\\nFurong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language\\nhallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 14375–14385, 2024.\\n[1175] Kedi Chen, Qin Chen, Jie Zhou, Yishen He, and Liang He. Diahalu: A dialogue-level hallucination evaluation\\nbenchmark for large language models. arXiv preprint arXiv:2403.00896 , 2024.\\n[1176] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He,\\nJiayi Zhou, Zhaowei Zhang, et al. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852 ,\\n2023.\\n252\\n[1177] Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ramana Kumar, Zac\\nKenton, Jan Leike, and Shane Legg. Specification gaming: The flip side of AI ingenuity, 2020. URL https://\\ndeepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/ .\\n[1178] Richard Ngo, Lawrence Chan, and Sören Mindermann. The alignment problem from a deep learning perspective.\\narXiv preprint arXiv:2209.00626 , 2022.\\n[1179] Shen Li, Liuyi Yao, Lan Zhang, and Yaliang Li. Safety layers in aligned large language models: The key to\\nLLM security. arXiv preprint arXiv:2408.17003 , 2024.\\n[1180] Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli Ouyang, and Yu Qiao. Emulated\\ndisalignment: Safety alignment for large language models may backfire! arXiv preprint arXiv:2402.12343 ,\\n2024.\\n[1181] Hasan Abed Al Kader Hammoud, Umberto Michieli, Fabio Pizzati, Philip Torr, Adel Bibi, Bernard Ghanem,\\nand Mete Ozay. Model merging and safety alignment: One bad model spoils the bunch. arXiv preprint\\narXiv:2406.14563 , 2024.\\n[1182] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov,\\nMuhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: A survey and guideline for evaluating large language\\nmodels’ alignment. arXiv preprint arXiv:2308.05374 , 2023.\\n[1183] Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal,\\nMengdi Wang, and Peter Henderson. Assessing the brittleness of safety alignment via pruning and low-rank\\nmodifications. arXiv preprint arXiv:2402.05162 , 2024.\\n[1184] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-\\ntuning aligned language models compromises safety, even when users do not intend to! arXiv preprint\\narXiv:2310.03693 , 2023.\\n[1185] Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, and Amnon Shashua. Fundamental limitations of\\nalignment in large language models. arXiv preprint arXiv:2304.11082 , 2023.\\n[1186] Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pre-trained models. In Proceedings\\nof the 58th Annual Meeting of the Association for Computational Linguistics , pages 5560–5574, 2020.\\n[1187] Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu, Wenhan Wang, Tianwei Zhang, and Yang\\nLiu. Badedit: Backdooring large language models by model editing. arXiv preprint arXiv:2403.13355 , 2024.\\n[1188] Tian Dong, Minhui Xue, Guoxing Chen, Rayne Holland, Yan Meng, Shaofeng Li, Zhen Liu, and Haojin Zhu.\\nThe philosopher’s stone: Trojaning plugins of large language models. arXiv preprint arXiv:2312.00374 , 2023.\\n[1189] Jaehan Kim, Minkyoo Song, Seung Ho Na, and Seungwon Shin. Obliviate: Neutralizing task-agnostic\\nbackdoors within the parameter-efficient fine-tuning paradigm. arXiv preprint arXiv:2409.14119 , 2024.\\n[1190] Sanghak Oh, Kiho Lee, Seonhye Park, Doowon Kim, and Hyoungshick Kim. Poisoned chatgpt finds work for\\nidle hands: Exploring developers’ coding practices with insecure suggestions from poisoned ai models. In 2024\\nIEEE Symposium on Security and Privacy (SP) , pages 1141–1159. IEEE, 2024.\\n[1191] Sumeet Ramesh Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip HS Torr, Lewis\\nHammond, and Christian Schroeder de Witt. Secret collusion among generative AI agents. arXiv preprint\\narXiv:2402.07510 , 2024.\\n[1192] Abdullah Arafat Miah and Yu Bi. Exploiting the vulnerability of large language models via defense-aware\\narchitectural backdoor. arXiv preprint arXiv:2409.01952 , 2024.\\n[1193] Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning language models during instruction\\ntuning. In International Conference on Machine Learning , pages 35413–35425. PMLR, 2023.\\n[1194] Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li. Agentpoison: Red-teaming llm agents\\nvia poisoning memory or knowledge bases. Advances in Neural Information Processing Systems , 37:130185–\\n130213, 2025.\\n[1195] Fatemeh Nazary, Yashar Deldjoo, and Tommaso di Noia. Poison-rag: Adversarial data poisoning attacks on\\nretrieval-augmented generation in recommender systems. arXiv preprint arXiv:2501.11759 , 2025.\\n[1196] Tingchen Fu, Mrinank Sharma, Philip Torr, Shay B Cohen, David Krueger, and Fazl Barez. Poisonbench:\\nAssessing large language model vulnerability to data poisoning. arXiv preprint arXiv:2410.08811 , 2024.\\n[1197] Bocheng Chen, Hanqing Guo, Guangjing Wang, Yuanda Wang, and Qiben Yan. The dark side of human\\nfeedback: Poisoning large language models via user inputs. arXiv preprint arXiv:2409.00787 , 2024.\\n253\\n[1198] Dillon Bowen, Brendan Murphy, Will Cai, David Khachaturov, Adam Gleave, and Kellin Pelrine. Scaling laws\\nfor data poisoning in LLMs. arXiv e-prints , pages arXiv–2408, 2024.\\n[1199] Jiaming He, Wenbo Jiang, Guanyu Hou, Wenshu Fan, Rui Zhang, and Hongwei Li. Talk too much: Poisoning\\nlarge language models under token limit. arXiv preprint arXiv:2404.14795 , 2024.\\n[1200] Tim Baumgärtner, Yang Gao, Dana Alon, and Donald Metzler. Best-of-venom: Attacking rlhf by injecting\\npoisoned preference data. arXiv preprint arXiv:2404.05530 , 2024.\\n[1201] Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham,\\nDaniel M Ziegler, Tim Maxwell, Newton Cheng, et al. Sleeper agents: Training deceptive LLMs that persist\\nthrough safety training. arXiv preprint arXiv:2401.05566 , 2024.\\n[1202] Fangzhou Wu, Shutong Wu, Yulong Cao, and Chaowei Xiao. Wipi: A new web threat for LLM-driven web\\nagents. arXiv preprint arXiv:2403.09875 , 2024.\\n[1203] Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Qi Alfred Chen, and\\nQi Zhu. Exploring backdoor attacks against large language model-based decision making. arXiv preprint\\narXiv:2405.20774 , 2024.\\n[1204] Huaizhi Ge, Yiming Li, Qifan Wang, Yongfeng Zhang, and Ruixiang Tang. When backdoors speak: Under-\\nstanding LLM backdoor attacks through model-generated explanations. arXiv preprint arXiv:2411.12701 ,\\n2024.\\n[1205] Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren,\\nand Hongxia Jin. Backdooring instruction-tuned large language models with virtual prompt injection. In\\nProceedings of the 2024 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies (Volume 1: Long Papers) , pages 6065–6086, 2024.\\n[1206] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against\\nmachine learning models. In 2017 IEEE symposium on security and privacy (SP) , pages 3–18. IEEE, 2017.\\n[1207] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and\\ntesting unintended memorization in neural networks. In 28th USENIX security symposium (USENIX security\\n19), pages 267–284, 2019.\\n[1208] Christopher A Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot. Label-only member-\\nship inference attacks. In International conference on machine learning , pages 1964–1974. PMLR, 2021.\\n[1209] Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, and Tao Jiang. Practical membership inference\\nattacks against fine-tuned large language models via self-prompt calibration. arXiv preprint arXiv:2311.06062 ,\\n2023.\\n[1210] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership\\ninference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy (SP) , pages 1897–1914.\\nIEEE, 2022.\\n[1211] Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Philip S Yu, and Xuyun Zhang. Membership\\ninference attacks on machine learning: A survey. ACM Computing Surveys (CSUR) , 54(11s):1–37, 2022.\\n[1212] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-V oss, Katherine Lee, Adam\\nRoberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models.\\nIn30th USENIX Security Symposium (USENIX Security 21) , pages 2633–2650, 2021.\\n[1213] Yang Bai, Ge Pei, Jindong Gu, Yong Yang, and Xingjun Ma. Special characters attack: Toward scalable training\\ndata extraction from large language models. arXiv preprint arXiv:2405.05990 , 2024.\\n[1214] Zhexin Zhang, Jiaxin Wen, and Minlie Huang. Ethicist: Targeted training data extraction through loss smoothed\\nsoft prompting and calibrated confidence estimation. arXiv preprint arXiv:2307.04401 , 2023.\\n[1215] John X Morris, Wenting Zhao, Justin T Chiu, Vitaly Shmatikov, and Alexander M Rush. Language model\\ninversion. arXiv preprint arXiv:2311.13647 , 2023.\\n[1216] Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang. Privacy risks of general-purpose language models. In\\n2020 IEEE Symposium on Security and Privacy (SP) , pages 1314–1331. IEEE, 2020.\\n[1217] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang.\\nQuantifying memorization across neural language models. arXiv preprint arXiv:2202.07646 , 2022.\\n[1218] Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A Feder\\nCooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, et al. Stealing part of a production\\nlanguage model. arXiv preprint arXiv:2403.06634 , 2024.\\n254\\n[1219] Yash More, Prakhar Ganesh, and Golnoosh Farnadi. Towards more realistic extraction attacks: An adversarial\\nperspective. arXiv preprint arXiv:2407.02596 , 2024.\\n[1220] Fábio Perez and Ian Ribeiro. Ignore previous prompt: Attack techniques for language models. In Workshop on\\nTrustworthy and Socially Responsible Machine Learning (TSRML@ NeurIPS 2022) , 2022.\\n[1221] Xinyue Shen, Yiting Qu, Michael Backes, and Yang Zhang. Prompt stealing attacks against {Text-to-Image }\\ngeneration models. In 33rd USENIX Security Symposium (USENIX Security 24) , pages 5823–5840, 2024.\\n[1222] Zhifeng Jiang, Zhihua Jin, and Guoliang He. Safeguarding system prompts for llms. arXiv preprint\\narXiv:2412.13426 , 2024.\\n[1223] Xinyao Zheng, Husheng Han, Shangyi Shi, Qiyan Fang, Zidong Du, Qi Guo, and Xing Hu. Inputsnatch:\\nStealing input in LLM services via timing side-channel attacks. arXiv preprint arXiv:2411.18191 , 2024.\\n[1224] Yiming Zhang, Nicholas Carlini, and Daphne Ippolito. Effective prompt extraction from language models.\\narXiv preprint arXiv:2307.06865 , 2023.\\n[1225] Rui Wen, Tianhao Wang, Michael Backes, Yang Zhang, and Ahmed Salem. Last one standing: A compar-\\native analysis of security and privacy of soft prompt tuning, lora, and in-context learning. arXiv preprint\\narXiv:2310.11397 , 2023.\\n[1226] Yanjie Zhao, Xinyi Hou, Shenao Wang, and Haoyu Wang. Llm app store analysis: A vision and roadmap. ACM\\nTransactions on Software Engineering and Methodology , 2024.\\n[1227] Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, and Zonghui Wang. Prsa: Prompt\\nreverse stealing attacks against large language models. arXiv preprint arXiv:2402.07870 , 2024.\\n[1228] Divyansh Agarwal, Alexander R Fabbri, Ben Risher, Philippe Laban, Shafiq Joty, and Chien-Sheng Wu. Prompt\\nleakage effect and defense strategies for multi-turn llm interactions. arXiv preprint arXiv:2404.16251 , 2024.\\n[1229] Divyansh Agarwal, Alexander R Fabbri, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu.\\nInvestigating the prompt leakage effect and black-box defenses for multi-turn LLM interactions. arXiv preprint\\narXiv:2402.06770 , 2024.\\n[1230] Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, and Haoyang Li. Why are my prompts leaked? unraveling\\nprompt extraction threats in customized large language models. arXiv preprint arXiv:2408.02416 , 2024.\\n[1231] Bo Hui, Haolin Yuan, Neil Gong, Philippe Burlina, and Yinzhi Cao. Pleak: Prompt leaking attacks against\\nlarge language model applications. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and\\nCommunications Security , pages 3600–3614, 2024.\\n[1232] Itay Yona, Ilia Shumailov, Jamie Hayes, and Nicholas Carlini. Stealing user prompts from mixture of experts.\\narXiv preprint arXiv:2410.22884 , 2024.\\n[1233] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and\\nYang Liu. Jailbreaker: Automated jailbreak across multiple large language model chatbots. arXiv preprint\\narXiv:2307.08715 , 2023.\\n[1234] Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. Gptfuzzer: Red teaming large language models with\\nauto-generated jailbreak prompts. arXiv preprint arXiv:2309.10253 , 2023.\\n[1235] Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun Li, Soheil Feizi, and Himabindu Lakkaraju.\\nCertifying llm safety against adversarial prompting. arXiv preprint arXiv:2309.02705 , 2023.\\n[1236] Alexander Robey, Eric Wong, Hamed Hassani, and George J Pappas. SmoothLLM: Defending large language\\nmodels against jailbreaking attacks. arXiv preprint arXiv:2310.03684 , 2023.\\n[1237] Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, and Qingyun Wu. Autodefense: Multi-agent LLM\\ndefense against jailbreak attacks. In Neurips Safe Generative AI Workshop 2024 , 2024. URL https://\\nopenreview.net/forum?id=WMwoSLAENS .\\n[1238] Zelong Li, Wenyue Hua, Hao Wang, He Zhu, and Yongfeng Zhang. Formal-llm: Integrating formal language\\nand natural language for controllable llm-based agents. arXiv preprint arXiv:2402.00798 , 2024.\\n[1239] Benji Peng, Ziqian Bi, Qian Niu, Ming Liu, Pohsun Feng, Tianyang Wang, Lawrence KQ Yan, Yizhu Wen,\\nYichao Zhang, and Caitlyn Heqi Yin. Jailbreaking and mitigation of vulnerabilities in large language models.\\narXiv preprint arXiv:2410.15236 , 2024.\\n[1240] Yuchen Yang, Hongwei Yao, Bingrun Yang, Yiling He, Yiming Li, Tianwei Zhang, and Zhan Qin. Tpia:\\nTowards target-specific prompt injection attack against code-oriented large language models. arXiv preprint\\narXiv:2407.09164 , 2024.\\n255\\n[1241] Md Ahsan Ayub and Subhabrata Majumdar. Embedding-based classifiers can detect prompt injection attacks.\\narXiv preprint arXiv:2410.22284 , 2024.\\n[1242] Sizhe Chen, Julien Piet, Chawin Sitawarin, and David Wagner. Struq: Defending against prompt injection with\\nstructured queries. arXiv preprint arXiv:2402.06363 , 2024.\\n[1243] Feiran Jia, Tong Wu, Xin Qin, and Anna Squicciarini. The task shield: Enforcing task alignment to defend\\nagainst indirect prompt injection in LLM agents. arXiv preprint arXiv:2412.16682 , 2024.\\n[1244] Kuo-Han Hung, Ching-Yun Ko, Ambrish Rawat, I Chung, Winston H Hsu, Pin-Yu Chen, et al. Attention\\ntracker: Detecting prompt injection attacks in llms. arXiv preprint arXiv:2411.00348 , 2024.\\n[1245] Yulin Chen, Haoran Li, Zihao Zheng, Yangqiu Song, Dekai Wu, and Bryan Hooi. Defense against prompt\\ninjection attack by leveraging attack techniques. arXiv preprint arXiv:2411.00459 , 2024.\\n[1246] Rongwu Xu, Brian Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu,\\nand Han Qiu. The earth is flat because...: Investigating llms’ belief towards misinformation via persuasive\\nconversation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 16259–16303, 2024.\\n[1247] Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, and Mohamed Abdelrazek. Seven\\nfailure points when engineering a retrieval augmented generation system. In Proceedings of the IEEE/ACM 3rd\\nInternational Conference on AI Engineering-Software Engineering for AI , pages 194–199, 2024.\\n[1248] Jiarui Li, Ye Yuan, and Zehua Zhang. Enhancing LLM factual accuracy with rag to counter hallucinations: A\\ncase study on domain-specific queries in private knowledge-bases. arXiv preprint arXiv:2403.10446 , 2024.\\n[1249] Christian Tomani, Kamalika Chaudhuri, Ivan Evtimov, Daniel Cremers, and Mark Ibrahim. Uncertainty-based\\nabstention in LLMs improves safety and reduces hallucinations. arXiv preprint arXiv:2404.10960 , 2024.\\n[1250] Ernesto Quevedo, Jorge Yero, Rachel Koerner, Pablo Rivas, and Tomas Cerny. Detecting hallucinations in large\\nlanguage model generation: A token probability approach. arXiv preprint arXiv:2405.19648 , 2024.\\n[1251] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and\\nEnhong Chen. Woodpecker: Hallucination correction for multimodal large language models. Science China\\nInformation Sciences , 67(12):220105, 2024.\\n[1252] Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Saddek Bensalem, Ronghui\\nMu, Yi Qi, Xingyu Zhao, et al. A survey of safety and trustworthiness of large language models through the\\nlens of verification and validation. arXiv preprint arXiv:2309.10635 , 2023.\\n[1253] Shikha Bordia and Samuel R Bowman. Identifying and reducing gender bias in word-level language models.\\nInProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Student Research Workshop , pages 7–15, 2019.\\n[1254] Kaifeng Lyu, Haoyu Zhao, Xinran Gu, Dingli Yu, Anirudh Goyal, and Sanjeev Arora. Keeping LLMs aligned\\nafter fine-tuning: The crucial role of prompt templates. arXiv preprint arXiv:2402.18540 , 2024.\\n[1255] James Y Huang, Sailik Sengupta, Daniele Bonadiman, Yi-an Lai, Arshit Gupta, Nikolaos Pappas, Saab Mansour,\\nKatrin Kirchoff, and Dan Roth. Deal: Decoding-time alignment for large language models. arXiv preprint\\narXiv:2402.06147 , 2024.\\n[1256] Xiangyu Qi, Ashwinee Panda, Kaifeng Lyu, Xiao Ma, Subhrajit Roy, Ahmad Beirami, Prateek Mittal, and\\nPeter Henderson. Safety alignment should be made more than just a few tokens deep. arXiv preprint\\narXiv:2406.05946 , 2024.\\n[1257] Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, and Ling Liu. Lazy safety alignment for large\\nlanguage models against harmful fine-tuning. arXiv preprint arXiv:2405.18641 , 2, 2024.\\n[1258] Pengyu Zhu, Zhenhong Zhou, Yuanhe Zhang, Shilinlu Yan, Kun Wang, and Sen Su. Demonagent: Dynamically\\nencrypted multi-backdoor implantation attack on llm-based agent. arXiv preprint arXiv:2502.12575 , 2025.\\n[1259] Xue Tan, Hao Luan, Mingyu Luo, Xiaoyan Sun, Ping Chen, and Jun Dai. Knowledge database or poison base?\\ndetecting rag poisoning attack through LLM activations. arXiv preprint arXiv:2411.18948 , 2024.\\n[1260] Biao Yi, Tiansheng Huang, Sishuo Chen, Tong Li, Zheli Liu, Chu Zhixuan, and Yiming Li. Probe before you\\ntalk: Towards black-box defense against backdoor unalignment for large language models. In ICLR , 2025.\\n[1261] Sahar Abdelnabi, Aideen Fay, Giovanni Cherubin, Ahmed Salem, Mario Fritz, and Andrew Paverd. Are you\\nstill on track!? catching LLM task drift with activations. arXiv preprint arXiv:2406.00799 , 2024.\\n[1262] Xi Li, Yusen Zhang, Renze Lou, Chen Wu, and Jiaqi Wang. Chain-of-scrutiny: Detecting backdoor attacks for\\nlarge language models. arXiv preprint arXiv:2406.05948 , 2024.\\n256\\n[1263] Wenjie Mo, Jiashu Xu, Qin Liu, Jiongxiao Wang, Jun Yan, Chaowei Xiao, and Muhao Chen. Test-time\\nbackdoor mitigation for black-box large language models with defensive demonstrations. arXiv preprint\\narXiv:2311.09763 , 2023.\\n[1264] Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Xiang Wang, Xiangnan He, and Tat-seng Chua.\\nAlphaedit: Null-space constrained knowledge editing for language models. arXiv preprint arXiv:2410.02355 ,\\n2024.\\n[1265] Zongru Wu, Pengzhou Cheng, Lingyong Fang, Zhuosheng Zhang, and Gongshen Liu. Gracefully filtering\\nbackdoor samples for generative large language models without retraining. arXiv preprint arXiv:2412.02454 ,\\n2024.\\n[1266] Hanlei Zhang, Yijie Bai, Yanjiao Chen, Zhongming Ma, and Wenyuan Xu. Barbie: Robust backdoor detection\\nbased on latent separability. In In the 32nd Annual Network and Distributed System Security Symposium\\n(NDSS) , 2025.\\n[1267] Yu He, Boheng Li, Liu Liu, Zhongjie Ba, Wei Dong, Yiming Li, Zhan Qin, Kui Ren, and Chun Chen. Towards\\nlabel-only membership inference attack against pre-trained large language models. In Proceedings of the 34th\\nUSENIX Security Symposium (USENIX Security) . USENIX Association, 2025.\\n[1268] Yu He, Boheng Li, Yao Wang, Mengda Yang, Juan Wang, Hongxin Hu, and Xingyu Zhao. Is difficulty\\ncalibration all we need? towards more practical membership inference attacks. In CCS, pages 1226–1240,\\n2024.\\n[1269] Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Nathaniel Hudson, Caleb Geniesse, Kyle Chard, Yaoqing\\nYang, Ian Foster, and Michael W Mahoney. Mitigating memorization in language models. arXiv preprint\\narXiv:2410.02159 , 2024.\\n[1270] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.\\nDeep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and\\ncommunications security , pages 308–318, 2016.\\n[1271] Lynn Chua, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Daogao Liu, Pasin Manurangsi,\\nAmer Sinha, and Chiyuan Zhang. Mind the privacy unit! user-level differential privacy for language model\\nfine-tuning. arXiv preprint arXiv:2406.14322 , 2024.\\n[1272] Panlong Wu, Kangshuo Li, Junbao Nan, and Fangxin Wang. Federated in-context llm agent learning. arXiv\\npreprint arXiv:2412.08054 , 2024.\\n[1273] Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li,\\nBolin Ding, and Jingren Zhou. Federatedscope-llm: A comprehensive package for fine-tuning large language\\nmodels in federated learning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery\\nand Data Mining , pages 5260–5271, 2024.\\n[1274] Pengtao Xie, Misha Bilenko, Tom Finley, Ran Gilad-Bachrach, Kristin Lauter, and Michael Naehrig. Crypto-\\nnets: Neural networks over encrypted data. arXiv preprint arXiv:1412.6181 , 2014.\\n[1275] Donghwan Rho, Taeseong Kim, Minje Park, Jung Woo Kim, Hyunsik Chae, Jung Hee Cheon, and Ernest K\\nRyu. Encryption-friendly LLM architecture. arXiv preprint arXiv:2410.02486 , 2024.\\n[1276] Antonio Muñoz, Ruben Ríos, Rodrigo Román, and Javier López. A survey on the (in) security of trusted\\nexecution environments. Computers & Security , 129:103180, 2023.\\n[1277] Brian Knott, Shobha Venkataraman, Awni Hannun, Shubho Sengupta, Mark Ibrahim, and Laurens van der\\nMaaten. Crypten: Secure multi-party computation meets machine learning. Advances in Neural Information\\nProcessing Systems , 34:4961–4973, 2021.\\n[1278] Junyuan Mao, Fanci Meng, Yifan Duan, Miao Yu, Xiaojun Jia, Junfeng Fang, Yuxuan Liang, Kun Wang, and\\nQingsong Wen. Agentsafe: Safeguarding large language model-based multi-agent systems via hierarchical data\\nmanagement. arXiv preprint arXiv:2503.04392 , 2025.\\n[1279] Yiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, Tao Wei, and Shu-Tao Xia. Black-box dataset ownership\\nverification via backdoor watermarking. IEEE Transactions on Information Forensics and Security , 2023.\\n[1280] Junfeng Guo, Yiming Li, Lixu Wang, Shu-Tao Xia, Heng Huang, Cong Liu, and Bo Li. Domain watermark:\\nEffective and harmless dataset copyright protection is closed at hand. In NeurIPS , 2023.\\n[1281] Boheng Li, Yanhao Wei, Yankai Fu, Zhenting Wang, Yiming Li, Jie Zhang, Run Wang, and Tianwei Zhang.\\nTowards reliable verification of unauthorized data usage in personalized text-to-image diffusion models. In\\nIEEE S&P , 2025.\\n257\\n[1282] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu\\nZhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and\\nPrivacy (SP) , pages 141–159. IEEE, 2021.\\n[1283] Houcheng Jiang, Junfeng Fang, Ningyu Zhang, Guojun Ma, Mingyang Wan, Xiang Wang, Xiangnan He, and\\nTat-seng Chua. Anyedit: Edit any knowledge encoded in language models. arXiv preprint arXiv:2502.05628 ,\\n2025.\\n[1284] Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, and Wenyuan Xu. SafeGen:\\nMitigating Sexually Explicit Content Generation in Text-to-Image Models. In Proceedings of the 2024 ACM\\nSIGSAC Conference on Computer and Communications Security (CCS) , 2024.\\n[1285] Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu,\\nHaifeng Chen, et al. Large language models can be good privacy protection learners. In EMNLP , 2024.\\n[1286] Lingzhi Yuan, Xinfeng Li, Chejian Xu, Guanhong Tao, Xiaojun Jia, Yihao Huang, Wei Dong, Yang Liu,\\nXiaoFeng Wang, and Bo Li. Promptguard: Soft prompt-guided unsafe content moderation for text-to-image\\nmodels. arXiv preprint arXiv:2501.03544 , 2025.\\n[1287] Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie\\nTang, and Minlie Huang. Safetybench: Evaluating the safety of large language models with multiple choice\\nquestions. arXiv preprint arXiv:2309.07045 , 2023.\\n[1288] Liang Xu, Kangkang Zhao, Lei Zhu, and Hang Xue. Sc-safety: A multi-round open-ended question adversarial\\nsafety benchmark for large language models in chinese. arXiv preprint arXiv:2310.05818 , 2023.\\n[1289] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang.\\nSafe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773 , 2023.\\n[1290] Junfeng Fang, Wei Liu, Yuan Gao, Zemin Liu, An Zhang, Xiang Wang, and Xiangnan He. Evaluating post-hoc\\nexplanations for graph neural networks via robustness analysis. Advances in neural information processing\\nsystems , 36:72446–72463, 2023.\\n[1291] Mansi Phute, Alec Helbling, Matthew Daniel Hull, ShengYun Peng, Sebastian Szyller, Cory Cornelius, and\\nDuen Horng Chau. LLM self defense: By self examination, LLMs know they are being tricked. In The Second\\nTiny Papers Track at ICLR 2024 , 2023.\\n[1292] Zhexin Zhang, Junxiao Yang, Pei Ke, Fei Mi, Hongning Wang, and Minlie Huang. Defending large language\\nmodels against jailbreaking attacks through goal prioritization. arXiv preprint arXiv:2311.09096 , 2023.\\n[1293] Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, and\\nDavid Wagner. Jatmo: Prompt injection defense by task-specific finetuning. In European Symposium on\\nResearch in Computer Security , pages 105–124. Springer, 2024.\\n[1294] Kun Wang, Yuxuan Liang, Xinglin Li, Guohao Li, Bernard Ghanem, Roger Zimmermann, Zhengyang Zhou,\\nHuahui Yi, Yudong Zhang, and Yang Wang. Brave the wind and the waves: Discovering robust and generalizable\\ngraph lottery tickets. IEEE Transactions on Pattern Analysis and Machine Intelligence , 46(5):3388–3405, 2023.\\n[1295] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-\\nefficient learning of deep networks from decentralized data. In Artificial intelligence and statistics , pages\\n1273–1282. PMLR, 2017.\\n[1296] Peng Xu, Xiatian Zhu, and David A Clifton. Multimodal learning with transformers: A survey. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence , 45(10):12113–12132, 2023.\\n[1297] Xinfeng Li, Chen Yan, Xuancun Lu, Zihan Zeng, Xiaoyu Ji, and Wenyuan Xu. Inaudible adversarial perturbation:\\nManipulating the recognition of user speech in real time. arXiv preprint arXiv:2308.01040 , 2023.\\n[1298] Elias Abad Rocamora, Yongtao Wu, Fanghui Liu, Grigorios Chrysos, and V olkan Cevher. Revisiting character-\\nlevel adversarial attacks for language models. In 41st International Conference on Machine Learning (ICML\\n2024) , 2024.\\n[1299] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard\\nprompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. Advances in Neural\\nInformation Processing Systems , 36, 2024.\\n[1300] Jialin Wu, Jiangyi Deng, Shengyuan Pang, Yanjiao Chen, Jiayang Xu, Xinfeng Li, and Wenyuan Xu. Legilimens:\\nPractical and unified content moderation for large language model services. In Proceedings of the 2024 on\\nACM SIGSAC Conference on Computer and Communications Security , pages 1151–1165, 2024.\\n[1301] Hannah Brown, Leon Lin, Kenji Kawaguchi, and Michael Shieh. Self-evaluation as a defense against adversarial\\nattacks on llms. arXiv preprint arXiv:2407.03234 , 2024.\\n258\\n[1302] Raha Moraffah, Shubh Khandelwal, Amrita Bhattacharjee, and Huan Liu. Adversarial text purification: A large\\nlanguage model approach for defense. In Pacific-Asia Conference on Knowledge Discovery and Data Mining ,\\npages 65–77. Springer, 2024.\\n[1303] Lujia Shen, Xuhong Zhang, Shouling Ji, Yuwen Pu, Chunpeng Ge, Xing Yang, and Yanghe Feng. Textdefense:\\nAdversarial text detection based on word importance entropy. arXiv preprint arXiv:2302.05892 , 2023.\\n[1304] Luke Bailey, Euan Ong, Stuart Russell, and Scott Emmons. Image hijacks: Adversarial images can control\\ngenerative models at runtime. arXiv preprint arXiv:2309.00236 , 2023.\\n[1305] Dongchen Han, Xiaojun Jia, Yang Bai, Jindong Gu, Yang Liu, and Xiaochun Cao. Ot-attack: Enhancing\\nadversarial transferability of vision-language models via optimal transport optimization. arXiv preprint\\narXiv:2312.04403 , 2023.\\n[1306] Sensen Gao, Xiaojun Jia, Xuhong Ren, Ivor Tsang, and Qing Guo. Boosting transferability in vision-language\\nattacks via diversification along the intersection region of adversarial trajectory. In European Conference on\\nComputer Vision , pages 442–460. Springer, 2024.\\n[1307] Linhao Huang, Xue Jiang, Zhiqiang Wang, Wentao Mo, Xi Xiao, Bo Han, Yongjie Yin, and Feng Zheng.\\nImage-based multimodal models as intruders: Transferable multimodal attacks on video-based mllms. arXiv\\npreprint arXiv:2501.01042 , 2025.\\n[1308] Chen Henry Wu, Rishi Rajesh Shah, Jing Yu Koh, Russ Salakhutdinov, Daniel Fried, and Aditi Raghunathan.\\nDissecting adversarial robustness of multimodal lm agents. In The Thirteenth International Conference on\\nLearning Representations , 2025.\\n[1309] Xiaoyu Ji, Yushi Cheng, Yuepeng Zhang, Kai Wang, Chen Yan, Wenyuan Xu, and Kevin Fu. Poltergeist:\\nAcoustic adversarial machine learning against cameras and computer vision. In 2021 IEEE symposium on\\nsecurity and privacy (SP) , pages 160–175. IEEE, 2021.\\n[1310] Xiaojun Jia, Yong Zhang, Baoyuan Wu, Ke Ma, Jue Wang, and Xiaochun Cao. Las-at: adversarial training\\nwith learnable attack strategy. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 13398–13408, 2022.\\n[1311] Xiaojun Jia, Yong Zhang, Xingxing Wei, Baoyuan Wu, Ke Ma, Jue Wang, and Xiaochun Cao. Improving\\nfast adversarial training with prior-guided knowledge. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence , 2024.\\n[1312] Xiaojun Jia, Sensen Gao, Simeng Qin, Ke Ma, Xinfeng Li, Yihao Huang, Wei Dong, Yang Liu, and Xiaochun\\nCao. Evolution-based region adversarial prompt learning for robustness enhancement in vision-language\\nmodels. arXiv preprint arXiv:2503.12874 , 2025.\\n[1313] Caixin Kang, Yinpeng Dong, Zhengyi Wang, Shouwei Ruan, Yubo Chen, Hang Su, and Xingxing Wei.\\nDiffender: Diffusion-based adversarial defense against patch attacks. In European Conference on Computer\\nVision , pages 130–147. Springer, 2024.\\n[1314] Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, and Mohan Kankanhalli. An LLM can\\nfool itself: A prompt-based adversarial attack. arXiv preprint arXiv:2310.13345 , 2023.\\n[1315] Zhicong Zheng, Xinfeng Li, Chen Yan, Xiaoyu Ji, and Wenyuan Xu. The silent manipulator: A practical and\\ninaudible backdoor attack against speech recognition systems. In Proceedings of the 31st ACM International\\nConference on Multimedia , pages 7849–7858, 2023.\\n[1316] Xinfeng Li, Junning Ze, Chen Yan, Yushi Cheng, Xiaoyu Ji, and Wenyuan Xu. Enrollment-stage backdoor\\nattacks on speaker recognition systems via adversarial ultrasound. IEEE Internet of Things Journal , 2023.\\n[1317] Junning Ze, Xinfeng Li, Yushi Cheng, Xiaoyu Ji, and Wenyuan Xu. Ultrabd: Backdoor attack against automatic\\nspeaker verification systems via adversarial ultrasound. In 2022 IEEE 28th International Conference on Parallel\\nand Distributed Systems (ICPADS) , pages 193–200. IEEE, 2023.\\n[1318] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang, Taimin Zhang, and Wenyuan Xu. Dolphinattack:\\nInaudible voice commands. In Proceedings of the 2017 ACM SIGSAC conference on computer and communica-\\ntions security , pages 103–117, 2017.\\n[1319] Junae Kim and Amardeep Kaur. A survey on adversarial robustness of lidar-based machine learning perception\\nin autonomous vehicles. arXiv preprint arXiv:2411.13778 , 2024.\\n[1320] James Tu, Tsunhsuan Wang, Jingkang Wang, Sivabalan Manivasagam, Mengye Ren, and Raquel Urtasun.\\nAdversarial attacks on multi-agent communication. In Proceedings of the IEEE/CVF International Conference\\non Computer Vision , pages 7768–7777, 2021.\\n259\\n[1321] Yunmok Son, Hocheol Shin, Dongkwan Kim, Youngseok Park, Juhwan Noh, Kibum Choi, Jungwoo Choi, and\\nYongdae Kim. Rocking drones with intentional sound noise on gyroscopic sensors. In 24th USENIX security\\nsymposium (USENIX Security 15) , pages 881–896, 2015.\\n[1322] Mohsin Kamal, Arnab Barua, Christian Vitale, Christos Laoudias, and Georgios Ellinas. Gps location spoofing\\nattack detection for enhancing the security of autonomous vehicles. In 2021 IEEE 94th Vehicular Technology\\nConference (VTC2021-Fall) , pages 1–7. IEEE, 2021.\\n[1323] Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer.\\nGrounding large language models in interactive environments with online reinforcement learning. In Interna-\\ntional Conference on Machine Learning , pages 3676–3713. PMLR, 2023.\\n[1324] Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong\\nYu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large language models: A survey. Computational\\nLinguistics , pages 1–79, 2024.\\n[1325] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In International\\nconference on machine learning , pages 7313–7324. PMLR, 2021.\\n[1326] Osama Mazhar, Robert Babuška, and Jens Kober. Gem: Glare or gloom, i can still see you–end-to-end\\nmulti-modal object detection. IEEE Robotics and Automation Letters , 6(4):6321–6328, 2021.\\n[1327] Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, and Yongfeng Zhang. Nphardeval: Dynamic benchmark\\non reasoning ability of large language models via complexity classes. arXiv preprint arXiv:2312.14890 , 2023.\\n[1328] Daniele Vilone and Eugenia Polizzi. Modeling opinion misperception and the emergence of silence in online\\nsocial system. Plos one , 19(1):e0296075, 2024.\\n[1329] Runsheng Xu, Jinlong Li, Xiaoyu Dong, Hongkai Yu, and Jiaqi Ma. Bridging the domain gap for multi-agent\\nperception. In 2023 IEEE International Conference on Robotics and Automation (ICRA) , pages 6035–6042.\\nIEEE, 2023.\\n[1330] Heechang Ryu, Hayong Shin, and Jinkyoo Park. Cooperative and competitive biases for multi-agent reinforce-\\nment learning. arXiv preprint arXiv:2101.06890 , 2021.\\n[1331] Xenia Ohmer, Michael Marino, Michael Franke, and Peter König. Mutual influence between language and\\nperception in multi-agent communication games. PLoS computational biology , 18(10):e1010658, 2022.\\n[1332] Runsheng Xu, Weizhe Chen, Hao Xiang, Xin Xia, Lantao Liu, and Jiaqi Ma. Model-agnostic multi-agent\\nperception framework. In 2023 IEEE International Conference on Robotics and Automation (ICRA) , pages\\n1471–1478. IEEE, 2023.\\n[1333] Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, and Chaowei Xiao. A new era in LLM security:\\nExploring security concerns in real-world LLM-based systems. arXiv preprint arXiv:2402.18649 , 2024.\\n[1334] Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang, Songyang Gao, Yilong Wu, Qi Zhang, Tao Gui, and\\nXuanjing Huang. Toolsword: Unveiling safety issues of large language models in tool learning across three\\nstages. arXiv preprint arXiv:2402.10753 , 2024.\\n[1335] Xinfeng Li, Zhicong Zheng, Chen Yan, Chaohao Li, Xiaoyu Ji, and Wenyuan Xu. Toward pitch-insensitive\\nspeaker verification via soundfield. IEEE Internet of Things Journal , 11(1):1175–1189, 2023.\\n[1336] Wanqi Yang, Yanda Li, Meng Fang, Yunchao Wei, Tianyi Zhou, and Ling Chen. Who can withstand chat-audio\\nattacks? an evaluation benchmark for large language models. arXiv preprint arXiv:2411.14842 , 2024.\\n[1337] Guoming Zhang, Xiaoyu Ji, Xinfeng Li, Gang Qu, and Wenyuan Xu. Eararray: Defending against dolphinattack\\nvia acoustic attenuation. In In the 28th Annual Network and Distributed System Security Symposium (NDSS) ,\\n2021.\\n[1338] Raghuveer Peri, Sai Muralidhar Jayanthi, Srikanth Ronanki, Anshu Bhatia, Karel Mundnich, Saket Dingliwal,\\nNilaksh Das, Zejiang Hou, Goeric Huybrechts, Srikanth Vishnubhotla, et al. Speechguard: Exploring the\\nadversarial robustness of multimodal large language models. arXiv preprint arXiv:2405.08317 , 2024.\\n[1339] Xinfeng Li, Xiaoyu Ji, Chen Yan, Chaohao Li, Yichen Li, Zhenning Zhang, and Wenyuan Xu. Learning\\nnormality is enough: A software-based mitigation against inaudible voice attacks. In 32nd USENIX Security\\nSymposium (USENIX Security 23) , pages 2455–2472, 2023.\\n[1340] Wenyuan Xu, Chen Yan, Weibin Jia, Xiaoyu Ji, and Jianhao Liu. Analyzing and enhancing the security of\\nultrasonic sensors for autonomous vehicles. IEEE Internet of Things Journal , 5(6):5015–5029, 2018.\\n[1341] Ruixu Geng, Jianyang Wang, Yuqin Yuan, Fengquan Zhan, Tianyu Zhang, Rui Zhang, Pengcheng Huang,\\nDongheng Zhang, Jinbo Chen, Yang Hu, et al. A survey of wireless sensing security from a role-based view:\\nVictim, weapon, and shield. arXiv preprint arXiv:2412.03064 , 2024.\\n260\\n[1342] Xiaoyu Ji, Wenjun Zhu, Shilin Xiao, and Wenyuan Xu. Sensor-based iot data privacy protection. Nature\\nReviews Electrical Engineering , 1(7):427–428, 2024.\\n[1343] Basudha Pal, Aniket Roy, Ram Prabhakar Kathirvel, Alice J O’Toole, and Rama Chellappa. Diversinet:\\nMitigating bias in deep classification networks across sensitive attributes through diffusion-generated data. In\\n2024 IEEE International Joint Conference on Biometrics (IJCB) , pages 1–10. IEEE, 2024.\\n[1344] Pedro Mendes, Paolo Romano, and David Garlan. Error-driven uncertainty aware training. arXiv preprint\\narXiv:2405.01205 , 2024.\\n[1345] Clayton Sanford, Bahare Fatemi, Ethan Hall, Anton Tsitsulin, Mehran Kazemi, Jonathan Halcrow, Bryan\\nPerozzi, and Vahab Mirrokni. Understanding transformer reasoning capabilities via graph algorithms. arXiv\\npreprint arXiv:2405.18512 , 2024.\\n[1346] Stephen Grossberg. A path toward explainable ai and autonomous adaptive intelligence: deep learning, adaptive\\nresonance, and models of perception, emotion, and action. Frontiers in neurorobotics , 14:36, 2020.\\n[1347] Donghee Shin. The effects of explainability and causability on perception, trust, and acceptance: Implications\\nfor explainable ai. International journal of human-computer studies , 146:102551, 2021.\\n[1348] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large\\nlanguage models can self-improve. In The 2023 Conference on Empirical Methods in Natural Language\\nProcessing , 2022.\\n[1349] Jingwei Yi, Yueqi Xie, Bin Zhu, Emre Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao Wu. Bench-\\nmarking and defending against indirect prompt injection attacks on large language models. arXiv preprint\\narXiv:2312.14197 , 2023.\\n[1350] Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan Zunger, and Emre Kiciman. Defending\\nagainst indirect prompt injection attacks with spotlighting. arXiv preprint arXiv:2403.14720 , 2024.\\n[1351] Kaijie Zhu, Xianjun Yang, Jindong Wang, Wenbo Guo, and William Yang Wang. Melon: Indirect prompt\\ninjection defense via masked re-execution and tool comparison. arXiv preprint arXiv:2502.05174 , 2025.\\n[1352] Maxwell Crouse, Ibrahim Abdelaziz, Kinjal Basu, Soham Dan, Sadhana Kumaravel, Achille Fokoue, Pavan\\nKapanipathi, and Luis Lastras. Formally specifying the high-level behavior of LLM-based agents. arXiv\\npreprint arXiv:2312.04572 , 2023.\\n[1353] Ayush RoyChowdhury, Mulong Luo, Prateek Sahu, Sarbartha Banerjee, and Mohit Tiwari. Confusedpilot:\\nConfused deputy risks in rag-based llms. arXiv preprint arXiv:2408.04870 , 2024.\\n[1354] Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. Poisonedrag: Knowledge corruption attacks to\\nretrieval-augmented generation of large language models. arXiv preprint arXiv:2402.07867 , 2024.\\n[1355] Avital Shafran, Roei Schuster, and Vitaly Shmatikov. Machine against the rag: Jamming retrieval-augmented\\ngeneration with blocker documents. arXiv preprint arXiv:2406.05870 , 2024.\\n[1356] Jiaqi Xue, Mengxin Zheng, Yebowen Hu, Fei Liu, Xun Chen, and Qian Lou. Badrag: Identifying vulnerabilities\\nin retrieval augmented generation of large language models. arXiv preprint arXiv:2406.00083 , 2024.\\n[1357] Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu, Wei Du, Ping Yi, Zhuosheng Zhang, and Gongshen Liu.\\nTrojanrag: Retrieval-augmented generation can be backdoor driver in large language models. arXiv preprint\\narXiv:2405.13401 , 2024.\\n[1358] Quanyu Long, Yue Deng, LeiLei Gan, Wenya Wang, and Sinno Jialin Pan. Whispers in grammars: Injecting\\ncovert backdoors to compromise dense retrieval systems. arXiv preprint arXiv:2402.13532 , 2024.\\n[1359] Anastasios Giannaros, Aristeidis Karras, Leonidas Theodorakopoulos, Christos Karras, Panagiotis Kranias,\\nNikolaos Schizas, Gerasimos Kalogeratos, and Dimitrios Tsolis. Autonomous vehicles: Sophisticated attacks,\\nsafety issues, challenges, open topics, blockchain, and future directions. Journal of Cybersecurity and Privacy ,\\n3(3):493–543, 2023.\\n[1360] Kurt Geihs. Engineering challenges ahead for robot teamwork in dynamic environments. Applied Sciences , 10\\n(4):1368, 2020.\\n[1361] Shah Zahid Khan, Mujahid Mohsin, and Waseem Iqbal. On gps spoofing of aerial platforms: a review of threats,\\nchallenges, methodologies, and future research directions. PeerJ Computer Science , 7:e507, 2021.\\n[1362] Jonathan Petit, Baris Stottelaar, Manfred Feiri, and Frank Kargl. Remote attacks on automated vehicles sensors:\\nExperiments on camera and lidar. Black Hat Europe , 111:99–108, 2015.\\n[1363] Jianying Zhou, Zhenfu Cao, Xiaolei Dong, and Athanasios V Vasilakos. Security and privacy in cyber-physical\\nsystems: A survey. IEEE communications surveys & tutorials , 19(2):1197–1229, 2017.\\n261\\n[1364] Yulong Cao, Chaowei Xiao, Dawei Yang, Jing Fang, Ruigang Yang, Mingyan Liu, and Bo Li. Adversarial\\nobjects against lidar-based autonomous driving systems. arXiv preprint arXiv:1907.05418 , 2019.\\n[1365] Sehoon Ha, Peng Xu, Zhenyu Tan, Sergey Levine, and Jie Tan. Learning to walk in the real world with minimal\\nhuman effort. arXiv preprint arXiv:2002.08550 , 2020.\\n[1366] Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao,\\nJian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, and Mark Gerstein. Prioritizing safeguarding over\\nautonomy: Risks of llm agents for science, 2024. URL https://arxiv.org/abs/2402.04247 .\\n[1367] Tong Liu, Zizhuang Deng, Guozhu Meng, Yuekang Li, and Kai Chen. Demystifying rce vulnerabilities in llm-\\nintegrated apps. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications\\nSecurity , pages 1716–1730, 2024.\\n[1368] Michael Guastalla, Yiyi Li, Arvin Hekmati, and Bhaskar Krishnamachari. Application of large language models\\nto ddos attack detection. In International Conference on Security and Privacy in Cyber-Physical Systems and\\nSmart Vehicles , pages 83–99. Springer, 2023.\\n[1369] Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, and Tom Goldstein. Coercing LLMs to do\\nand reveal (almost) anything. arXiv preprint arXiv:2402.14020 , 2024.\\n[1370] Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, and\\nHuan Sun. Eia: Environmental injection attack on generalist web agents for privacy leakage. arXiv preprint\\narXiv:2409.11295 , 2024.\\n[1371] Chejian Xu, Mintong Kang, Jiawei Zhang, Zeyi Liao, Lingbo Mo, Mengqi Yuan, Huan Sun, and Bo Li. Advweb:\\nControllable black-box attacks on vlm-powered web agents. arXiv preprint arXiv:2410.17401 , 2024.\\n[1372] Weidi Luo, Shenghong Dai, Xiaogeng Liu, Suman Banerjee, Huan Sun, Muhao Chen, and Chaowei Xiao.\\nAgrail: A lifelong agent guardrail with effective and adaptive safety detection. arXiv preprint arXiv:2502.11448 ,\\n2025.\\n[1373] Lewis Hammond, Alan Chan, Jesse Clifton, Jason Hoelscher-Obermaier, Akbir Khan, Euan McLean, Chandler\\nSmith, Wolfram Barfuss, Jakob Foerster, Tomáš Gaven ˇciak, et al. Multi-agent risks from advanced ai. arXiv\\npreprint arXiv:2502.14143 , 2025.\\n[1374] Aidan O’Gara. Hoodwinked: Deception and cooperation in a text-based game for language models. arXiv\\npreprint arXiv:2308.01404 , 2023.\\n[1375] Kanghua Mo, Weixuan Tang, Jin Li, and Xu Yuan. Attacking deep reinforcement learning with decoupled\\nadversarial policy. IEEE Transactions on Dependable and Secure Computing , 20(1):758–768, 2022.\\n[1376] Guanghui Wen, Peijun Wang, Yuezu Lv, Guanrong Chen, and Jialing Zhou. Secure consensus of multi-agent\\nsystems under denial-of-service attacks. Asian Journal of Control , 25(2):695–709, 2023.\\n[1377] Sumeet Ramesh Motwani, Mikhail Baranchuk, Lewis Hammond, and Christian Schroeder de Witt. A Perfect\\nCollusion Benchmark: How can AI agents be prevented from colluding with information-theoretic undetectabil-\\nity? In Multi-Agent Security Workshop NeurIPS 23 , 2023.\\n[1378] Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William Yang Wang. On the risk\\nof misinformation pollution with large language models. In arXiv preprint arXiv:2305.13661 , 2023.\\n[1379] Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, and Min Lin. Agent\\nsmith: A single image can jailbreak one million multimodal LLM agents exponentially fast. arXiv preprint\\narXiv:2402.08567 , 2024.\\n[1380] Dan Zhang, Gang Feng, Yang Shi, and Dipti Srinivasan. Physical safety and cyber security analysis of\\nmulti-agent systems: A survey of recent advances. IEEE/CAA Journal of Automatica Sinica , 8(2):319–333,\\n2021.\\n[1381] Silen Naihin, David Atkinson, Marc Green, Merwane Hamadi, Craig Swift, Douglas Schonholtz, Adam Tauman\\nKalai, and David Bau. Testing language model agents safely in the wild. arXiv preprint arXiv:2311.10538 ,\\n2023.\\n[1382] Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou,\\nFangqi Li, Zhuosheng Zhang, et al. R-judge: Benchmarking safety risk awareness for LLM agents. arXiv\\npreprint arXiv:2401.10019 , 2024.\\n[1383] Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller.\\nChemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376 , 2023.\\n262\\n[1384] Naruki Yoshikawa, Marta Skreta, Kourosh Darvish, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bjørn Kristensen,\\nAndrew Zou Li, Yuchi Zhao, Haoping Xu, Artur Kuramshin, et al. Large language models for chemistry\\nrobotics. Autonomous Robots , 47(8):1057–1086, 2023.\\n[1385] Jiyan He, Weitao Feng, Yaosen Min, Jingwei Yi, Kunsheng Tang, Shuai Li, Jie Zhang, Kejiang Chen, Wenbo\\nZhou, Xing Xie, et al. Control risk for potential misuse of artificial intelligence in science. arXiv preprint\\narXiv:2312.06632 , 2023.\\n[1386] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas\\nJoseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv\\npreprint arXiv:2112.00861 , 2021.\\n[1387] Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana,\\nErik Jenner, Stephen Casper, Oliver Sourbut, et al. Foundational challenges in assuring alignment and safety of\\nlarge language models. arXiv preprint arXiv:2404.09932 , 2024.\\n[1388] OpenAI. Introducing superalignment. https://openai.com/index/introducing-superalignment/ ,\\nJuly 2023. Accessed: 2025-03-26.\\n[1389] Eliezer Yudkowsky. Ai alignment: Why it’s hard, and where to start. Symbolic Systems Distinguished Speaker\\nSeries , 2016.\\n[1390] David Krueger, Tegan Maharaj, and Jan Leike. Hidden incentives for auto-induced distributional shift. arXiv\\npreprint arXiv:2009.09153 , 2020.\\n[1391] Joseph Carlsmith. Is power-seeking ai an existential risk? arXiv preprint arXiv:2206.13353 , 2022.\\n[1392] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement\\nlearning from human preferences. In Advances in Neural Information Processing Systems , pages 4299–4307,\\n2017.\\n[1393] Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment\\nvia reward modeling: a research direction. arXiv preprint arXiv:1811.07871 , 2018.\\n[1394] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford, Dario\\nAmodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in neural information\\nprocessing systems , 33:3008–3021, 2020.\\n[1395] Geoffrey Irving and Amanda Askell. Ai safety needs social scientists. Distill , 4(2):e14, 2019.\\n[1396] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining\\nChen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: eliciting strong\\ncapabilities with weak supervision. In Proceedings of the 41st International Conference on Machine Learning ,\\npages 4971–5012, 2024.\\n[1397] Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Risks from learned\\noptimization in advanced machine learning systems. arXiv preprint arXiv:1906.01820 , 2019.\\n[1398] Hao Zhang, Ramesh Kumar, and Wei Li. Balancing immediate accuracy and long-term goal adherence in\\nreinforcement learning. In Proceedings of the 40th Conference on Neural Information Processing Systems\\n(NeurIPS 2023) , pages 1234–1245, 2023.\\n[1399] Ming Wu, Lei Zhao, and Jun Chen. Dynamic calibration of composite rewards for robust reinforcement\\nlearning. In Proceedings of the 2023 International Conference on Learning Representations (ICLR 2023) ,\\npages 567–578, 2023.\\n[1400] Xiangwen Wang, Yibo Jacky Zhang, Zhoujie Ding, Katherine Tsai, and Sanmi Koyejo. Aligning compound ai\\nsystems via system-level dpo. arXiv preprint arXiv:2502.17721 , 2025.\\n[1401] Jixuan Leng, Chengsong Huang, Banghua Zhu, and Jiaxin Huang. Taming overconfidence in llms: Reward\\ncalibration in rlhf. arXiv preprint arXiv:2410.09724 , 2024.\\n[1402] Chenghua Huang, Zhizhen Fan, Lu Wang, Fangkai Yang, Pu Zhao, Zeqi Lin, Qingwei Lin, Dongmei Zhang,\\nSaravan Rajmohan, and Qi Zhang. Self-evolved reward learning for llms. arXiv preprint arXiv:2411.00418 ,\\n2024.\\n[1403] Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval Noah Harari,\\nYa-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, et al. Managing extreme ai risks amid rapid progress. Science ,\\n384(6698):842–845, 2024.\\n[1404] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint\\narXiv:2001.08361 , 2020.\\n263\\n[1405] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large\\nlanguage models. arXiv preprint arXiv:2203.15556 , 2022.\\n[1406] Zonghao Ying, Aishan Liu, Siyuan Liang, Lei Huang, Jinyang Guo, Wenbo Zhou, Xianglong Liu, and Dacheng\\nTao. Safebench: A safety evaluation framework for multimodal large language models. arXiv preprint\\narXiv:2410.18927 , 2024.\\n[1407] Reda Alami, Ali Khalifa Almansoori, Ahmed Alzubaidi, Mohamed El Amine Seddik, Mugariya Farooq,\\nand Hakim Hacid. Alignment with preference optimization is all you need for LLM safety. arXiv preprint\\narXiv:2409.07772 , 2024.\\n[1408] Huayu Chen, Guande He, Lifan Yuan, Ganqu Cui, Hang Su, and Jun Zhu. Noise contrastive alignment of\\nlanguage models with explicit rewards. arXiv preprint arXiv:2402.05369 , 2024.\\n[1409] Yi-Lin Tuan, Xilun Chen, Eric Michael Smith, Louis Martin, Soumya Batra, Asli Celikyilmaz, William Yang\\nWang, and Daniel M Bikel. Towards safety and helpfulness balanced responses via controllable large language\\nmodels. arXiv preprint arXiv:2404.01295 , 2024.\\n[1410] Suyu Ge, Chunting Zhou, Rui Hou, Madian Khabsa, Yi-Chia Wang, Qifan Wang, Jiawei Han, and Yuning Mao.\\nMart: Improving LLM safety with multi-round automatic red-teaming. arXiv preprint arXiv:2311.07689 , 2023.\\n[1411] Zaifan Jiang, Xing Huang, and Chao Wei. Preference as reward, maximum preference optimization with\\nimportance sampling. arXiv preprint arXiv:2312.16430 , 2023.\\n[1412] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust dpo: Aligning language models\\nwith noisy feedback. arXiv preprint arXiv:2403.00409 , 2024.\\n[1413] Yao Zhao, Misha Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J Liu. Calibrating\\nsequence likelihood improves conditional language generation. arXiv preprint arXiv:2210.00045 , 2022.\\n[1414] Yang Chao, Lu Chaochao, Wang Yingchun, and Zhou Bowen. Towards AI-45◦law: A roadmap to trustworthy\\nAGI. arXiv preprint arXiv:2412.14186 , 2024.\\n[1415] Artificial Analysis. LLM Leaderboard - Compare GPT-4o, Llama 3, Mistral, Gemini & other models, 2024.\\nURL https://artificialanalysis.ai/leaderboards/models .\\n[1416] Clémentine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, and Thomas Wolf. Open llm leaderboard\\nv2, 2024. URL https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard .\\n264\\n',\n",
              " 'statistics': {'characters': 1115992, 'words': 156388},\n",
              " 'token_counts': {'gpt2': 278683,\n",
              "  't5': 303155,\n",
              "  'bert': 265430,\n",
              "  'roberta': 278685}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}