{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSarl3T3UidIlHyHkZmba4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonpalafox/colab_notebooks/blob/main/TokenizerCount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "LZslhFdJ75y2",
        "outputId": "56cd3ef3-d272-49e3-dcf6-22f5a15d22ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--tokenizers TOKENIZERS [TOKENIZERS ...]] [--custom CUSTOM]\n",
            "                                pdf_path\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "# PDF Token Counter for Google Colab\n",
        "# This script counts tokens in PDF files using various tokenizers from Hugging Face\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from typing import Dict, List, Optional\n",
        "from google.colab import files\n",
        "\n",
        "# PDF processing\n",
        "import PyPDF2\n",
        "\n",
        "# Hugging Face tokenizers\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    GPT2Tokenizer,\n",
        "    T5Tokenizer,\n",
        "    BertTokenizer,\n",
        "    RobertaTokenizer\n",
        ")\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract all text from a PDF file\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file\n",
        "\n",
        "    Returns:\n",
        "        Extracted text as a string\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            num_pages = len(reader.pages)\n",
        "\n",
        "            print(f\"PDF has {num_pages} pages\")\n",
        "\n",
        "            for page_num in range(num_pages):\n",
        "                page = reader.pages[page_num]\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def count_tokens(text: str, tokenizer_name: str) -> int:\n",
        "    \"\"\"\n",
        "    Count tokens in text using the specified tokenizer\n",
        "\n",
        "    Args:\n",
        "        text: Text to tokenize\n",
        "        tokenizer_name: Name of the tokenizer to use\n",
        "\n",
        "    Returns:\n",
        "        Number of tokens\n",
        "    \"\"\"\n",
        "    tokenizers = {\n",
        "        \"gpt2\": GPT2Tokenizer.from_pretrained(\"gpt2\"),\n",
        "        \"t5\": T5Tokenizer.from_pretrained(\"t5-base\"),\n",
        "        \"bert\": BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
        "        \"roberta\": RobertaTokenizer.from_pretrained(\"roberta-base\"),\n",
        "    }\n",
        "\n",
        "    # Add support for custom tokenizers from Hugging Face\n",
        "    if tokenizer_name not in tokenizers:\n",
        "        try:\n",
        "            tokenizers[tokenizer_name] = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading tokenizer {tokenizer_name}: {e}\")\n",
        "            return -1\n",
        "\n",
        "    tokenizer = tokenizers[tokenizer_name]\n",
        "    tokens = tokenizer.encode(text)\n",
        "\n",
        "    return len(tokens)\n",
        "\n",
        "def count_tokens_with_multiple_tokenizers(text: str, tokenizer_names: Optional[List[str]] = None) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Count tokens in text using multiple tokenizers\n",
        "\n",
        "    Args:\n",
        "        text: Text to tokenize\n",
        "        tokenizer_names: List of tokenizer names to use\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping tokenizer names to token counts\n",
        "    \"\"\"\n",
        "    if not tokenizer_names:\n",
        "        # Default tokenizers\n",
        "        tokenizer_names = [\"gpt2\", \"t5\", \"bert\", \"roberta\"]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for tokenizer_name in tokenizer_names:\n",
        "        token_count = count_tokens(text, tokenizer_name)\n",
        "        results[tokenizer_name] = token_count\n",
        "\n",
        "    return results\n",
        "\n",
        "def find_pdf_files():\n",
        "    \"\"\"Find PDF files in the current directory and subdirectories\"\"\"\n",
        "    pdf_files = []\n",
        "    for root, _, files in os.walk('.'):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.pdf'):\n",
        "                pdf_files.append(os.path.join(root, file))\n",
        "    return pdf_files\n",
        "\n",
        "# For use in Colab notebooks\n",
        "def analyze_pdf(pdf_path=None, tokenizers=None, custom_tokenizer=None):\n",
        "    \"\"\"Analyze a PDF file and count tokens\"\"\"\n",
        "\n",
        "    if pdf_path is None:\n",
        "        # List available PDF files\n",
        "        pdf_files = find_pdf_files()\n",
        "\n",
        "        if not pdf_files:\n",
        "            print(\"No PDF files found in the current directory.\")\n",
        "            print(\"Please upload a PDF file using the code below:\")\n",
        "            print(\"from google.colab import files\")\n",
        "            print(\"uploaded = files.upload()\")\n",
        "            return\n",
        "\n",
        "        print(\"Available PDF files:\")\n",
        "        for i, file in enumerate(pdf_files):\n",
        "            print(f\"{i+1}. {file}\")\n",
        "\n",
        "        choice = input(\"Enter the number of the PDF to analyze (or upload a new one): \")\n",
        "\n",
        "        try:\n",
        "            choice_idx = int(choice) - 1\n",
        "            if 0 <= choice_idx < len(pdf_files):\n",
        "                pdf_path = pdf_files[choice_idx]\n",
        "            else:\n",
        "                print(\"Invalid choice. Please upload a PDF file.\")\n",
        "                uploaded = files.upload()\n",
        "                pdf_path = list(uploaded.keys())[0]\n",
        "        except:\n",
        "            print(\"Invalid input. Please upload a PDF file.\")\n",
        "            uploaded = files.upload()\n",
        "            pdf_path = list(uploaded.keys())[0]\n",
        "\n",
        "    # Check if the PDF file exists\n",
        "    if not os.path.isfile(pdf_path):\n",
        "        print(f\"Error: PDF file {pdf_path} not found\")\n",
        "        return\n",
        "\n",
        "    # Set default tokenizers if not provided\n",
        "    if tokenizers is None:\n",
        "        tokenizers = [\"gpt2\", \"t5\", \"bert\", \"roberta\"]\n",
        "\n",
        "    # Add custom tokenizer if provided\n",
        "    tokenizers_to_use = tokenizers.copy()\n",
        "    if custom_tokenizer:\n",
        "        tokenizers_to_use.append(custom_tokenizer)\n",
        "\n",
        "    # Extract text from PDF\n",
        "    print(f\"Extracting text from {pdf_path}...\")\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    if not text:\n",
        "        print(\"No text extracted from PDF. Check if the PDF contains extractable text.\")\n",
        "        return\n",
        "\n",
        "    # Print text statistics\n",
        "    print(f\"\\nText statistics:\")\n",
        "    print(f\"Characters: {len(text):,}\")\n",
        "    print(f\"Words: {len(text.split()):,}\")\n",
        "\n",
        "    # Count tokens using different tokenizers\n",
        "    print(f\"\\nCounting tokens using {len(tokenizers_to_use)} tokenizers...\")\n",
        "    token_counts = count_tokens_with_multiple_tokenizers(text, tokenizers_to_use)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nToken counts:\")\n",
        "    for tokenizer_name, count in token_counts.items():\n",
        "        if count >= 0:\n",
        "            print(f\"{tokenizer_name}: {count:,} tokens\")\n",
        "        else:\n",
        "            print(f\"{tokenizer_name}: Error loading tokenizer\")\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"statistics\": {\n",
        "            \"characters\": len(text),\n",
        "            \"words\": len(text.split())\n",
        "        },\n",
        "        \"token_counts\": token_counts\n",
        "    }\n",
        "\n",
        "# For direct execution in Colab\n",
        "if __name__ == \"__main__\":\n",
        "    # Filter out Colab-specific arguments\n",
        "    args = [arg for arg in sys.argv if not arg.startswith('-f')]\n",
        "    sys.argv = args\n",
        "\n",
        "    # Create argparse without the required argument, which causes problems in Colab\n",
        "    parser = argparse.ArgumentParser(description=\"Count tokens in PDF files using various tokenizers\")\n",
        "    parser.add_argument(\"pdf_path\", nargs='?', help=\"Path to the PDF file\")\n",
        "    parser.add_argument(\"--tokenizers\", nargs=\"+\", default=[\"gpt2\", \"t5\", \"bert\", \"roberta\"],\n",
        "                      help=\"List of tokenizers to use (default: gpt2, t5, bert, roberta)\")\n",
        "    parser.add_argument(\"--custom\", help=\"Use a custom tokenizer from Hugging Face (e.g., 'gpt-neox-20b')\")\n",
        "\n",
        "    try:\n",
        "        args = parser.parse_args()\n",
        "        analyze_pdf(args.pdf_path, args.tokenizers, args.custom)\n",
        "    except SystemExit:\n",
        "        # Called from a notebook without arguments\n",
        "        analyze_pdf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "00k1tLUW8MFr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}