{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqawijOyRrBI77SnXaTXeL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonpalafox/colab_notebooks/blob/main/TokenizerCount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF Token Counter for Google Colab\n",
        "\n",
        "This script helps you analyze PDF documents and count tokens using different tokenizers from Hugging Face. It's specifically designed to work in Google Colab notebooks.\n",
        "\n",
        "## Features\n",
        "\n",
        "- Extract and analyze text from PDF documents\n",
        "- Count tokens using multiple tokenizers (GPT-2, T5, BERT, RoBERTa)\n",
        "- Support for custom tokenizers from Hugging Face\n",
        "- Interactive file selection in Colab\n",
        "- Detailed text statistics\n",
        "\n",
        "## Requirements\n",
        "\n",
        "The script requires these Python packages:\n",
        "- PyPDF2\n",
        "- transformers\n",
        "- torch\n",
        "\n",
        "These are typically pre-installed in Google Colab, but you can install any missing packages with:\n",
        "\n",
        "```python\n",
        "!pip install PyPDF2 transformers torch\n",
        "```\n",
        "\n",
        "## Basic Usage\n",
        "\n",
        "### Simple Method\n",
        "\n",
        "Run the script and then call the `analyze_pdf()` function without arguments:\n",
        "\n",
        "```python\n",
        "# After pasting the script in a cell and running it\n",
        "analyze_pdf()\n",
        "```\n",
        "\n",
        "This will:\n",
        "1. Search for PDF files in your Colab environment\n",
        "2. Let you select one from a list or upload a new one\n",
        "3. Analyze the PDF and display token counts from multiple tokenizers\n",
        "\n",
        "### Advanced Method\n",
        "\n",
        "You can specify parameters directly:\n",
        "\n",
        "```python\n",
        "analyze_pdf(\n",
        "    pdf_path=\"your_document.pdf\",              # Path to a specific PDF file\n",
        "    tokenizers=[\"gpt2\", \"bert\"],               # Only use these tokenizers\n",
        "    custom_tokenizer=\"facebook/opt-350m\"       # Add a custom tokenizer\n",
        ")\n",
        "```\n",
        "\n",
        "## Function Reference\n",
        "\n",
        "### `analyze_pdf(pdf_path=None, tokenizers=None, custom_tokenizer=None)`\n",
        "\n",
        "Analyzes a PDF file and counts tokens.\n",
        "\n",
        "**Parameters:**\n",
        "- `pdf_path` (str, optional): Path to the PDF file. If not provided, will interactively find/request a file.\n",
        "- `tokenizers` (list, optional): List of tokenizer names to use. Defaults to [\"gpt2\", \"t5\", \"bert\", \"roberta\"].\n",
        "- `custom_tokenizer` (str, optional): Name of a custom tokenizer from Hugging Face.\n",
        "\n",
        "**Returns:**\n",
        "A dictionary containing:\n",
        "- `text`: The extracted text\n",
        "- `statistics`: Character and word counts\n",
        "- `token_counts`: Token counts for each tokenizer\n",
        "\n",
        "## Example Output\n",
        "\n",
        "```\n",
        "PDF has 5 pages\n",
        "Extracting text from example.pdf...\n",
        "\n",
        "Text statistics:\n",
        "Characters: 15,428\n",
        "Words: 2,547\n",
        "\n",
        "Counting tokens using 4 tokenizers...\n",
        "\n",
        "Token counts:\n",
        "gpt2: 3,256 tokens\n",
        "t5: 4,121 tokens\n",
        "bert: 3,891 tokens\n",
        "roberta: 3,347 tokens\n",
        "```\n",
        "\n",
        "## Understanding Token Counts\n",
        "\n",
        "Different tokenizers will produce different token counts for the same text because they:\n",
        "\n",
        "1. **Use different vocabularies** - Words common in the training data may be a single token\n",
        "2. **Have different tokenization strategies** - Some keep punctuation separate, others combine\n",
        "3. **Handle whitespace differently** - Some tokenizers preserve whitespace as tokens\n",
        "4. **Process capitalization differently** - Some are case-sensitive\n",
        "\n",
        "This is why the script shows results from multiple tokenizers, helping you understand the range of possible token counts for your document.\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "- **No text extracted**: Some PDFs contain images of text rather than actual text characters. Try using OCR software first.\n",
        "- **Error loading tokenizer**: Check your internet connection or try a different tokenizer.\n",
        "- **Memory errors**: Very large PDFs may cause memory issues. Try processing the document in smaller chunks.\n",
        "\n",
        "## Advanced Usage\n",
        "\n",
        "You can capture the results for further analysis:\n",
        "\n",
        "```python\n",
        "results = analyze_pdf(\"your_document.pdf\")\n",
        "\n",
        "# Access extracted text\n",
        "text = results[\"text\"]\n",
        "\n",
        "# Get statistics\n",
        "character_count = results[\"statistics\"][\"characters\"]\n",
        "word_count = results[\"statistics\"][\"words\"]\n",
        "\n",
        "# Get token counts for specific tokenizers\n",
        "gpt2_count = results[\"token_counts\"][\"gpt2\"]\n",
        "bert_count = results[\"token_counts\"][\"bert\"]\n",
        "```"
      ],
      "metadata": {
        "id": "8-MtK9v7AVyd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZslhFdJ75y2"
      },
      "outputs": [],
      "source": [
        "# PDF Token Counter for Google Colab\n",
        "# This script counts tokens in PDF files using various tokenizers from Hugging Face\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from typing import Dict, List, Optional\n",
        "from google.colab import files\n",
        "\n",
        "# PDF processing\n",
        "import PyPDF2\n",
        "\n",
        "# Hugging Face tokenizers\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    GPT2Tokenizer,\n",
        "    T5Tokenizer,\n",
        "    BertTokenizer,\n",
        "    RobertaTokenizer\n",
        ")\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract all text from a PDF file\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file\n",
        "\n",
        "    Returns:\n",
        "        Extracted text as a string\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            num_pages = len(reader.pages)\n",
        "\n",
        "            print(f\"PDF has {num_pages} pages\")\n",
        "\n",
        "            for page_num in range(num_pages):\n",
        "                page = reader.pages[page_num]\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def count_tokens(text: str, tokenizer_name: str) -> int:\n",
        "    \"\"\"\n",
        "    Count tokens in text using the specified tokenizer\n",
        "\n",
        "    Args:\n",
        "        text: Text to tokenize\n",
        "        tokenizer_name: Name of the tokenizer to use\n",
        "\n",
        "    Returns:\n",
        "        Number of tokens\n",
        "    \"\"\"\n",
        "    tokenizers = {\n",
        "        \"gpt2\": GPT2Tokenizer.from_pretrained(\"gpt2\"),\n",
        "        \"t5\": T5Tokenizer.from_pretrained(\"t5-base\"),\n",
        "        \"bert\": BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
        "        \"roberta\": RobertaTokenizer.from_pretrained(\"roberta-base\"),\n",
        "    }\n",
        "\n",
        "    # Add support for custom tokenizers from Hugging Face\n",
        "    if tokenizer_name not in tokenizers:\n",
        "        try:\n",
        "            tokenizers[tokenizer_name] = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading tokenizer {tokenizer_name}: {e}\")\n",
        "            return -1\n",
        "\n",
        "    tokenizer = tokenizers[tokenizer_name]\n",
        "    tokens = tokenizer.encode(text)\n",
        "\n",
        "    return len(tokens)\n",
        "\n",
        "def count_tokens_with_multiple_tokenizers(text: str, tokenizer_names: Optional[List[str]] = None) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Count tokens in text using multiple tokenizers\n",
        "\n",
        "    Args:\n",
        "        text: Text to tokenize\n",
        "        tokenizer_names: List of tokenizer names to use\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping tokenizer names to token counts\n",
        "    \"\"\"\n",
        "    if not tokenizer_names:\n",
        "        # Default tokenizers\n",
        "        tokenizer_names = [\"gpt2\", \"t5\", \"bert\", \"roberta\"]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for tokenizer_name in tokenizer_names:\n",
        "        token_count = count_tokens(text, tokenizer_name)\n",
        "        results[tokenizer_name] = token_count\n",
        "\n",
        "    return results\n",
        "\n",
        "def find_pdf_files():\n",
        "    \"\"\"Find PDF files in the current directory and subdirectories\"\"\"\n",
        "    pdf_files = []\n",
        "    for root, _, files in os.walk('.'):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.pdf'):\n",
        "                pdf_files.append(os.path.join(root, file))\n",
        "    return pdf_files\n",
        "\n",
        "# For use in Colab notebooks\n",
        "def analyze_pdf(pdf_path=None, tokenizers=None, custom_tokenizer=None):\n",
        "    \"\"\"Analyze a PDF file and count tokens\"\"\"\n",
        "\n",
        "    if pdf_path is None:\n",
        "        # List available PDF files\n",
        "        pdf_files = find_pdf_files()\n",
        "\n",
        "        if not pdf_files:\n",
        "            print(\"No PDF files found in the current directory.\")\n",
        "            print(\"Please upload a PDF file using the code below:\")\n",
        "            print(\"from google.colab import files\")\n",
        "            print(\"uploaded = files.upload()\")\n",
        "            print(\"pdf_path = list(uploaded.keys())[0]\")\n",
        "            print(\"analyze_pdf(pdf_path)\")\n",
        "            return\n",
        "\n",
        "        print(\"Available PDF files:\")\n",
        "        for i, file in enumerate(pdf_files):\n",
        "            print(f\"{i+1}. {file}\")\n",
        "\n",
        "        # Avoid using input() which can cause widget issues in Colab\n",
        "        print(\"\\nTo analyze a file, call: analyze_pdf('file_path.pdf')\")\n",
        "        print(\"Example: analyze_pdf('\" + pdf_files[0] + \"')\")\n",
        "        return\n",
        "\n",
        "    # Check if the PDF file exists\n",
        "    if not os.path.isfile(pdf_path):\n",
        "        print(f\"Error: PDF file {pdf_path} not found\")\n",
        "        return\n",
        "\n",
        "    # Set default tokenizers if not provided\n",
        "    if tokenizers is None:\n",
        "        tokenizers = [\"gpt2\", \"t5\", \"bert\", \"roberta\"]\n",
        "\n",
        "    # Add custom tokenizer if provided\n",
        "    tokenizers_to_use = tokenizers.copy()\n",
        "    if custom_tokenizer:\n",
        "        tokenizers_to_use.append(custom_tokenizer)\n",
        "\n",
        "    # Extract text from PDF\n",
        "    print(f\"Extracting text from {pdf_path}...\")\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    if not text:\n",
        "        print(\"No text extracted from PDF. Check if the PDF contains extractable text.\")\n",
        "        return\n",
        "\n",
        "    # Print text statistics\n",
        "    print(f\"\\nText statistics:\")\n",
        "    print(f\"Characters: {len(text):,}\")\n",
        "    print(f\"Words: {len(text.split()):,}\")\n",
        "\n",
        "    # Count tokens using different tokenizers\n",
        "    print(f\"\\nCounting tokens using {len(tokenizers_to_use)} tokenizers...\")\n",
        "    token_counts = count_tokens_with_multiple_tokenizers(text, tokenizers_to_use)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nToken counts:\")\n",
        "    for tokenizer_name, count in token_counts.items():\n",
        "        if count >= 0:\n",
        "            print(f\"{tokenizer_name}: {count:,} tokens\")\n",
        "        else:\n",
        "            print(f\"{tokenizer_name}: Error loading tokenizer\")\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"statistics\": {\n",
        "            \"characters\": len(text),\n",
        "            \"words\": len(text.split())\n",
        "        },\n",
        "        \"token_counts\": token_counts\n",
        "    }\n",
        "\n",
        "# For direct execution in Colab\n",
        "if __name__ == \"__main__\":\n",
        "    # Filter out Colab-specific arguments\n",
        "    args = [arg for arg in sys.argv if not arg.startswith('-f')]\n",
        "    sys.argv = args\n",
        "\n",
        "    # Create argparse without the required argument, which causes problems in Colab\n",
        "    parser = argparse.ArgumentParser(description=\"Count tokens in PDF files using various tokenizers\")\n",
        "    parser.add_argument(\"pdf_path\", nargs='?', help=\"Path to the PDF file\")\n",
        "    parser.add_argument(\"--tokenizers\", nargs=\"+\", default=[\"gpt2\", \"t5\", \"bert\", \"roberta\"],\n",
        "                      help=\"List of tokenizers to use (default: gpt2, t5, bert, roberta)\")\n",
        "    parser.add_argument(\"--custom\", help=\"Use a custom tokenizer from Hugging Face (e.g., 'gpt-neox-20b')\")\n",
        "\n",
        "    try:\n",
        "        args = parser.parse_args()\n",
        "        analyze_pdf(args.pdf_path, args.tokenizers, args.custom)\n",
        "    except SystemExit:\n",
        "        # Called from a notebook without arguments\n",
        "        analyze_pdf()"
      ]
    }
  ]
}